{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# データの前処理\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot-encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_o = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_o = enc.fit_transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape)\n",
    "print(y_train_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_o, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】全結合層のクラス化¶\n",
    "全結合層のクラス化を行なってください。 以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B =  initializer.B(self.n_nodes2)\n",
    "        # adagrad用\n",
    "        #if Adagrad = True: \n",
    "        self.h_W = np.zeros((n_nodes1, n_nodes2))\n",
    "        self.h_B = np.zeros(n_nodes2)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"      \n",
    "        \n",
    "        self.A =  (X @ self.W) + self.B\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.Z = Z\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.Z.T @ dA \n",
    "        dZ = dA @ self.W.T\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】初期化方法のクラス化¶\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        self.W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return self.W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        self.B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return self.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, batch):\n",
    "        self.lr = lr\n",
    "        self.batch = batch\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr * layer.dW / self.batch\n",
    "        layer.B = layer.B - self.lr * layer.dB / self.batch\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def forward(self, A):\n",
    "        # overflow対策\n",
    "        A_max = np.max(A, axis=1)\n",
    "        exp_A = np.exp(A - A_max.reshape(-1, 1))\n",
    "        sum_exp_A = np.sum(exp_A, axis=1).reshape(-1, 1)\n",
    "        return exp_A / sum_exp_A\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        loss = self._cross_entropy(Z, Y)\n",
    "        D  = Z - Y\n",
    "        return D, loss \n",
    "    \n",
    "    def _cross_entropy(self,Z, y):\n",
    "        \"\"\"\n",
    "        パラメータ\n",
    "        ーーーーーーーーーーーーー\n",
    "        y : 正解ラベル（one-hot表現）\n",
    "        z３　：　クラスの確率\n",
    "        n : バッチサイズ\n",
    "\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:    # 次元が 1 の場合\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -1* np.sum(y * np.log(Z)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】ReLUクラスの作成¶\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def forward(self, A):\n",
    "        self.mask = (A <= 0)\n",
    "        a = A.copy()\n",
    "        self.A = A\n",
    "        a[self.mask] = 0\n",
    "        return np.maximum(0, A)\n",
    "      \n",
    "    def backward(self, dA):\n",
    "        dA[self.mask] = 0\n",
    "        out = dA\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XavierInitializerクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer():\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = 1/ np.sqrt(sigma)\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeInitializerクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeInitializer():\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = np.sqrt(2 / sigma)\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        self.W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return self.W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        self.B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return self.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, batch):\n",
    "        self.lr = lr\n",
    "        self.batch = batch\n",
    "        # 過去の勾配の２乗和の保管用\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        layer.h_W += (layer.dW/ self.batch) * (layer.dW/ self.batch)\n",
    "        layer.h_B += (layer.dB/self.batch) * (layer.dB / self.batch)\n",
    "        layer.W -= self.lr * (1 / (np.sqrt(layer.h_W) + 1e-07)) * (layer.dW/ self.batch)\n",
    "        layer.B -= self.lr * (1 / (np.sqrt(layer.h_B) + 1e-07)) * (layer.dB/ self.batch)\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】クラスの完成¶\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=10):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \n",
    "    def __init__(self, lr=0.01,sigma=0.1, n_iter=30, batch=20, vervose=False):\n",
    "        self.iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.batch = batch\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y,  X_val=None, y_val=None):\n",
    "        self.n_features = 784\n",
    "        self.n_nodes1 = 400\n",
    "        self.n_nodes2 =  200\n",
    "        self.n_output = 10\n",
    "        \n",
    "        #更新方法の定義\n",
    "        self.optimizer = AdaGrad(self.lr, self.batch)\n",
    "        # 各層の定義\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(self.n_features), self.optimizer)\n",
    "        self.activation1 = Relu()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(self.n_nodes1), self.optimizer)\n",
    "        self.activation2 = Relu()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, HeInitializer(self.n_nodes2), self.optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        # 学習開始\n",
    "        for i in range(self.iter):\n",
    "            # １エポックのloss記録保管\n",
    "            history = np.zeros(X.shape[0] // self.batch)\n",
    "            val_history = np.zeros(X_val.shape[0] // self.batch)\n",
    "            #  バッチサイズのカウント\n",
    "            count = 0\n",
    "            #　ミニバッチの取り出しコード\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # forward\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # backward\n",
    "                dA3, history[count] = self.activation3.backward(Z3, mini_y_train)\n",
    "                dZ2 = self.FC3.backward(dA3, Z2)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2, Z1)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1, mini_X_train)\n",
    "                count += 1\n",
    "            \n",
    "            A1 = self.FC1.forward(X)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            _, history= self.activation3.backward(Z3, y)\n",
    "            self.loss[i] = history\n",
    "            \n",
    "            # X_valあった場合の処理\n",
    "            if np.any(X_val): \n",
    "                A1 = self.FC1.forward(X_val)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                # 学習記録\n",
    "                _, val_history = self.activation3.backward(Z3, y_val)\n",
    "                self.val_loss[i] = val_history\n",
    "            \n",
    "            \n",
    "    # 推定\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return np.argmax(Z3, axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題9】学習と推定\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDNNC = ScratchDeepNeuralNetrowkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SDNNC.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SDNNC.loss)\n",
    "plt.plot(SDNNC.val_loss)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = SDNNC.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 追加\n",
    "別モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier1():\n",
    "    \n",
    "    def __init__(self, lr=0.01,sigma=0.1, n_iter=30, batch=20, vervose=False):\n",
    "        self.iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.batch = batch\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y,  X_val=None, y_val=None):\n",
    "        self.n_features = 784\n",
    "        self.n_nodes1 = 400\n",
    "        self.n_nodes2 =  200\n",
    "        self.n_nodes3 = 100\n",
    "        self.n_output = 10\n",
    "        \n",
    "        #更新方法の定義\n",
    "        self.optimizer = SGD(self.lr, self.batch)\n",
    "        # 各層の定義\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, HeInitializer(self.n_features), self.optimizer)\n",
    "        self.activation1 = Relu()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, HeInitializer(self.n_nodes1), self.optimizer)\n",
    "        self.activation2 = Relu()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_nodes3, HeInitializer(self.n_nodes2), self.optimizer)\n",
    "        self.activation3 = Relu()\n",
    "        self.FC4 = FC(self.n_nodes3, self.n_output, HeInitializer(self.n_nodes3), self.optimizer)\n",
    "        self.activation4 = Softmax()\n",
    "        \n",
    "        # 学習開始\n",
    "        for i in range(self.iter):\n",
    "            # １エポックのloss記録保管\n",
    "            history = np.zeros(X.shape[0] // self.batch)\n",
    "            val_history = np.zeros(X_val.shape[0] // self.batch)\n",
    "            #  バッチサイズのカウント\n",
    "            count = 0\n",
    "            #　ミニバッチの取り出しコード\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # forward\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                A4 = self.FC4.forward(Z3)\n",
    "                Z4 = self.activation4.forward(A4)\n",
    "\n",
    "                # backward\n",
    "                dA4, history[count] = self.activation4.backward(Z4, mini_y_train)\n",
    "                dZ3 = self.FC4.backward(dA4, Z3)\n",
    "                dA3 = self.activation3.backward(dZ3)\n",
    "                dZ2 = self.FC3.backward(dA3, Z2)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2, Z1)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1, mini_X_train)\n",
    "                count += 1\n",
    "            \n",
    "            A1 = self.FC1.forward(mini_X_train)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            A4 = self.FC4.forward(Z3)\n",
    "            Z4 = self.activation4.forward(A4)\n",
    "            \n",
    "            _, history= self.activation3.backward(Z3, y)\n",
    "            self.loss[i] = history\n",
    "            \n",
    "            # X_valあった場合の処理\n",
    "            if np.any(X_val): \n",
    "                A1 = self.FC1.forward(X_val)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                A4 = self.FC4.forward(Z3)\n",
    "                Z4 = self.activation4.forward(A4)\n",
    "                # 学習記録\n",
    "                _, val_history = self.activation4.backward(Z4, y_val)\n",
    "                self.val_loss[i] = val_history\n",
    "            \n",
    "            \n",
    "    # 推定\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        A4 = self.FC4.forward(Z3)\n",
    "        Z4 = self.activation4.forward(A4)\n",
    "        return np.argmax(Z3, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDNNC1 = ScratchDeepNeuralNetrowkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDNNC.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "plt.plot(SDNNC.loss)\n",
    "plt.plot(SDNNC.val_loss)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = SDNNC.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
