{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN、SPPnetという手法が存在。各手法についての概要は以下記載。\n",
    "\n",
    "RCNN：\n",
    "一般物体検出の研究がディープラーニングの時代に突入した際のモデル。\n",
    "R-CNNはCVPR 2014で発表された論文 Rich feature hierarchies for accurate object detection and semantic segmentationで提案された一般物体検出手法。\n",
    "この手法の1番のポイントは、CNN(畳み込みニューラルネットワーク)を特徴抽出器として利用した点。\n",
    "入力画像からSelective Searchで物体が写っている領域の候補(region proposals)矩形を2000個ほど抽出し、CNNの入力画像とする。Selective Searchであらかじめ候補領域を絞り込むことで、画像全体に隈なく認識Windowを走らせるよりも高速化を図っている。\n",
    "CNNの入力サイズ(ピクセル数)は固定なため、Selective Searchで抽出した領域はCNNの入力サイズに合わせて変形・リサイズする。\n",
    "\n",
    "SPPNet：\n",
    "ECCV2014で発表された論文 ”Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition”で提案された手法。（http://kaiminghe.com/eccv14sppnet/index.html）\n",
    "画像全体をCNNに入力して全体の特徴マップを抽出する。互いに重なる部分も多い2000個の矩形領域を個別にCNNにかける冗長性を排除した。\n",
    "SPP（Spatial Pyramid Pooling）では元画像からSelective Searchで検出した矩形に対応する特徴マップの領域にSpatial Pyramid Pooling(空間ピラミッドプーリング)と呼ばれるpooling処理を施す。このSpatial Pyramid Poolingにより、矩形領域のサイズに関わらず固定長のベクトルを得ることができる。\n",
    "SPPでは、特徴マップを異なるサイズのウィンドウ(16×16, 4×4, 1×1)でmax-poolingした結果を平坦化して連結し、固定長のベクトルにしてから次の全結合層の入力とする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝論文＝＝<br>\n",
    "<strong>ABstraction</strong><br>\n",
    "—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
    "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectiveSearch手法の改善としてRPN手法を利用し高速化している。\n",
    "RPNの手法については後述に記載。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝論文＝＝<br>\n",
    "<strong>1 INTRODUCTION</strong><br>\n",
    "In this paper, we show that an algorithmic change\n",
    "—computing proposals with a deep convolutional neural network—\n",
    "leads to an elegant and effective solution　where proposal computation is nearly cost-free given\n",
    "the detection network’s computation. To this end, we\n",
    "introduce novel Region Proposal Networks (RPNs) that\n",
    "share convolutional layers with state-of-the-art object\n",
    "detection networks [1], [2]. By sharing convolutions at\n",
    "test-time, the marginal cost for computing proposals\n",
    "is small (e.g., 10ms per image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-stage手法はクラス固有検出を行なっていて,\n",
    "Two-stage手法はクラス固有とクラスに依存しない2段階の検出を行なっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==論文==<br>\n",
    "<strong>One-Stage Detection vs. Two-Stage Proposal + Detection</strong><br>\n",
    "OverFeatis a one-stage, class-specific detection pipeline, and ours　is a two-stage cascade consisting of class-agnostic proposals and class-specific detections.\n",
    "\n",
    "The first module is a deep　fully convolutional network that proposes regions,the second module is the Fast R-CNN detector [2]that uses the proposed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN はfully convolutional networkの１種で、広いレンジを使った領域提案を効率的に行なっている。\n",
    "アンカーと呼ばれる新しいフィルタ機能を利用している。Anchor boxにさまざまな形、サイズを用意しておくとで多種多様な物体を検出できるようにしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==論文==<br>\n",
    "<strong>1 INTRODUCTION</strong><br>\n",
    "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. In\n",
    "contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters\n",
    "(Figure 1, b), we introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) RoIプーリングとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体サイズによらず同一サイズの特徴量ベクトルを生成し、物体識別用ネットワークに入力するためのプーリング方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝論文＝＝<br>\n",
    "<strong>3.2 Sharing Features for RPN and Fast R-CNN</strong><br>\n",
    "Non-approximate joint training. As discussed　above, the bounding boxes predicted by RPN are　also functions of the input. The RoI pooling layer[2] in Fast R-CNN accepts the convolutional features　and also the predicted bounding boxes as input, so　a theoretically valid　backpropagation solver should\n",
    "also involve gradients w.r.t. the box coordinates. These　gradients are ignored in the above approximate joint　training. In a non-approximate joint training solution,　we need an RoI pooling layer that is differentiable　w.r.t. the box coordinates. This is a nontrivial problem　and a solution can be given by an “RoI warping” layer　as developed in [15], which is beyond the scope of this　paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Anchorのサイズはどうするのが適切か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この論文内ではアンカーの数9個、featuremapのサイズがW*H、３スケールと３アスペクトのrationsで設定されている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==論文==<br>\n",
    "<strong>3.1.1 Anchors</strong><br>\n",
    "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is\n",
    "denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not\n",
    "object for each proposal4. \n",
    "The k proposals are parameterized relative to k reference boxes, which we call anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect\n",
    "ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size\n",
    "W × H (typically ∼2,400), there are W Hk anchors in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットはMSCOCOとPASCALVOCのデータを利用している。\n",
    "\n",
    "先行研究に比べ、先行研究よりも良いmAP,IoUが得られている。\n",
    "\n",
    "物体検出に使われる代表的な評価指標：mAP、IoUについて<br>\n",
    "mAP:　<br>\n",
    "ある画像(物体)の情報が与えられた時点までの適合率(Precision)の平均であるAP(Average Precision)の平均です。\n",
    "<br>IoU:<br>\n",
    "$\n",
    "{IoU = \\frac{TP}{TP∪TN∪FN} \\\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝論文＝＝<br>\n",
    "<strong>4.3 From MS COCO to PASCAL VOC</strong><br>\n",
    "Large-scale data is of crucial importance for improving deep neural networks. Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC. \n",
    "As a simple baseline, we directly evaluate the COCO detection model on the PASCAL VOC dataset, without fine-tuning on any PASCAL VOC data. This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC. The categories that are exclusive on COCO are ignored in this experiment, and the softmax layer is performed only on the 20 categories plus background. The mAP under this setting is 76.1% on the PASCAL VOC 2007 test set (Table 12). This result is better than that trained on VOC07+12 (73.2%) by a good margin, even though the PASCAL VOC data are not exploited. \n",
    "Then we fine-tune the COCO detection model on the VOC dataset. In this experiment, the COCO model is in place of the ImageNet-pre-trained model (that is used to initialize the network weights), and the Faster R-CNN system is fine-tuned as described in Section 3.2. Doing so leads to 78.8% mAP on the PASCAL VOC 2007 test set. The extra data from the COCO set increases the mAP by 5.6%. Table 6 shows that the model trained on COCO+VOC has the best AP for every individual category on PASCAL VOC 2007. Similar improvements are observed on the PASCAL VOC 2012 test set (Table 12 and Table 7). We note that the test-time speed of obtaining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask-RCNN\n",
    "論文PDF：\n",
    "Mask-RCNNではFaster-RCNNをベースに作られていて、Head層にMaskBranchという分岐処理を追加したもの。\n",
    "\n",
    "引用箇所"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気になったもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNNのアルゴリズム流れ\n",
    "- 物体らしさ(Objectness)を見つける既存手法(Selective Search)を用いて、画像から領域候補(Region Proposals)を探します(2000個程度)\n",
    "- 領域候補の領域画像を 全て一定の大きさにリサイズして CNNにかけて特徴量を取り出す\n",
    "- 取り出した特徴量を使って複数のSVMによって学習しカテゴリ識別、regressorによってBounding Box の正確な位置を推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selective Search：　物体候補(object proposal)アルゴリズム\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-end 学習\n",
    "- 前処理かけたり複数のモデルを組み合わせたりすることなく、入力と出力の関係を直接単一のモデルで学習すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
