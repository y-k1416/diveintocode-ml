{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X1 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y1 = df['SalePrice']\n",
    "X1 = X1.apply(np.log)\n",
    "y1 = y1.apply(np.log)\n",
    "X1 = X1.values\n",
    "y1 = y1.values\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # １の列の追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Xの列数分のランダムの重み作成\n",
    "    w = np.random.rand(n)\n",
    "\n",
    "    yp = X @ w.T\n",
    "    \n",
    "    return yp, w, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp, w1, X1 = _linear_hypothesis(X1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gradient_descent( X, y, w, yp):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    X:配列\n",
    "    y:正解値\n",
    "    w: _linear_hypothesisで求めた重み\n",
    "    yp: _linear_hypothesisで求めた予測値\n",
    "    \n",
    "    \"\"\"\n",
    "    # yを配列にする\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # 誤差求める関数\n",
    "    def error(yp , y):\n",
    "        yd = yp - y\n",
    "        return yd\n",
    "\n",
    "    #正解との誤差\n",
    "    yd = error(yp, y)\n",
    "    \n",
    "    # パラメータ\n",
    "    alpha = 0.00001\n",
    "    \n",
    "    #  学習回数\n",
    "    epoch = 1000\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    \n",
    "    # 勾配降下法\n",
    "    for k in range(epoch):\n",
    "        w = w - alpha* (X.T @ yd) / M\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = _gradient_descent(X1, y1_train, w1, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42625181, 0.4499624 , 1.12531434])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 hθ(x)の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Xの列に１の値を追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    y = X @ w\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X1_test, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.42357226, 12.29889109, 12.27493652, 11.96934219, 12.29764213,\n",
       "       12.16560364, 12.24882648, 12.61067032, 12.51877674, 12.27454785,\n",
       "       12.31275871, 12.06865464, 12.02175559, 12.10958152, 12.41676573,\n",
       "       12.25040105, 12.42389685, 11.93310467, 12.30631261, 11.96125096,\n",
       "       12.42411137, 12.14457218, 12.0063544 , 12.1132184 , 12.45728622,\n",
       "       12.39181089, 12.13022328, 12.44979307, 12.49982267, 12.26683779,\n",
       "       12.16052151, 11.98621698, 12.28225456, 12.25677646, 12.13640826,\n",
       "       12.07177786, 12.19351545, 12.15563575, 12.23700832, 12.60870278,\n",
       "       12.01479332, 12.19794733, 12.49974113, 12.37176636, 12.04448477,\n",
       "       12.19703466, 12.1438153 , 12.33007643, 12.23541036, 12.45685466,\n",
       "       12.12198032, 12.02255475, 12.20722982, 12.45972007, 12.08452098,\n",
       "       12.08781526, 12.26866335, 12.26867247, 12.2516207 , 12.26488965,\n",
       "       12.13814979, 12.42222065, 12.17029578, 12.31973841, 12.16269087,\n",
       "       12.40549885, 12.06613284, 12.12323614, 12.06528729, 12.3318313 ,\n",
       "       12.07646655, 12.39230412, 12.17353056, 12.38870403, 12.01862369,\n",
       "       12.2665365 , 12.37115861, 12.32041452, 12.30967987, 12.3017966 ,\n",
       "       11.95324614, 12.01931774, 12.14944828, 12.32019825, 12.05499422,\n",
       "       12.16138545, 12.15876649, 12.17061333, 12.14118225, 12.2572933 ,\n",
       "       12.08185699, 12.25729452, 12.33153665, 12.30857768, 12.44613908,\n",
       "       12.30715495, 12.37023557, 12.17810953, 12.30927762, 12.40830042,\n",
       "       12.41892913, 11.93713126, 12.38920999, 12.28898601, 12.08157269,\n",
       "       12.35942556, 12.24383675, 12.46182834, 12.24334868, 12.08919241,\n",
       "       12.11800245, 12.25360487, 12.29171248, 12.52186692, 11.97680705,\n",
       "       12.2871945 , 12.27006321, 12.31413261, 12.26121354, 12.0171658 ,\n",
       "       12.35422301, 12.33624835, 12.02763631, 12.01594742, 12.19703466,\n",
       "       12.19019569, 12.2814893 , 12.32656867, 12.31785108, 12.30541631,\n",
       "       12.19516093, 12.00624408, 12.30971758, 12.12947293, 12.06577987,\n",
       "       11.94045879, 12.51709476, 12.36462022, 12.28847467, 12.07704379,\n",
       "       12.38468648, 12.36833047, 12.32879248, 12.23922402, 12.08566693,\n",
       "       12.12206908, 12.03911599, 12.19424104, 12.23755168, 12.39944232,\n",
       "       12.32244521, 12.47918643, 12.23412755, 12.09319148, 12.19511862,\n",
       "       12.05396373, 11.9505999 , 12.37582237, 12.31775658, 12.43335317,\n",
       "       11.86298026, 12.21413122, 12.27391586, 12.12922112, 12.09863274,\n",
       "       12.32002948, 12.22695922, 12.46893801, 12.0961451 , 12.00028815,\n",
       "       12.39188603, 12.21462274, 12.21727211, 12.24567094, 12.39791898,\n",
       "       12.45251705, 12.24928357, 12.42169042, 12.32898759, 12.52406829,\n",
       "       12.27661902, 12.2820474 , 12.35024142, 12.39703162, 11.88917757,\n",
       "       12.24855052, 12.37499799, 12.30693043, 12.27267427, 12.36334922,\n",
       "       12.28738109, 12.42053731, 12.34565616, 12.52348968, 12.11048042,\n",
       "       12.31275322, 12.22652552, 12.02443115, 12.13111574, 12.18355432,\n",
       "       12.18912693, 12.09397209, 12.10489346, 12.48199877, 12.17871639,\n",
       "       12.12625987, 12.41212406, 12.5473944 , 12.31890714, 12.50095137,\n",
       "       12.33127344, 11.98621698, 12.25158019, 12.386261  , 12.28201312,\n",
       "       12.32520397, 12.27006756, 12.2848167 , 12.61375557, 12.33322   ,\n",
       "       12.44235073, 12.02427761, 12.47485471, 12.17273405, 12.14495948,\n",
       "       11.89881345, 12.27433856, 12.37507396, 12.45655382, 12.32177248,\n",
       "       12.26956907, 12.10757044, 12.39771528, 12.16579288, 12.02980551,\n",
       "       12.35092655, 12.1330416 , 12.12097691, 12.4209233 , 12.05956173,\n",
       "       12.12826806, 12.53615082, 12.3647229 , 12.32915914, 12.19682538,\n",
       "       12.19430438, 12.28860953, 11.8982332 , 12.41169522, 12.29277453,\n",
       "       12.29938152, 11.98829347, 11.91421041, 12.17498035, 12.20965737,\n",
       "       12.15630683, 12.28335482, 12.21634044, 12.05198819, 12.195273  ,\n",
       "       12.29796   , 12.3374106 , 12.21581288, 12.249389  , 12.34992105,\n",
       "       12.0330927 , 12.36131582, 12.1516232 , 12.10489346, 12.21142207,\n",
       "       12.32548578, 12.30103086, 12.20152526, 12.06584954, 12.32955056,\n",
       "       12.25295887, 12.25862084, 12.09694219, 12.29715088, 12.02358049,\n",
       "       12.15614177, 12.4057173 , 12.04681622, 12.1142997 , 12.23635445,\n",
       "       12.13601914, 12.172294  , 12.22681025, 11.94065675, 12.07992467,\n",
       "       12.48010194, 12.35386126])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\ \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n-1} (y_i – \\hat{y_i})^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    len_y = len(y_pred)\n",
    "    \n",
    "    yd = y_pred - y\n",
    "    mse = sum(yd**2) / len_y\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14536223427521053"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y1_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_func(y, y_pred):\n",
    "    \"\"\"\n",
    "    目的関数を求める関数\n",
    "    ------------------------------\n",
    "    parameter\n",
    "    \n",
    "    y_pred: yの予測値\n",
    "    y: 正解値\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    m = len(y_pred)\n",
    "    j = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        j [i] = (1/2*m)* np.sum(np.square(y_pred - y))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的なCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_iter, alpha, bias=None, verbose=None):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "        self.iter = num_iter\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "            \n",
    "        \"\"\"\n",
    "        y = y.values\n",
    "        X = X.values\n",
    "        if np.any(X_val):\n",
    "            y_val = y_val.values\n",
    "            X_val = X_val.values\n",
    "        \n",
    "        \n",
    "        if  not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            if np.any(X_val):\n",
    "                X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "\n",
    "        n = X.shape[1]\n",
    "        self.w = np.zeros(n,)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            # 予測関数の呼び出し\n",
    "            y_pred1 = self._linear_hypothesis(X)\n",
    "            #y_pred2 = self._linear_hypothesis(X_val)\n",
    "\n",
    "            error1 = y_pred1 - y\n",
    "            #error2 = y_pred2 - y_val\n",
    "\n",
    "            # 最急降下法で学習\n",
    "            self._gradient_descent1(X, error1, i)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"loss: \", self.loss[i])\n",
    "            \n",
    "        if np.any(y_val):\n",
    "            n = X_val.shape[1]\n",
    "            self.w = np.zeros(n,)\n",
    "            for i in range(self.iter):\n",
    "                y_pred2 = self._linear_hypothesis(X_val)\n",
    "                error2 = y_pred2 - y_val\n",
    "                self._gradient_descent2(X_val, error2, i)\n",
    "\n",
    "                #学習過程の表示\n",
    "                if self.verbose:\n",
    "                    print(\"val_loss: \", self.val_loss[i])\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        y_pred = np.dot(X, self.w)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def _gradient_descent1(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "    def _gradient_descent2(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.val_loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        if  not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = np.dot(X, self.w) \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#　データ作成\n",
    "df = pd.read_csv('train.csv')\n",
    "X2 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y2 = df['SalePrice']\n",
    "X2 = X2.apply(np.log)\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# priceデータの対数変換\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# テストデータとtrainデータの作成\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SLR = ScratchLinearRegression(num_iter=100, alpha=0.001, bias=True ,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  4217339.21574309\n",
      "loss:  3337192.956326109\n",
      "loss:  2640771.4256416173\n",
      "loss:  2089723.2947221317\n",
      "loss:  1653702.8223279857\n",
      "loss:  1308698.7411530185\n",
      "loss:  1035711.9780457617\n",
      "loss:  819709.3914392894\n",
      "loss:  648795.9092124766\n",
      "loss:  513559.47732878913\n",
      "loss:  406552.74613918166\n",
      "loss:  321882.9512608902\n",
      "loss:  254887.40412587827\n",
      "loss:  201876.72174335687\n",
      "loss:  159931.6555608851\n",
      "loss:  126742.33096468462\n",
      "loss:  100481.04447712019\n",
      "loss:  79701.61370273466\n",
      "loss:  63259.73731118523\n",
      "loss:  50249.979350037414\n",
      "loss:  39955.907667879546\n",
      "loss:  31810.64061372743\n",
      "loss:  25365.629353487922\n",
      "loss:  20265.95667257658\n",
      "loss:  16230.791991056423\n",
      "loss:  13037.92626584101\n",
      "loss:  10511.535130365623\n",
      "loss:  8512.496398383919\n",
      "loss:  6930.728725175989\n",
      "loss:  5679.129522849188\n",
      "loss:  4688.7782959561655\n",
      "loss:  3905.141249260325\n",
      "loss:  3285.0681586723035\n",
      "loss:  2794.416125630035\n",
      "loss:  2406.169357126093\n",
      "loss:  2098.951429290314\n",
      "loss:  1855.8481061661837\n",
      "loss:  1663.4758873267467\n",
      "loss:  1511.2449900505771\n",
      "loss:  1390.776179121879\n",
      "loss:  1295.439329575043\n",
      "loss:  1219.9883114318607\n",
      "loss:  1160.272089848906\n",
      "loss:  1113.0061312093783\n",
      "loss:  1075.5915266900583\n",
      "loss:  1045.9718725942448\n",
      "loss:  1022.5200259740465\n",
      "loss:  1003.9484992718428\n",
      "loss:  989.2385594910951\n",
      "loss:  977.5841274490315\n",
      "loss:  968.3473876913978\n",
      "loss:  961.0236645456225\n",
      "loss:  955.2136300669056\n",
      "loss:  950.6013133927936\n",
      "loss:  946.9367005003879\n",
      "loss:  944.021966149823\n",
      "loss:  941.7005798189821\n",
      "loss:  939.8486857027239\n",
      "loss:  938.3682820805813\n",
      "loss:  937.1818244465915\n",
      "loss:  936.227955200301\n",
      "loss:  935.4581247366782\n",
      "loss:  934.8339178612987\n",
      "loss:  934.3249382987652\n",
      "loss:  933.907134796101\n",
      "loss:  933.5614766410399\n",
      "loss:  933.2729056571657\n",
      "loss:  933.0295069631806\n",
      "loss:  932.8218528307151\n",
      "loss:  932.6424835075146\n",
      "loss:  932.4854964153602\n",
      "loss:  932.3462211002336\n",
      "loss:  932.2209620344915\n",
      "loss:  932.1067951073926\n",
      "loss:  932.0014065968982\n",
      "loss:  931.9029657550387\n",
      "loss:  931.810023990256\n",
      "loss:  931.7214350947628\n",
      "loss:  931.6362921239222\n",
      "loss:  931.5538774516369\n",
      "loss:  931.4736232513482\n",
      "loss:  931.3950802263687\n",
      "loss:  931.317892867551\n",
      "loss:  931.2417798757542\n",
      "loss:  931.1665186709945\n",
      "loss:  931.0919331352069\n",
      "loss:  931.0178839136245\n",
      "loss:  930.9442607406835\n",
      "loss:  930.8709763678411\n",
      "loss:  930.7979617589244\n",
      "loss:  930.7251622884218\n",
      "loss:  930.6525347333454\n",
      "loss:  930.5800448930328\n",
      "loss:  930.5076657057975\n",
      "loss:  930.4353757587123\n",
      "loss:  930.3631581084696\n",
      "loss:  930.2909993483755\n",
      "loss:  930.2188888701062\n",
      "loss:  930.1468182795677\n",
      "loss:  930.0747809346888\n",
      "val_loss:  263649.95874410565\n",
      "val_loss:  208738.04341962148\n",
      "val_loss:  165265.82645237132\n",
      "val_loss:  130850.09626207934\n",
      "val_loss:  103604.1314082747\n",
      "val_loss:  82034.26770125896\n",
      "val_loss:  64958.013298977006\n",
      "val_loss:  51439.222737026015\n",
      "val_loss:  40736.776035247174\n",
      "val_loss:  32263.94939282492\n",
      "val_loss:  25556.250112679794\n",
      "val_loss:  20245.952417639906\n",
      "val_loss:  16041.93817377784\n",
      "val_loss:  12713.737359238836\n",
      "val_loss:  10078.893353209995\n",
      "val_loss:  7992.96039128977\n",
      "val_loss:  6341.584832812278\n",
      "val_loss:  5034.23612336685\n",
      "val_loss:  3999.243774509255\n",
      "val_loss:  3179.868280475091\n",
      "val_loss:  2531.190573677816\n",
      "val_loss:  2017.6494942930462\n",
      "val_loss:  1611.092274331561\n",
      "val_loss:  1289.231160824414\n",
      "val_loss:  1034.4215678966145\n",
      "val_loss:  832.6947741999794\n",
      "val_loss:  672.9921367328834\n",
      "val_loss:  546.5588395039946\n",
      "val_loss:  446.4639414357471\n",
      "val_loss:  367.2204118163274\n",
      "val_loss:  304.4843230758028\n",
      "val_loss:  254.81671018483286\n",
      "val_loss:  215.49504145230665\n",
      "val_loss:  184.36396526926933\n",
      "val_loss:  159.71715051259952\n",
      "val_loss:  140.20374292276685\n",
      "val_loss:  124.75430925450594\n",
      "val_loss:  112.52220934806962\n",
      "val_loss:  102.83718205038984\n",
      "val_loss:  95.16860049698029\n",
      "val_loss:  89.09638235421916\n",
      "val_loss:  84.28796027801857\n",
      "val_loss:  80.48005007501385\n",
      "val_loss:  77.46421706961401\n",
      "val_loss:  75.07544940358859\n",
      "val_loss:  73.1831118393977\n",
      "val_loss:  71.68378414124341\n",
      "val_loss:  70.49559142318864\n",
      "val_loss:  69.55371564555286\n",
      "val_loss:  68.80684219311436\n",
      "val_loss:  68.2143467312178\n",
      "val_loss:  67.74406811904227\n",
      "val_loss:  67.37054528780091\n",
      "val_loss:  67.07362142690414\n",
      "val_loss:  66.83733895748327\n",
      "val_loss:  66.64906471407396\n",
      "val_loss:  66.4987973756094\n",
      "val_loss:  66.37861917805736\n",
      "val_loss:  66.28226185076782\n",
      "val_loss:  66.20476298051271\n",
      "val_loss:  66.14219396458081\n",
      "val_loss:  66.0914446389107\n",
      "val_loss:  66.05005277425334\n",
      "val_loss:  66.01606909308967\n",
      "val_loss:  65.98795040732814\n",
      "val_loss:  65.96447501843203\n",
      "val_loss:  65.94467574208423\n",
      "val_loss:  65.92778688570257\n",
      "val_loss:  65.91320227203212\n",
      "val_loss:  65.90044200760721\n",
      "val_loss:  65.88912617427893\n",
      "val_loss:  65.8789540015388\n",
      "val_loss:  65.86968737783461\n",
      "val_loss:  65.86113779694422\n",
      "val_loss:  65.8531560237867\n",
      "val_loss:  65.84562391313739\n",
      "val_loss:  65.83844793273408\n",
      "val_loss:  65.83155403570372\n",
      "val_loss:  65.82488360120631\n",
      "val_loss:  65.81839022075762\n",
      "val_loss:  65.81203715405151\n",
      "val_loss:  65.80579531480544\n",
      "val_loss:  65.79964167621223\n",
      "val_loss:  65.79355800857999\n",
      "val_loss:  65.78752987995824\n",
      "val_loss:  65.78154586496116\n",
      "val_loss:  65.77559691841557\n",
      "val_loss:  65.76967587949528\n",
      "val_loss:  65.76377707915871\n",
      "val_loss:  65.75789602936818\n",
      "val_loss:  65.75202917705336\n",
      "val_loss:  65.74617370933123\n",
      "val_loss:  65.74032739930445\n",
      "val_loss:  65.73448848398354\n",
      "val_loss:  65.72865556764235\n",
      "val_loss:  65.72282754530626\n",
      "val_loss:  65.717003542181\n",
      "val_loss:  65.71118286569904\n",
      "val_loss:  65.7053649675569\n",
      "val_loss:  65.69954941366096\n"
     ]
    }
   ],
   "source": [
    "SLR.fit(X2_train, y2_train,  X_val=X2_val, y_val=y2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SLR.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001540948955252724"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y2_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003095456683099062"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y2_val, lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAETCAYAAABwaNKCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYXGWd9vHv3fuSTjpLJyEJWZAd\njBEjBEGIGkcYFtHBqCNKiEwigzI6LgMyDq+OL/iKCiLIMpIgDE5ckD0wKpIAYe2EJSzKGiFkXztr\nd6f79/5Rp5NK6E46na4+3V3357rqSp3nLM/vVEHd/Zxz6pQiAjMzs65WkHYBZmaWnxxAZmaWCgeQ\nmZmlwgFkZmapcACZmVkqHEBmZpYKB5D1SpLmSDqzHcv9H0lXtzFvkaTxnV/dO/oZKencXPezm/7b\nfA06sK2QNKgzttXKtjtUp6RPShqXi5ps3ziAzFIkqQC4Gbg/mR4n6UFJj0h6WtKlkoqSeZMknZhm\nvXtD0mhJsyU9nOzLtZKqUijlj8C1kipS6Nt2wwFklq7JwF8iYnEyPQv4fkQcDxwDVAGHJPPOAsZ0\nfYkddh1wW0R8EDgKWAm8r6uLiIgNwN3Al7u6b9s9B5ClLjlsc7GkRyUtlHScpHslPSPpvyQpWe4g\nSQ9IelLSY5KOydrGZEnPSvpTcpimPGteoaTLJT2erHtNy6hiL2osl3SVpCeSx08llSfzDpH0UPL4\nX0mHJO3TJNVKmifpSknFrWz6s8Dvs6b7AdUAEdEQEV+JiBckfQo4Gfh2Mjoqaev1SEYeiyRdlIym\nXsk+xCfpPUlND0v6LTBql9fqJ5KeS7b7H1nz5kj6bDKqmaKMS5P37H5JF+yyb9n7EhHxHxExJ9nW\nEEm/Td6TBZK+ndXPhUn/j0m6ruX9b+U9OStZ/xFJ90ganrRPkXRTUtuvk8VvT15r604iwg8/Un0A\nAXwleX4psA7YHygEXgZOTOYdC0xMnh8EPJQ8PwRYAYxKpscBG4Ezk+lvAVcDSh7/BXwzmfd/gKvb\nqGsRMD55/jPgRjJ/tBUkz69K5l0FfCN5/m6gf/K8DhiUPD+xjT6WADVZ0x8BFgN/Bj4FFGbNuwmY\nkjXd1usxGmjOek2PBDYk+14EvAp8PJk3AFjY8hoAA4Fzk30U8DDwrmTeHOA+oG8yPRV4DChNpv8p\neS9b9vk9wF+AJ4EvtiyXzLsP+E7yvBi4FhhC5g+HLwFlybxbgQ/v+l4BE5K++yTTZwH3Js+nAK8B\nH9vltd4ElKT937sfOx579VegWQ61/KX6CvBoRLwFIOkVdvyFvp7MCOD7QBOZkAKYBPwpIv4GEBHP\nSFqQte1PAiXAg8l0BXs/+p9M5sO+OanrR8n2LgB+A/xS0kBgRkSsTda5HrhP0vXA/7Sx3UFAy/JE\nxAOSxgCnJdu+WNJJEbGslXXbej0A6oFrkucvAH2A/sB+QEVE3Jn0t0bS7WSCCDJBNQKYSyZMDkqm\nX0vm3xMRdcnzk4GZEVGfTN8I3JC1L89KOpzM+/PPwHcknQy8CXwUODNZrhE4DyAZ7ZQB90sqTPbp\nD63s+xlkAuueZIBUQCY8W6yJiP/dZZ11yTJLW9mepcCH4Ky7aE7+jaznLdMt/53eAcwDTgD+jsxf\n6Lsu0yJ7uhC4JCImJo+jI+KLHahx1zv3CiAiHmHHX/v3Szolaf8mmcM+hwDzJVW2ss0NwE7tEdEY\nEb+PiIlkPvjPa6Oetl4PgKaWsIzkz38yr8meXqtvAMcDp0bECWRCNnu7y7NL3WXdd3yeRERzRPwh\nIs4AbgMuzFq3tTshf5bMaOkzkTl39D+79N+iELg96z09ISKOaKPOFn3IjEqtm3AAWU/SF3g8+WCd\nBttH8H8APiLpXQCSPgq8P2u93wPfarkCS9KHJf3LXvb9O+CC5LyHgK+R+UBF0glAVUT8ksyo46PJ\nuZRPAq8nQdRAZjSxq78AByfb6SvpVkkjk+kCMqO1t5NlG4E+WedE2no9duevwDpJk5M+RgGfy5rf\nF3ghItZLGk1m9NLauSuAu4BztePqsn8n+eNBUpGkX0p6d9byfYC3I2ITmffsm8myLeeSDk/6fz0i\nliUjyk+00f8dwOclHZhsY7ikn7e105KGAHVJ39ZN+BCc9STTgVslrQHuBd6U1C8iXk1Osv9e0lbg\nmWR+i8vJfPg9KqmOzPmitkYVbflmsp3HyPxFXkvm3BLJtn8nqWX0NpXM/1vHABdKagYeAZ5rZbt3\nkhm9PBURdZJmA7dJaiTzwfsg8Itk2TvInCs5RdIZbb0eu9uJiGiS9AngeknfSF6LX5G5YADgCuDX\nkp4gc37qZ2QC8o+tbOu/k9B/QtJGMqOVlcm8bZJ+Reby56LkNXse+H6y+hTgZ5KeSl6zP5EJ40XA\nSZJqgdVkAv3gVvqeJ+mrwG8lbQG2seP9aM3fkQlM60a0Y3RuZl1NUl8yIXNsRDSkXU9vlIwYHwLO\najlPaN2DA8gsZcmJ+bER8f/SrqU3knQWmavqfrHHha1LOYDMugFJBS0XDVjnklQYEU1p12Hv5AAy\nM7NU+Co4MzNLha+C241BgwbF6NGj0y7DzKxHmT9//qqIqNnTcg6g3Rg9ejS1tbVpl2Fm1qNIatfV\nhj4EZ2ZmqXAAmZlZKhxAZmaWCp8DMjPbjcbGRhYvXszWrVvTLqXbKSsrY8SIERQXt3W7wN1zAJmZ\n7cbixYupqqpi9OjRtPHbeHkpIli9ejWLFy9mzJiO/VCvD8GZme3G1q1bGThwoMNnF5IYOHDgPo0M\nHUBmZnvg8Gndvr4uDqAceGrRGn54/19obvZtjszM2uIAyoFn31rHz+e8xob6bWmXYma9wKJFi5gw\nYULaZXQ6B1AOVFeUALBus3/excysLb4KLgf6V2QuSVy7uZFRA1Muxsw6zXfvfoEXl9R16jYPH9aX\nS047ol3LbtmyhenTp7No0SK2bdvGt7/9bU499VQefPBB/u3f/o2ysjKmTp3KlClT+NrXvsaTTz5J\nZWUl119/fYevVMslB1AOtIyA1noEZGad6LLLLuOggw7i5ptvZu3atUyYMIFjjz2W2bNnc8kll3Dy\nySezZMkSAObOncu8efNYv349AwYMSLny1jmAcmBApQ/BmfVG7R2p5MqCBQv47ne/C0D//v0ZO3Ys\nL730EpdccglXXnkls2fP5txzz2XEiBHMnDmTiy66iMrKSi6++GJKSkpSrb01PgeUAy2H4NZsaky5\nEjPrTcaNG8cDDzwAwPr163nuuec45JBDWLFiBRdddBE/+tGPuOCCCwCoqKjgyiuv5OCDD+YXv+ie\nv0buEVAO9C0rpkAeAZlZ57rooouYPn06EydOpL6+nh/+8IfU1NTw4IMPMmXKFLZu3crkyZNpaGjg\n8ssv5+WXX2bz5s3MnDkz7dJb5QDKgYIC0a+82OeAzKxTjB49mscffxyA//7v/37H/MmTJzN58uSd\n2m644YYuqW1f5PwQnKTvSJqTPH+PpLmSHpd0t6T+SXu1pNskPSrpCUnjknZJuixpe0bS57K2O1nS\nk5LmS/pxVvte9ZEr/StKWLvZh+DMzNqS0wCSNB4YkzwXMAu4ICImAPcB30sWvRyYExEfAP4JuClp\n/0fgIGACcAJwsaT9JI0C/hP4KDAeGCHpHzrYR05UVxT7EJyZ2W7kLIAklQNXAhcmTQcDayPi2WT6\nF8ApyfO/T6aJiOeAOknvAk4FboiMOuB3ybInAbdFxPqICOB64IwO9rFr3dMk1UqqXblyZYf3v39F\nCWt9EYKZWZtyOQK6HLgyIlYk0wOBZS0zI6KBHeegiiJiS9a6S4HBu66zt+3t7GMnEXFDRIyPiPE1\nNTXt3NV3qq4o8QjIzGw3chJAkj4G9I+I32U1LyfrA19SKdDyCb0lmW4xNFl+p3X2tr2dfeRE/4pi\nnwMyM9uNXI2ATgVqJN0h6Q7gSOASoI+kI5NlPk/mHA3APcA5AJIOA6oi4nXgTuCLSXsF8MlkndnA\nJyRVJetPBe6MiNc60EdO9K8sYUtjE1sbm3LVhZlZj5aTy7Aj4ivZ05LmRMQXkivP/ktSM7AaODtZ\n5DvALyWdDQSZQAG4DThWUm3S/oOIWJps81LgIUkNwMMRcVuyzpS97CMnqpMvo67b3MjQfoW57MrM\nrEfqku8BRcTE5N9ngGNbmb8WOL2V9gC+3sY2bwVubaV9r/rIlQFZ94Mb2q+sq7o1szw1ceJErrvu\nOg499NBW5w8dOpRly5a1Oi8t/iJqjviGpGa90H0XwrKFnbvNoe+Gk3/QudvsIXwvuBzpX7njEJyZ\nWUd9+MMfZuHCTOg9+OCDfOpTn+LTn/40EyZM4MQTT2Tp0qV7tb2I4F//9V857rjjmDBhAjNmzADg\n+eef59hjj+XEE0/k0ksvBeDHP/4xEyZMYOLEiSxYsKBzdwyPgHKmfzICWrPJIyCzXiOFkcr555/P\njTfeyJVXXsmMGTO44IILqKur45RTTuHmm2/mV7/6FV//eqtnKlo1c+ZM6urqmDdvHvX19Rx//PEc\nc8wxzJ07l7POOovzzz+fN998E4Dbb7+de++9l4KCAgoLO/9ctkdAObLjIgQHkJl13BlnnMHcuXNZ\nuXIlixcvZuTIkdxyyy2ceOKJ/OQnP2HDhg17tb0FCxZw8sknA1BaWsrEiRN5+umnmTZtGo2NjZx3\n3nm88sorANxyyy384Ac/4Pvf/z719fWdvm8OoBwpLSqkoqTQ3wUys31SWFjImWeeybRp05g6dSpX\nXHHF9hHLl7/8ZTLXarVf9k86NDQ0MHfuXMaOHcvy5cuZPn0611xzDRdemLmBzbZt27j88ss5/fTT\nueyyyzp933wILocyNyT1CMjM9s20adO47rrrmDVrFgceeCDTp0/nz3/+M5MmTeKtt97aq21NnTqV\nhQsX8sEPfpDGxkbOPfdcxo4dy5w5c/jc5z5HQ0MDxx9/PAAzZszg0UcfZdOmTVx++eWdvl/a2/TM\nJ+PHj4/a2toOr3/KVQ8zpG8ZM6a8vxOrMrOu9NJLL3HYYYelXUa7feYzn3nH5dazZs1i6NChOemv\ntddH0vyIGL+ndT0CyiGPgMysq82aNSvtEtrN54ByKPOTDD4HZNbT+UhR6/b1dXEA5ZBHQGY9X1lZ\nGatXr3YI7SIiWL16NWVlHb/Tiw/B5VD/yhLWb2mkqTkoLFDa5ZhZB4wYMYLFixezL78P1luVlZUx\nYsSIDq/vAMqh/hXFREDdlkb6V5akXY6ZdUBxcTFjxoxJu4xeyYfgcqi/7wdnZtYmB1AOtdwNwV9G\nNTN7JwdQDrWMgHw7HjOzd3IA5dCOQ3AeAZmZ7coBlEPVyU8yrPUdsc3M3sEBlENVpUUUFcgXIZiZ\ntcIBlEOSqK4o9iE4M7NWOIByrLqixBchmJm1wgGUY/0rin0IzsysFQ6gHMuMgHwIzsxsVw6gHBvg\nG5KambXKAZRj1ZWZixB8J10zs505gHKsf0UJDdua2dLYlHYpZmbdigMoxwYkd0NY4y+jmpntxAGU\nY4OqMgG0aqMDyMwsmwMox2r6ZH4tcOWG+pQrMTPrXhxAOVZTVQrAig1bU67EzKx7cQDl2MA+mUNw\nHgGZme3MAZRjxYUFDKgscQCZme3CAdQFBleVOoDMzHbhAOoCNVWlrNzoADIzy+YA6gI1fTwCMjPb\nlQOoC9Qkh+B8Ox4zsx0cQF2gpqqU+m3NbKjflnYpZmbdhgOoC7R8F8iH4czMdnAAdYGaPsmXUesc\nQGZmLRxAXWD7CMhXwpmZbZezAJL0LUmPSlogaYakEkkjJd2ftM+RNCpZtkTSjVnLT8razgWSnpL0\njKRvZLV/SNJjkp6UdIukkqR9r/vINR+CMzN7p5wEkKRBQD/guIg4CqgAPg7cCFwTER8Afghcnazy\nTWBd0n4acK2kUknHAZ8FjgOOBs6QNF5SH2AmcGZEHA0sBb6SbGuv+sjF/u+qX3kxJYUFDiAzsyw5\nCaCIWBURF0dEJGHRD3gRODQi7k6WmQ0cmYxcTgWuT9rfBh4Djk/aZ0ZEQ0Q0ADPIBNlxwKPJsgDX\nkQmnig70sRNJ0yTVSqpduXJlp7wekrZfim1mZhk5PQck6VbgDeABYB2w6yf6CmBg8liW1b4UGNyB\n9uoO9LGTiLghIsZHxPiampo972Q7DfLdEMzMdlKUy41HxOeSUcktQB2ZEMhWA6wClpMJg7qkfWjS\n1tJOO9tXdaCPLlHTp5S3123pqu7MzLq9XJ0DGifpbICI2Ay8TOY80EJJJyXLTAJeiIhG4E7g3KR9\nCDABmJe0f0FSsaRC4GzgrmTeMZL2S7r8InBncphub/voEj4EZ2a2s1yNgP4KnCfpK8AWYDHwn8Dt\nwE2SvgPUA+cky18F3CjpCUDA+RFRD9RKugt4AmgCZkVELYCk84B7JNUDrwLfS7Z1/l720SVqqkpZ\nvamebU3NFBX66nczs5wEUERsAaa3MutvwIdaWb4B+Hwb2/oR8KNW2v8EvK+V9r3uoyvUVJUSAWs2\nNTC4b1laZZiZdRv+U7yLbL8bgg/DmZkBDqAu47shmJntzAHURQb7bghmZjtxAHUR347HzGxnDqAu\nUlZcSFVZkQPIzCzhAOpCNb4bgpnZdg6gLlTTp5SV/k0gMzPAAdSlPAIyM9vBAdSFfDseM7MdHEBd\nqKaqlI3129jcsC3tUszMUucA6kItd0NYtaEh5UrMzNLnAOpCQ5J7wC1d759lMDNzAHWhYdXlACxx\nAJmZOYC60vCWAFq3NeVKzMzS5wDqQuUlhQyoLPEvo5qZ4QDqcsOry3l7rQPIzMwB1MWGVZexxCMg\nMzMHUFcbXl3B2+u2EBFpl2JmlioHUBcbVl3G5oYm1m9pTLsUM7NUOYC62Ij+mSvhFvs8kJnlOQdQ\nF9v+XSCfBzKzPOcA6mIt3wXypdhmlu8cQF1sQGUJZcUFHgGZWd5zAHUxSQyrLvcIyMzyngMoBcOr\ny3nbt+MxszznAErB8OpyH4Izs7znAErBsOpyVm6oZ2tjU9qlmJmlxgGUgpYr4Zat92E4M8tfDqAU\nDPOl2GZmDqA0tNwNwQFkZvmsXQEkaZSkIknlkr4s6bBcF9abDelbhoR/lsHM8lp7R0DXAwOAbwN1\nwMycVZQHSooKGFxV6ivhzCyvtTeA+gDrgT4RcTPgT859NNxfRjWzPNfeAHoKmAfMkHQg8HLuSsoP\nw/xdIDPLc0XtWSgivpY9LenLuSknfwzvX84fXlhOc3NQUKC0yzEz63LtvQjhC5KGSRon6SHAAbSP\nhleX09DUzKpN9WmXYmaWivYegpsaEUuALwJ/B3w6dyXlh+0/y+Ar4cwsT7U3gMolnQwsAxoA/9m+\nj0YOqADgzTWbU67EzCwd7Q2gfwM+BFwBjAKuzllFeWLkwAoKBK+t3JR2KWZmqWhXAEXEHOAyYBxQ\nFxG/3dM6kiZLekzSw5J+I6lC0nskzZX0uKS7JfVPlq2WdJukRyU9IWlc0i5JlyVtz0j63C7bf1LS\nfEk/zmrfqz7SUlpUyIj+FbyxygFkZvmpvRchnAI8BnwVeETSqXtYfgDwLeDDEfFB4G/APwGzgAsi\nYgJwH/C9ZJXLgTkR8YFkuZuS9n8EDgImACcAF0vaT9Io4D+BjwLjgRGS/kGSOtBHasYMquT1lRvT\nLsPMLBXtPQR3ETAhIiaTCYOLdrdwRKwBjo+IljPsRcBWYG1EPJu0/QI4JXn+98k0EfEcUCfpXcCp\nwA2RUQf8Lln2JOC2iFgfEUHmTg1nAAd3oI+dSJomqVZS7cqVK9v58nTMATWVvLFqE5ldMDPLL+0N\noKaIWAcQEevJXIiwWxGxVVKZpJ8C5cDzZC5iaJnfwI7vIRVlhRXAUmAwMDB7nb1tb2cfu9Z9Q0SM\nj4jxNTU1e9rNfXLAoEo2NzSxvM7XdJhZ/mlvAC2SdLGk90r6JrB4TytIGgHcDtwfEV8iEwyDs+aX\nsiPItiTTLYYCy5PH4I62t7OP1BxQ0weA11f5MJyZ5Z/2BtB0oAT4LlAFTNvdwpLKyJxjmRYR9wFE\nxGtAH0lHJot9nsw5GoB7gHOSdQ8DqiLideBOMt89QlIF8MlkndnAJyRVJetPBe7sYB+pGTOoEoDX\nfSWcmeWh3d6KR9JjQMsJipb7xXwUmAR8YDerTgIOA27JXBcAwJ+BKcB/SWoGVgNnJ/O+A/xS0tlJ\nf1OT9tuAYyXVJu0/iIilSW2XAg9JagAejojbknX2to/UDO1bRnlxoQPIzPLSnu4F95mObDQi7gGG\ntzH72FaWXwuc3kp7AF9vo49bgVtbaX9mb/pIU0GBGD2okjd8CM7M8tBuAygi/tZVheSrA2oqef7t\n9WmXYWbW5fyT3Ck7YFAlb63ZTMO25rRLMTPrUg6glB1QU0lzwJtrfB7IzPKLAyhlYwYll2L7QgQz\nyzMOoJRtvxTb94QzszzjAEpZv/JiBvUp4Q2PgMwszziAuoEDBvXx3RDMLO84gLqBA2oqfQ7IzPKO\nA6gbGDOoktWbGli/uTHtUszMuowDqBvwTUnNLB85gLqBA2oyV8K9usIBZGb5wwHUDYweWEl5cSEv\nLq1LuxQzsy7jAOoGCgvEoftV8cISB5CZ5Q8HUDdxxLC+vLSkzj/PbWZ5wwHUTRy+Xz821G/jrTVb\n9rywmVkv4ADqJo4Y1heAF5b4pxnMLD84gLqJQ4ZWUVggX4hgZnnDAdRNlBUX8q6aSl+IYGZ5wwHU\njRwxrJ8PwZlZ3nAAdSOH79eX5XX1rNpYn3YpZmY55wDqRlouRHjRh+HMLA84gLqRw1sCyBcimFke\ncAB1I9UVJQyvLveFCGaWFxxA3czhw/r6QgQzywsOoG7miGF9eWPVJjY3bEu7FDOznHIAdTOH79eX\nCHhp6Ya0SzEzyykHUDdzxPB+ADz/tg/DmVnv5gDqZob1K2No3zKeWrQm7VLMzHLKAdTNSOLoMQN4\n8o01/mkGM+vVHEDd0PvHDGDFhnreXLM57VLMzHLGAdQNHTNmAABPvOHDcGbWezmAuqEDa/rQv6KY\npxxAZtaLOYC6oYICMX70AJ70hQhm1os5gLqpY8YM4G+rN7O8bmvapZiZ5YQDqJt6/+jMeaAnfRjO\nzHopB1A3dcSwvlSUFPr7QGbWazmAuqmiwgLeN6q/R0Bm1ms5gLqxo0cP4C/LNrBuc0PapZiZdbqc\nBJCkMyX9RtKbWW0jJd0v6VFJcySNStpLJN2YtC+QNClrnQskPSXpGUnfyGr/kKTHJD0p6RZJJR3t\nozt7f/J9oNpFa1OuxMys8+VqBLQS+GegJKvtRuCaiPgA8EPg6qT9m8C6pP004FpJpZKOAz4LHAcc\nDZwhabykPsBM4MyIOBpYCnylI33kYsc707j9qykpKuCRV1elXYqZWafLSQBFxNyI2P6pKakCODQi\n7k7mzwaOTEYupwLXJ+1vA48BxyftMyOiISIagBnAx8kE0qPJsgDXkQmnjvTRrZUVF3L8gYN44C/L\nfV84M+t1uuocUDWZUVG2FcDA5LEsq30pMLgD7R3p4x0kTZNUK6l25cpdN9f1PnLYYN5as4VXVmxM\nuxQzs07VVQG0ikwIZKtJ2pezcxgMTdr2tr0jfbxDRNwQEeMjYnxNTc0edyzXPnLoEAD++GKr5ZqZ\n9VhdEkDJIbSFkk4CSC4CeCEiGoE7gXOT9iHABGBe0v4FScWSCoGzgbuSecdI2i/Z/BeBOzvYR7c3\ntF8ZY0f0408vOYDMrHcp6sK+zgdukvQdoB44J2m/CrhR0hOAgPMjoh6olXQX8ATQBMyKiFoASecB\n90iqB14FvtfBPnqESYcN4Yo/vczKDfXUVHX7ayfMzNpFPrndtvHjx0dtbW3aZfDCkvWcctUj/PAf\nxjL5/funXY6Z2W5Jmh8R4/e0nL+I2gMcvl9fhvUr448+DGdmvYgDqAeQxKTDh/DIK6vY2tiUdjlm\nZp3CAdRDTDpsCFsam3j0NX8p1cx6BwdQD3HMAQPoU1rE7IXL9rywmVkP4ADqIUqLCjl17H7MXriU\njfXb0i7HzGyfOYB6kE+N35/NDU3Mfm5p2qWYme0zB1APctTIat5VU8mva99KuxQzs33mAOpBJDF5\n/P7M/9taXvW94cysh3MA9TCfPGoEhQXit/M9CjKzns0B1MPUVJXy4UMHc9v8t2lsak67HDOzDnMA\n9UCTx+/Pqo31zPlr+j8XYWbWUQ6gHmjiITXUVJVy82OL0i7FzKzDHEA9UHFhAeccN5qHX1nFs2+t\nS7scM7MOcQD1UJ+fMIp+5cVc/eCraZdiZtYhDqAeqqqsmHOOG80fX1zOS0vr0i7HzGyvOYB6sCkf\nGE2f0iKu8SjIzHogB1APVl1RwuePHcW9C5fy2kp/MdXMehYHUA/3xePHUFpUwFUPvJJ2KWZme8UB\n1MMN6lPKuccfwJ3PLOHx11enXY6ZWbs5gHqB8z90ICP6l/PvdzxPwzbfHcHMegYHUC9QXlLI9z5+\nBK+u2MgvHnk97XLMzNrFAdRLfPjQIXzsiCFc9cArvLVmc9rlmJntkQOoF7nktCMokPj3O56nuTnS\nLsfMbLccQL3IsOpyLjr5UOa+vJJr576WdjlmZrvlAOplzpowio+PG8aP//BXHnllVdrlmJm1yQHU\ny0jisk++mwMH9+GCWU+zdP2WtEsyM2uVA6gXqigp4tqz3kd9YxPTb5nPhq2NaZdkZvYODqBe6l01\nfbjqs+/lxSV1nDPzKTbVb0u7JDOznTiAcqFxK7z1ZNpV8JHDhvCzz76Xp99axzk3PcXmBoeQmXUf\nDqBceOhymHESPHVj2pVw8rv344pPj6N20RrOmfkU6zY3pF2SmRngAMqN4/4FDpwE9/4rzP4WNKU7\n8jj9PcO44tPjePrNdZx+9Tz+umxDqvWYmYEDKDfK+sJn/weO/TI8eT386lOwcUWqJX183HBmTZ/A\n1sYmPvHzedy3cGmq9ZiZOYBypaAQPvZ/4bSrYNE8uOZoeO63EOndoeCokf25+yvHc/CQKs67dQFf\nnfU0qzfWp1aPmeU3B1Cuve9s+NIjMPBA+P25MOsfYXV6dykY0reMX0+fwL985CDuXbiUST+Zy+/m\nL/ate8ysyylS/Iu8uxs/fnzarh9xAAAJUklEQVTU1tZ2zsaam+Dxn8ODl0JTA7z383Dit6DvsM7Z\nfge8vHwDF972HAveXMchQ6r46qSD+NgRQykoUGo1mVnPJ2l+RIzf43IOoLZ1agC12LAcHv4R1M4E\nFcC7z4T3nwvDj+rcftqpuTm4+7kl/PSBV3h95SYOHVrFF44dzcfHDaOytCiVmsysZ3MAdYKcBFCL\ntX+DeVfCs7+Gxk0w7CgY+2k4/PRURkVNzcFdz77NtXNe4+XlG6ksKeT0ccM5bex+HD1mAEWFPlpr\nZu3jAOoEOQ2gFlvXZ0Jo/k2w4oVM2/7HwIEfhTEnZEZGhcW5rSFLRLDgzbX86om3uHfhErY2NlNd\nUcxHDh3CBw8axIQDBjK0X1mX1WNmPY8DqBN0SQBlW/UKvHgnvHQXLH0OCCjpkwmhYUdl/h1yJPQf\nnbnKLsc2N2zjoZdX8r8vLOeBl5ZTtzXzfabRAysYt381Rw7vx5HD+3HIkCr6V5bkvB4z6xkcQK2Q\nNBn4BlAIzImIr+9u+S4PoGyb18Cih+GNh2BxLSx/AZqTm4oWlsKgg2HgAVA9EqpHQb/9oe9+ULUf\nVAyCgs49ZNbUHLy0tI7HX1/NE2+sYeHi9Syr27p9/sDKEt5V04eRAysY0b+c4dXl7NevnMF9SxlS\nVUbf8iIkX9xglg8cQLuQNAr4A3A0UAfMAn4TEbe1tU6qAbSrxq2ZQ3QrXoKVf4GVf4U1r8O6NzNX\n1WVTAZQPgMpBmX/L+u14lPbJjKpK+kBJBRSXQ3EFFJUlj5JMwBWWZA79FRZnnhcU7fIoZOXGRp5f\nWsdrKzby2sqNvLpiI2+t2cLyDVvf8XWn4kJRXVHCgIoS+lUU07esmL7lRVSVFlFRWkSf0iLKiwsp\nLymkvLiQsuICSosKKS0qoCR5FBdmHkUFoqhQ258X7vqQKJB8NZ9ZStobQPl0mdNJwG0RsR5A0vXA\nOUCbAdStFJfB8PdlHtmam2HjMqhbAhuWQt1S2LQCNq2CzaszI6n1izMjqK3roWEDRHOnlFQDfEgF\nfEiFmUOCKgAVEn1FMwU0BzQjmkM0QWZ6o2jekHm+vS1ES15F9nNEoOR51i4jGoCGZJk9CekdS21v\nUUtf2ZPtC642l9qn3HNoWvew6n1f5f2nnpvTPvIpgAYCy7KmlwKDd11I0jRgGsDIkSO7prJ9UVCQ\nuWquvVfORUDjFmjYBNu27Hje1ADbtmZGWk0NWY/GzKG/psbMd5mat2Wmm5shmjJt0ZQJteYmiEDR\nTGE0URgBRKadSO4Ckf0v26ebItjW1ERTczNNzUFTUzPNETQ1B83NQUTQnDwyzyGimUg2F9FM0PI8\nSHrYfueJCAh2PIcd3Wv7nKyXieyF3vF014X3vEy7tLF2fhyksG6mot/AnPeRTwG0HBiTNT00adtJ\nRNwA3ACZQ3BdU1oXkjKH3koq0q5kJ4XJw8zyRz59uWM28AlJVcn0VODOFOsxM8treTMCioilki4F\nHpLUADy8uwsQzMwst/ImgAAi4lbg1rTrMDOz/DoEZ2Zm3YgDyMzMUuEAMjOzVDiAzMwsFQ4gMzNL\nRd7cC64jJK0E/tbB1QcBqzqxnJ4iH/c7H/cZ8nO/83GfYe/3e1RE1OxpIQdQjkiqbc/N+HqbfNzv\nfNxnyM/9zsd9htzttw/BmZlZKhxAZmaWCgdQ7tyQdgEpycf9zsd9hvzc73zcZ8jRfvsckJmZpcIj\nIDMzS4UDyMzMUuEAygFJkyU9KWm+pB+nXU+uJPv5mKSHJf1GUoWk90iaK+lxSXdL6p92nbkg6TuS\n5iTPe/0+S9pf0l2SHpT0R0lHSRop6X5Jj0qaI2lU2nV2JknfTv4/nifpt5KqeuN7LenM5P/fN7Pa\nWn1vJZVIujFpXyBp0j51HslPHPvROQ9gFPBXoB8g4NfAP6RdVw72cwBQC5Qn05cD/wK8BLwnaftn\n4Gdp15qDfR8PzADmJO9xPuzzfcDYrPe+BvgjcFrS9vfA3WnX2Yn7+27gCaAwmb4C+GZvfK+BE8l8\n0XRZVlur7y1wMfDj5Plw4BWgtKN9ewTU+U4CbouI9ZF5l64Hzki5pk4XEWuA4yNiS9JUBGwF1kbE\ns0nbL4BT0qgvVySVA1cCFyZNB9P793koUAZMlfQQ8H+BTcChEXE3QETMBo6UVJJepZ1qFVDPjt9M\nKwTq6IXvdUTMjYjtdzmQVEHb7+2pZD7TiIi3gceA4zvatwOo8w0ElmVNLwUGp1RLTkXEVkllkn4K\nlAPPk7XvEdFA7/vRw8uBKyNiRTK90/vdS/d5JPBe4JcRcQKwhEwIr9xluRVkXo8eLyKWAlcDP5d0\nEbCW/PjvG6Catt/bTv18cwB1vuXs/IYMTdp6HUkjgNuB+yPiS2T+wxycNb8UaEipvE4n6WNA/4j4\nXVbzTu93b9vnxDrgxYh4Opn+HXAQ7wybGnrJfdIkfQg4ISK+GBGXAS8AX6L3v9eQeQ/bem879fPN\nAdT5ZgOfkFSVTE8F7kyxnpyQVAbcBEyLiPsAIuI1oI+kI5PFPk/m3EFvcSpQI+kOSXcARwKX0Lv3\nGeBVoEzSocn0JGA+sFDSSQDJyegXIqIxpRo726FAadZ0CZnRTm9/r1tGdm29t3cC5ybtQ4AJwLyO\n9uUvouaApM8B3yDz19HDEfGNlEvqdJJajgW/ktX8Z+Au4FqgGVgNnB0Ra7u+wtyTNCciJkoaRy/f\nZ0ljgZ+S+RBeQeYPq2oyf4SUkDlfck5EdPTu8d2KpErg58D7gPXAFjIfvNX00vda0rKIGJo8H0Ur\n721yHuhGMuc+BXw7Iv7U4T4dQGZmlgYfgjMzs1Q4gMzMLBUOIDMzS4UDyMzMUuEAMjOzVDiAzLop\nSQdKujm528T4tOsx62y+DNusm5M0EZgSEVNSLsWsUzmAzLopSaOBWWS+CHgI8Jfki6+nAf8ObAPu\niYjLkpA6C6gEfh8Rv02laLO90BtvpGfW21xCMgKSVA38DDgqItZI+r2k9ybLfQQ4NiKWtbkls27E\nAWTWsxxIMsqRBNCXzOhoGfC0w8d6EgeQWfcXZO7JBfAa8BZwSkRsknQImXuVHUrvvDOz9WK+Cs6s\n+3sJOFrSbDI/ivYfwJ8kPQx8n8w5IrMexxchmJlZKjwCMjOzVDiAzMwsFQ4gMzNLhQPIzMxS4QAy\nM7NUOIDMzCwVDiAzM0vF/wdHjDkVTkfhUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(SLR.loss, label=\"loss\")\n",
    "plt.plot(SLR.val_loss, label=\"val_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.title('madel loss (StandardScaler)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自作のコードでのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(itera, alpha, w, X, y):\n",
    "    X = np.insert(X , 0, 1, axis=1)\n",
    "    print(X)\n",
    "    m = len(y)\n",
    "    cost = np.zeros(m)\n",
    "    for i in range(itera):\n",
    "        cost[i] = (1/2*m)* np.sum(np.square(np.dot(X, w)-y))\n",
    "        w = w -alpha* (1/m)* np.dot(X.T, (np.dot(X, w)-y))\n",
    "    \n",
    "    return [cost, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         7.44424865 7.60240134]\n",
      " [1.         7.14045304 7.58882988]\n",
      " [1.         7.48773376 7.60140233]\n",
      " ...\n",
      " [1.         7.75790621 7.57095858]\n",
      " [1.         6.98286275 7.57558465]\n",
      " [1.         7.13568735 7.58324752]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X3 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y3 = df['SalePrice']\n",
    "X3 = X3.apply(np.log)\n",
    "y3 = y3.apply(np.log)\n",
    "X3 = X3.values\n",
    "y3 = y3.values\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "itera = 1000\n",
    "\n",
    "init_theta = np.zeros(3).reshape(3,)\n",
    "\n",
    "cost , w = grad(itera, alpha, init_theta, X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEOCAYAAACJlmBtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4EAwM7OUA8HMzAAHgpmZpf4fr/pRDrMj9JYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost)\n",
    "plt.xlabel('Cost')\n",
    "plt.ylabel('Iteration')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
