{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X1 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y1 = df['SalePrice']\n",
    "X1 = X1.apply(np.log)\n",
    "y1 = y1.apply(np.log)\n",
    "X1 = X1.values\n",
    "y1 = y1.values\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # １の列の追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Xの列数分のランダムの重み作成\n",
    "    w = np.random.rand(n)\n",
    "\n",
    "    yp = X @ w.T\n",
    "    \n",
    "    return yp, w, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp, w1, X1 = _linear_hypothesis(X1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gradient_descent( X, y, w, yp):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    X:配列\n",
    "    y:正解値\n",
    "    w: _linear_hypothesisで求めた重み\n",
    "    yp: _linear_hypothesisで求めた予測値\n",
    "    \n",
    "    \"\"\"\n",
    "    # yを配列にする\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # 誤差求める関数\n",
    "    def error(yp , y):\n",
    "        yd = yp - y\n",
    "        return yd\n",
    "\n",
    "    #正解との誤差\n",
    "    yd = error(yp, y)\n",
    "    \n",
    "    # パラメータ\n",
    "    alpha = 0.00001\n",
    "    \n",
    "    #  学習回数\n",
    "    epoch = 1000\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    \n",
    "    # 勾配降下法\n",
    "    for k in range(epoch):\n",
    "        w = w - alpha* (X.T @ yd) / M\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = _gradient_descent(X1, y1_train, w1, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55405362, 0.46710499, 1.157662  ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 hθ(x)の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Xの列に１の値を追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    y = X @ w\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X1_test, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.9214118 , 12.58645848, 12.67689855, 12.90409814, 12.57346446,\n",
       "       12.60281333, 12.49721161, 12.82507988, 12.77216881, 12.48386909,\n",
       "       12.82114949, 12.96337609, 12.6055666 , 12.71948493, 12.89672595,\n",
       "       12.60550534, 12.47452888, 12.69167484, 12.89164374, 12.69063462,\n",
       "       12.65759051, 12.78752154, 12.83571271, 12.74646387, 12.78848137,\n",
       "       12.53683815, 12.8514661 , 12.85486231, 12.75835516, 12.61534308,\n",
       "       12.74265978, 12.88800114, 12.79905051, 12.64136308, 12.83331983,\n",
       "       12.73851234, 12.72614407, 12.88290943, 12.80932342, 12.61910022,\n",
       "       12.5718038 , 12.6708182 , 12.7243162 , 12.82377425, 12.85173248,\n",
       "       12.88690242, 12.58980696, 12.46893871, 12.78485805, 12.77415164,\n",
       "       12.80126768, 12.65011944, 12.52828361, 13.03213859, 12.50527581,\n",
       "       12.95150781, 12.93465484, 12.70388641, 12.83595131, 12.74815819,\n",
       "       12.61823846, 12.79375254, 12.63699671, 12.68371696, 12.55153999,\n",
       "       12.70889263, 12.83133055, 12.76792553, 12.69350647, 12.59592569,\n",
       "       12.9036121 , 12.69217575, 12.7245431 , 12.84377479, 12.75689589,\n",
       "       12.71053246, 12.38408617, 12.85082495, 12.72740994, 12.73067182,\n",
       "       12.60485648, 12.82369489, 12.7093064 , 12.78903767, 12.68223523,\n",
       "       13.05848194, 12.85453465, 12.48678076, 12.78465973, 12.9144246 ,\n",
       "       12.96597626, 12.5818862 , 12.73908182, 12.67521715, 12.76634093,\n",
       "       12.8041517 , 12.73345501, 12.58897949, 12.78835808, 12.73489608,\n",
       "       12.61123638, 12.3478357 , 12.82355988, 12.51365033, 12.98908796,\n",
       "       12.67036641, 12.78942672, 12.80611443, 12.68635061, 12.69350647,\n",
       "       13.02565091, 12.51131589, 12.52155252, 12.6959175 , 12.66394917,\n",
       "       12.86609757, 12.71443994, 12.80906516, 12.77788883, 12.84942177,\n",
       "       12.8677609 , 12.67669359, 12.57196714, 12.76562372, 12.70057978,\n",
       "       12.89375668, 12.59597911, 12.82216317, 12.75835516, 12.76865626,\n",
       "       12.78419771, 12.78723595, 12.96029894, 12.75101574, 12.62797781,\n",
       "       12.60471049, 12.40983445, 12.91209075, 12.52312189, 12.76782991,\n",
       "       12.77104648, 12.87431266, 12.51131589, 12.50921993, 12.50733077,\n",
       "       12.39662738, 12.82018138, 12.72103966, 12.60903452, 12.83925764,\n",
       "       12.82884815, 12.52809559, 12.96005403, 12.66553606, 12.70177323,\n",
       "       12.55879212, 12.80299395, 12.60604326, 12.49734665, 12.84526066,\n",
       "       12.55582332, 12.98698814, 12.92463791, 12.64747462, 12.72560838,\n",
       "       12.92614951, 12.58095931, 12.69065576, 12.78392002, 12.59447939,\n",
       "       12.72333342, 12.75719981, 12.85722328, 12.71352092, 13.01083925,\n",
       "       12.71157815, 12.89546937, 12.62201837, 12.84113032, 12.59409879,\n",
       "       12.9390483 , 12.8356106 , 12.80452808, 12.68437583, 12.56244993,\n",
       "       12.78319273, 12.83695076, 12.63782736, 12.66565171, 12.80760927,\n",
       "       12.9191993 , 12.54502794, 12.91302613, 12.73983728, 12.79767176,\n",
       "       12.58674658, 12.78151179, 12.74127903, 12.80604208, 12.64448062,\n",
       "       12.79837218, 12.53654635, 12.4201004 , 12.56898729, 12.51439184,\n",
       "       12.80647754, 12.51474183, 13.0172304 , 12.82896837, 12.67107331,\n",
       "       12.5878772 , 12.71831154, 12.84789481, 12.63823809, 12.53621361,\n",
       "       12.7659143 , 12.82624899, 12.56839347, 12.59876121, 12.81507046,\n",
       "       12.61532767, 12.457291  , 12.76392002, 12.807056  , 12.7998518 ,\n",
       "       12.79263369, 12.79272503, 12.95411281, 13.00389733, 12.7553484 ,\n",
       "       12.80878224, 12.605352  , 12.58629224, 12.568451  , 12.62504398,\n",
       "       12.85768462, 13.00522138, 12.72712171, 12.86803408, 12.57667943,\n",
       "       12.82849716, 12.63345328, 12.80709381, 12.64498732, 12.71430566,\n",
       "       12.7928296 , 12.67632858, 12.82972308, 12.59775325, 12.82033649,\n",
       "       12.68998729, 12.51298411, 12.88710701, 12.81870473, 12.78950675,\n",
       "       12.83234167, 12.90232999, 12.89638692, 12.66149279, 12.58663703,\n",
       "       12.83159725, 12.87922453, 12.67027899, 12.87875652, 12.55754259,\n",
       "       12.58557881, 12.46744674, 12.75587563, 13.001393  , 12.90988532,\n",
       "       12.74459891, 12.90445779, 12.6678667 , 12.53829039, 12.99307438,\n",
       "       12.8843689 , 12.67951794, 12.75221606, 12.81534084, 12.75422282,\n",
       "       12.81680219, 12.62416586, 12.51150138, 12.75835516, 12.74745314,\n",
       "       12.69152281, 12.46337767, 12.66803042, 12.8077953 , 12.77435641,\n",
       "       12.71579523, 12.66193983])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\ \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n-1} (y_i – \\hat{y_i})^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    len_y = len(y_pred)\n",
    "    \n",
    "    yd = y_pred - y\n",
    "    mse = sum(yd**2) / len_y\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6059627592536255"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y1_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_func(y, y_pred):\n",
    "    \"\"\"\n",
    "    目的関数を求める関数\n",
    "    ------------------------------\n",
    "    parameter\n",
    "    \n",
    "    y_pred: yの予測値\n",
    "    y: 正解値\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    m = len(y_pred)\n",
    "    j = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        j [i] = (1/2*m)* np.sum(np.square(y_pred - y))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的なCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_iter, alpha, bias=None, verbose=None):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "        self.iter = num_iter\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "            \n",
    "        \"\"\"\n",
    "        y = y.values\n",
    "        y_val = y_val.values\n",
    "        \n",
    "        X = X.values\n",
    "        X_val = X_val.values\n",
    "        \n",
    "        \n",
    "        if not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "\n",
    "        n = X.shape[1]\n",
    "        self.w = np.zeros(n,)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            # 予測関数の呼び出し\n",
    "            y_pred1 = self._linear_hypothesis(X)\n",
    "            #y_pred2 = self._linear_hypothesis(X_val)\n",
    "\n",
    "            error1 = y_pred1 - y\n",
    "            #error2 = y_pred2 - y_val\n",
    "\n",
    "            # 最急降下法で学習\n",
    "            self._gradient_descent1(X, error1, i)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"loss: \", self.loss[i])\n",
    "            \n",
    "        if any(y_val):\n",
    "            n = X.shape[1]\n",
    "            self.w = np.zeros(n,)\n",
    "            for i in range(self.iter):\n",
    "                y_pred2 = self._linear_hypothesis(X_val)\n",
    "                error2 = y_pred2 - y_val\n",
    "                self._gradient_descent2(X_val, error2, i)\n",
    "\n",
    "                #学習過程の表示\n",
    "                if self.verbose:\n",
    "                    print(\"val_loss: \", self.val_loss[i])\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        y_pred = np.dot(X, self.w)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def _gradient_descent1(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "    def _gradient_descent2(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.val_loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = np.dot(X, self.w) \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#　データ作成\n",
    "df = pd.read_csv('train.csv')\n",
    "X2 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y2 = df['SalePrice']\n",
    "X2 = X2.apply(np.log)\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# priceデータの対数変換\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# テストデータとtrainデータの作成\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLR = ScratchLinearRegression(num_iter=100, alpha=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  4217339.21574309\n",
      "loss:  3329695.4198566927\n",
      "loss:  2628918.9296311135\n",
      "loss:  2075670.3243695712\n",
      "loss:  1638891.9425377666\n",
      "loss:  1294064.400705684\n",
      "loss:  1021830.151199485\n",
      "loss:  806906.8092473629\n",
      "loss:  637229.247166375\n",
      "loss:  503272.2954113681\n",
      "loss:  397516.0290177529\n",
      "loss:  314023.62227287586\n",
      "loss:  248108.07368291946\n",
      "loss:  196069.09220578318\n",
      "loss:  154985.37435685872\n",
      "loss:  122550.61126690112\n",
      "loss:  96944.01963544781\n",
      "loss:  76728.1285879544\n",
      "loss:  60768.08450694843\n",
      "loss:  48167.94386000915\n",
      "loss:  38220.37770098269\n",
      "loss:  30366.96441068953\n",
      "loss:  24166.841634404467\n",
      "loss:  19271.957632834627\n",
      "loss:  15407.532733988955\n",
      "loss:  12356.634052276817\n",
      "loss:  9947.997547277591\n",
      "loss:  8046.413790225449\n",
      "loss:  6545.137724876954\n",
      "loss:  5359.89633021259\n",
      "loss:  4424.157793690501\n",
      "loss:  3685.396621071129\n",
      "loss:  3102.145017630294\n",
      "loss:  2641.6650144220043\n",
      "loss:  2278.11065995933\n",
      "loss:  1991.0771084386934\n",
      "loss:  1764.4551548080221\n",
      "loss:  1585.526913820385\n",
      "loss:  1444.2518772923358\n",
      "loss:  1332.7032710294886\n",
      "loss:  1244.6230702405526\n",
      "loss:  1175.070693381451\n",
      "loss:  1120.1456531885742\n",
      "loss:  1076.7685953879065\n",
      "loss:  1042.5084332705037\n",
      "loss:  1015.4458740039472\n",
      "loss:  994.0656754691139\n",
      "loss:  977.17158525424\n",
      "loss:  963.8191867445535\n",
      "loss:  953.2628824946131\n",
      "loss:  944.9140386938959\n",
      "loss:  938.3079410853355\n",
      "loss:  933.0777073443306\n",
      "loss:  928.9336914398579\n",
      "loss:  925.6472238021888\n",
      "loss:  923.0377745205801\n",
      "loss:  920.9628189525009\n",
      "loss:  919.3098368310737\n",
      "loss:  917.9899957253706\n",
      "loss:  916.9331642625432\n",
      "loss:  916.0839751695147\n",
      "loss:  915.3987171255743\n",
      "loss:  914.8428809440716\n",
      "loss:  914.3892223334778\n",
      "loss:  914.0162324871636\n",
      "loss:  913.7069306455782\n",
      "loss:  913.4479108489783\n",
      "loss:  913.2285893683623\n",
      "loss:  913.0406105677099\n",
      "loss:  912.8773778444722\n",
      "loss:  912.7336833167561\n",
      "loss:  912.6054154689925\n",
      "loss:  912.4893283442244\n",
      "loss:  912.3828593261674\n",
      "loss:  912.2839852819144\n",
      "loss:  912.1911089895568\n",
      "loss:  912.1029694751396\n",
      "loss:  912.0185712255357\n",
      "loss:  911.937128303466\n",
      "loss:  911.8580202274652\n",
      "loss:  911.7807571400197\n",
      "loss:  911.7049523085311\n",
      "loss:  911.6303004153982\n",
      "loss:  911.5565604184821\n",
      "loss:  911.4835420197905\n",
      "loss:  911.411094982797\n",
      "loss:  911.3391006986689\n",
      "loss:  911.2674655279903\n",
      "loss:  911.1961155441832\n",
      "loss:  911.1249923835478\n",
      "loss:  911.0540499689599\n",
      "loss:  910.9832519232883\n",
      "loss:  910.9125695273499\n",
      "loss:  910.8419801077503\n",
      "loss:  910.7714657641258\n",
      "loss:  910.7010123643162\n",
      "loss:  910.6306087510852\n",
      "loss:  910.5602461158359\n",
      "loss:  910.4899175041705\n",
      "loss:  910.4196174255483\n",
      "val_loss:  263649.95874410565\n",
      "val_loss:  208269.19766511786\n",
      "val_loss:  164524.25359385446\n",
      "val_loss:  129970.37407767339\n",
      "val_loss:  102676.46329423931\n",
      "val_loss:  81117.15982259931\n",
      "val_loss:  64087.58949875758\n",
      "val_loss:  50636.029189348585\n",
      "val_loss:  40010.718295921586\n",
      "val_loss:  31617.845469807904\n",
      "val_loss:  24988.362561191818\n",
      "val_loss:  19751.77114942365\n",
      "val_loss:  15615.416674928194\n",
      "val_loss:  12348.13299375812\n",
      "val_loss:  9767.323305832642\n",
      "val_loss:  7728.755454874869\n",
      "val_loss:  6118.5012948544045\n",
      "val_loss:  4846.569642161662\n",
      "val_loss:  3841.876981356175\n",
      "val_loss:  3048.2748547257056\n",
      "val_loss:  2421.411920275895\n",
      "val_loss:  1926.2553094855373\n",
      "val_loss:  1535.1327621334945\n",
      "val_loss:  1226.1861199224306\n",
      "val_loss:  982.1497500369072\n",
      "val_loss:  789.3856289772939\n",
      "val_loss:  637.1211608610795\n",
      "val_loss:  516.8471345074004\n",
      "val_loss:  421.84217322022073\n",
      "val_loss:  346.79710042254686\n",
      "val_loss:  287.51822824351575\n",
      "val_loss:  240.6929868894503\n",
      "val_loss:  203.70479663961189\n",
      "val_loss:  174.48683630639556\n",
      "val_loss:  151.406535787816\n",
      "val_loss:  133.17433740278955\n",
      "val_loss:  118.77162699757653\n",
      "val_loss:  107.39380714360297\n",
      "val_loss:  98.40533098566681\n",
      "val_loss:  91.30418373871377\n",
      "val_loss:  85.693826827576\n",
      "val_loss:  81.26103672521768\n",
      "val_loss:  77.75839997924463\n",
      "val_loss:  74.99048613458798\n",
      "val_loss:  72.80292580510499\n",
      "val_loss:  71.07378350550941\n",
      "val_loss:  69.70674310120896\n",
      "val_loss:  68.62572503454626\n",
      "val_loss:  67.77063450294156\n",
      "val_loss:  67.09400296939643\n",
      "val_loss:  66.55833531106805\n",
      "val_loss:  66.13401434727048\n",
      "val_loss:  65.79764563826274\n",
      "val_loss:  65.53075005138524\n",
      "val_loss:  65.31873102661268\n",
      "val_loss:  65.15005882558751\n",
      "val_loss:  65.01562617464936\n",
      "val_loss:  64.90823929099132\n",
      "val_loss:  64.82221584716463\n",
      "val_loss:  64.75306740556528\n",
      "val_loss:  64.69724857527291\n",
      "val_loss:  64.6519588724878\n",
      "val_loss:  64.61498621123661\n",
      "val_loss:  64.58458327758615\n",
      "val_loss:  64.559369878349\n",
      "val_loss:  64.53825580689171\n",
      "val_loss:  64.52037991528292\n",
      "val_loss:  64.50506198773478\n",
      "val_loss:  64.49176472571318\n",
      "val_loss:  64.48006372019645\n",
      "val_loss:  64.46962373293547\n",
      "val_loss:  64.46017996115887\n",
      "val_loss:  64.45152323867163\n",
      "val_loss:  64.44348834628875\n",
      "val_loss:  64.43594477831401\n",
      "val_loss:  64.42878944903568\n",
      "val_loss:  64.42194093162843\n",
      "val_loss:  64.4153349074969\n",
      "val_loss:  64.408920571737\n",
      "val_loss:  64.40265779383138\n",
      "val_loss:  64.39651487489901\n",
      "val_loss:  64.39046677616017\n",
      "val_loss:  64.38449371960952\n",
      "val_loss:  64.3785800826981\n",
      "val_loss:  64.37271352524719\n",
      "val_loss:  64.36688429980379\n",
      "val_loss:  64.36108470689368\n",
      "val_loss:  64.35530866472917\n",
      "val_loss:  64.34955136932403\n",
      "val_loss:  64.34380902602015\n",
      "val_loss:  64.33807863742128\n",
      "val_loss:  64.33235783588438\n",
      "val_loss:  64.32664475120426\n",
      "val_loss:  64.32093790609945\n",
      "val_loss:  64.31523613365651\n",
      "val_loss:  64.30953851211976\n",
      "val_loss:  64.30384431338227\n",
      "val_loss:  64.2981529622995\n",
      "val_loss:  64.29246400455054\n",
      "val_loss:  64.28677708125251\n",
      "[4.21733922e+06 3.32969542e+06 2.62891893e+06 2.07567032e+06\n",
      " 1.63889194e+06 1.29406440e+06 1.02183015e+06 8.06906809e+05\n",
      " 6.37229247e+05 5.03272295e+05 3.97516029e+05 3.14023622e+05\n",
      " 2.48108074e+05 1.96069092e+05 1.54985374e+05 1.22550611e+05\n",
      " 9.69440196e+04 7.67281286e+04 6.07680845e+04 4.81679439e+04\n",
      " 3.82203777e+04 3.03669644e+04 2.41668416e+04 1.92719576e+04\n",
      " 1.54075327e+04 1.23566341e+04 9.94799755e+03 8.04641379e+03\n",
      " 6.54513772e+03 5.35989633e+03 4.42415779e+03 3.68539662e+03\n",
      " 3.10214502e+03 2.64166501e+03 2.27811066e+03 1.99107711e+03\n",
      " 1.76445515e+03 1.58552691e+03 1.44425188e+03 1.33270327e+03\n",
      " 1.24462307e+03 1.17507069e+03 1.12014565e+03 1.07676860e+03\n",
      " 1.04250843e+03 1.01544587e+03 9.94065675e+02 9.77171585e+02\n",
      " 9.63819187e+02 9.53262882e+02 9.44914039e+02 9.38307941e+02\n",
      " 9.33077707e+02 9.28933691e+02 9.25647224e+02 9.23037775e+02\n",
      " 9.20962819e+02 9.19309837e+02 9.17989996e+02 9.16933164e+02\n",
      " 9.16083975e+02 9.15398717e+02 9.14842881e+02 9.14389222e+02\n",
      " 9.14016232e+02 9.13706931e+02 9.13447911e+02 9.13228589e+02\n",
      " 9.13040611e+02 9.12877378e+02 9.12733683e+02 9.12605415e+02\n",
      " 9.12489328e+02 9.12382859e+02 9.12283985e+02 9.12191109e+02\n",
      " 9.12102969e+02 9.12018571e+02 9.11937128e+02 9.11858020e+02\n",
      " 9.11780757e+02 9.11704952e+02 9.11630300e+02 9.11556560e+02\n",
      " 9.11483542e+02 9.11411095e+02 9.11339101e+02 9.11267466e+02\n",
      " 9.11196116e+02 9.11124992e+02 9.11054050e+02 9.10983252e+02\n",
      " 9.10912570e+02 9.10841980e+02 9.10771466e+02 9.10701012e+02\n",
      " 9.10630609e+02 9.10560246e+02 9.10489918e+02 9.10419617e+02]\n"
     ]
    }
   ],
   "source": [
    "SLR.fit(X2_train, y2_train, X_val= X2_val, y_val=y2_val)\n",
    "print(SLR.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SLR.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015078131898317855"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y2_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003095456683099062"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y2_val, lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAETCAYAAABwaNKCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSYcQGZmlgkHkJmZZeL/A4+nMLrSSLiiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(SLR.loss, label=\"loss\")\n",
    "plt.plot(SLR.val_loss, label=\"val_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.title('madel loss (StandardScaler)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自作のコードでのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(itera, alpha, w, X, y):\n",
    "    X = np.insert(X , 0, 1, axis=1)\n",
    "    print(X)\n",
    "    m = len(y)\n",
    "    cost = np.zeros(m)\n",
    "    for i in range(itera):\n",
    "        cost[i] = (1/2*m)* np.sum(np.square(np.dot(X, w)-y))\n",
    "        w = w -alpha* (1/m)* np.dot(X.T, (np.dot(X, w)-y))\n",
    "    \n",
    "    return [cost, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         7.44424865 7.60240134]\n",
      " [1.         7.14045304 7.58882988]\n",
      " [1.         7.48773376 7.60140233]\n",
      " ...\n",
      " [1.         7.75790621 7.57095858]\n",
      " [1.         6.98286275 7.57558465]\n",
      " [1.         7.13568735 7.58324752]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X3 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y3 = df['SalePrice']\n",
    "X3 = X3.apply(np.log)\n",
    "y3 = y3.apply(np.log)\n",
    "X3 = X3.values\n",
    "y3 = y3.values\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "itera = 1000\n",
    "\n",
    "init_theta = np.zeros(3).reshape(3,)\n",
    "\n",
    "cost , w = grad(itera, alpha, init_theta, X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEOCAYAAACJlmBtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4EAwM7OUA8HMzAAHgpmZpf4fr/pRDrMj9JYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost)\n",
    "plt.xlabel('Cost')\n",
    "plt.ylabel('Iteration')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
