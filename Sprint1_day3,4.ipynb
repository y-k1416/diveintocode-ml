{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X1 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y1 = df['SalePrice']\n",
    "X1 = X1.apply(np.log)\n",
    "y1 = y1.apply(np.log)\n",
    "X1 = X1.values\n",
    "y1 = y1.values\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # １の列の追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Xの列数分のランダムの重み作成\n",
    "    w = np.random.rand(n)\n",
    "\n",
    "    yp = X @ w.T\n",
    "    \n",
    "    return yp, w, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp, w1, X1 = _linear_hypothesis(X1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gradient_descent( X, y, w, yp):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    X:配列\n",
    "    y:正解値\n",
    "    w: _linear_hypothesisで求めた重み\n",
    "    yp: _linear_hypothesisで求めた予測値\n",
    "    \n",
    "    \"\"\"\n",
    "    # yを配列にする\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # 誤差求める関数\n",
    "    def error(yp , y):\n",
    "        yd = yp - y\n",
    "        return yd\n",
    "\n",
    "    #正解との誤差\n",
    "    yd = error(yp, y)\n",
    "    \n",
    "    # パラメータ\n",
    "    alpha = 0.00001\n",
    "    \n",
    "    #  学習回数\n",
    "    epoch = 1000\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    \n",
    "    # 勾配降下法\n",
    "    for k in range(epoch):\n",
    "        w = w - alpha* (X.T @ yd) / M\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = _gradient_descent(X1, y1_train, w1, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08856416, 0.73652155, 0.98693649])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 hθ(x)の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    # 配列化\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Xの列に１の値を追加\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    y = X @ w\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X1_test, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.12560039, 13.07534041, 12.99884215, 13.0102667 , 12.8587042 ,\n",
       "       12.86328679, 13.14518514, 12.68178257, 12.94422672, 12.98920221,\n",
       "       12.43730813, 12.60569947, 12.68632718, 13.16310754, 13.02837588,\n",
       "       12.97651378, 12.96307817, 12.84327492, 12.96563877, 13.20071552,\n",
       "       12.95754506, 12.88259348, 13.02171342, 12.90738304, 12.85755847,\n",
       "       12.81463075, 13.24300811, 12.87194398, 12.74382968, 12.64658271,\n",
       "       12.7694186 , 12.70992904, 12.89783276, 12.61095774, 12.97058999,\n",
       "       13.12382708, 12.86010237, 13.05412119, 12.85328853, 12.9551061 ,\n",
       "       12.58144112, 13.54079261, 12.75753226, 12.75753226, 13.07108345,\n",
       "       13.15141339, 12.97673399, 13.24935179, 13.11525764, 13.24694571,\n",
       "       12.66751518, 12.98348636, 13.18022542, 12.76601108, 12.86353718,\n",
       "       12.6886368 , 13.07350773, 13.07342759, 12.95818694, 12.97773682,\n",
       "       12.67244686, 13.16433526, 12.49703847, 13.07018117, 12.78491069,\n",
       "       13.07941572, 13.00715999, 12.92733609, 13.14048038, 13.22641051,\n",
       "       12.58353124, 12.93604509, 12.71825611, 13.0633097 , 12.73666597,\n",
       "       13.35740561, 12.63701954, 13.37776364, 12.74357974, 12.89774149,\n",
       "       12.97635391, 12.81363005, 12.92980683, 13.29354309, 12.91006548,\n",
       "       13.07803968, 13.12619686, 12.61146231, 13.18134618, 13.35191655,\n",
       "       13.23455275, 12.88946611, 12.96366914, 13.03119188, 12.47284848,\n",
       "       12.92524037, 12.55787578, 13.95618062, 12.53095497, 12.60444988,\n",
       "       13.26790828, 12.81248662, 13.11717208, 12.76376401, 13.00408093,\n",
       "       12.5579283 , 13.22229522, 13.77975367, 13.05569224, 13.06040236,\n",
       "       13.03597353, 12.51919512, 12.59211115, 13.12748156, 12.55842091,\n",
       "       12.58549705, 13.04102256, 13.16444653, 13.08491405, 13.02131453,\n",
       "       13.17140287, 13.41691164, 13.09460525, 13.04946083, 13.2420111 ,\n",
       "       12.65909646, 12.9391576 , 12.26747219, 12.54573509, 13.04488058,\n",
       "       13.06965896, 13.47636253, 12.5579283 , 12.81636132, 13.04594563,\n",
       "       13.16854333, 12.46855022, 13.25462065, 12.83238529, 12.6950498 ,\n",
       "       12.9672612 , 12.51430595, 12.55842091, 13.040351  , 13.29561971,\n",
       "       13.03704817, 13.465193  , 13.12865147, 13.105955  , 13.05658174,\n",
       "       13.18010708, 12.81390567, 12.76758885, 12.62263324, 12.76417352,\n",
       "       12.81463075, 13.43389831, 12.51552033, 12.59110561, 12.92295922,\n",
       "       13.24887318, 13.25246912, 12.71361982, 13.16287605, 13.16591532,\n",
       "       12.97778996, 12.95421749, 12.72614138, 12.87697544, 12.89120922,\n",
       "       12.9833966 , 12.83549194, 12.55529996, 12.97455097, 12.99356585,\n",
       "       12.56903823, 12.32266746, 13.391094  , 12.57813569, 12.60894913,\n",
       "       13.32009445, 13.07889868, 12.9949707 , 12.55580082, 12.87327255,\n",
       "       12.96164407, 13.0660001 , 12.59322028, 12.90141776, 12.96207268,\n",
       "       12.95272218, 12.97849021, 12.8792278 , 13.34708766, 12.89379256,\n",
       "       13.31802389, 13.07924722, 12.6111041 , 12.67950263, 12.96199406,\n",
       "       12.75387134, 12.80969606, 13.01467333, 13.22370751, 12.57943718,\n",
       "       12.60752774, 13.01778268, 13.18411575, 12.54926978, 13.02749878,\n",
       "       12.98869071, 12.8311828 , 12.59417239, 13.2206744 , 12.80781933,\n",
       "       13.13295211, 12.46612591, 12.75556305, 13.16827194, 13.20167156,\n",
       "       13.0967619 , 12.8032852 , 12.81413839, 13.09703877, 12.7237789 ,\n",
       "       13.07898688, 13.12735724, 12.70109229, 12.78707087, 13.3272377 ,\n",
       "       12.94753686, 12.87464238, 12.97006546, 12.99737048, 12.65382983,\n",
       "       13.02971872, 12.84518914, 12.87811838, 12.65332897, 12.91362409,\n",
       "       13.17039795, 12.69000209, 13.41063966, 13.09335734, 13.13909405,\n",
       "       13.53475807, 13.04807067, 12.65058854, 13.3561489 , 13.00496423,\n",
       "       12.8879433 , 13.07646991, 12.80573046, 12.70209578, 12.74191637,\n",
       "       13.18431366, 12.95221762, 12.75271761, 12.60848626, 13.16693003,\n",
       "       12.9299029 , 13.28054728, 12.95134907, 12.74381873, 12.46955168,\n",
       "       12.98094223, 13.1072296 , 12.94103416, 12.56633927, 13.27988687,\n",
       "       12.32266746, 13.09906807, 12.82405951, 13.29819863, 12.73966696,\n",
       "       12.88098955, 12.86231358, 12.88384378, 12.68127631, 12.84305605,\n",
       "       13.05618509, 12.89283345, 13.37768346, 13.06903658, 12.90697768,\n",
       "       13.27711175, 13.14575527, 12.99547649, 12.82362251, 12.3934923 ,\n",
       "       12.70488036, 12.91208404])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\ \\displaystyle\\frac{1}{n} \\sum_{i=0}^{n-1} (y_i – \\hat{y_i})^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    len_y = len(y_pred)\n",
    "    \n",
    "    yd = y_pred - y\n",
    "    mse = sum(yd**2) / len_y\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8610074295654322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y1_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_func(y, y_pred):\n",
    "    \"\"\"\n",
    "    目的関数を求める関数\n",
    "    ------------------------------\n",
    "    parameter\n",
    "    \n",
    "    y_pred: yの予測値\n",
    "    y: 正解値\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    m = len(y_pred)\n",
    "    j = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        j [i] = (1/2*m)* np.sum(np.square(y_pred - y))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的なCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_iter, alpha, bias=None, verbose=None):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "        self.iter = num_iter\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "            \n",
    "        \"\"\"\n",
    "        y = y.values\n",
    "        y_val = y_val.values\n",
    "        \n",
    "        X = X.values\n",
    "        X_val = X_val.values\n",
    "        \n",
    "        \n",
    "        if  not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "\n",
    "        n = X.shape[1]\n",
    "        self.w = np.zeros(n,)\n",
    "        print(self.w)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            # 予測関数の呼び出し\n",
    "            y_pred1 = self._linear_hypothesis(X)\n",
    "            #y_pred2 = self._linear_hypothesis(X_val)\n",
    "\n",
    "            error1 = y_pred1 - y\n",
    "            #error2 = y_pred2 - y_val\n",
    "\n",
    "            # 最急降下法で学習\n",
    "            self._gradient_descent1(X, error1, i)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"loss: \", self.loss[i])\n",
    "            \n",
    "        if any(y_val):\n",
    "            n = X.shape[1]\n",
    "            self.w = np.zeros(n,)\n",
    "            for i in range(self.iter):\n",
    "                y_pred2 = self._linear_hypothesis(X_val)\n",
    "                error2 = y_pred2 - y_val\n",
    "                self._gradient_descent2(X_val, error2, i)\n",
    "\n",
    "                #学習過程の表示\n",
    "                if self.verbose:\n",
    "                    print(\"val_loss: \", self.val_loss[i])\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        y_pred = np.dot(X, self.w)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def _gradient_descent1(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "    def _gradient_descent2(self, X, error, i):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.w = self.w - self.alpha * (1/m) * (X.T @ error)\n",
    "        self.val_loss[i] = (1/2*m)* np.sum(np.square(error))\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        if  not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = np.dot(X, self.w) \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#　データ作成\n",
    "df = pd.read_csv('train.csv')\n",
    "X2 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y2 = df['SalePrice']\n",
    "X2 = X2.apply(np.log)\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# priceデータの対数変換\n",
    "y2 = y2.apply(np.log)\n",
    "\n",
    "# テストデータとtrainデータの作成\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SLR = ScratchLinearRegression(num_iter=100, alpha=0.001, bias=None ,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "loss:  4217339.21574309\n",
      "loss:  3329695.4198566927\n",
      "loss:  2628918.9296311135\n",
      "loss:  2075670.3243695712\n",
      "loss:  1638891.9425377666\n",
      "loss:  1294064.400705684\n",
      "loss:  1021830.151199485\n",
      "loss:  806906.8092473629\n",
      "loss:  637229.247166375\n",
      "loss:  503272.2954113681\n",
      "loss:  397516.0290177529\n",
      "loss:  314023.62227287586\n",
      "loss:  248108.07368291946\n",
      "loss:  196069.09220578318\n",
      "loss:  154985.37435685872\n",
      "loss:  122550.61126690112\n",
      "loss:  96944.01963544781\n",
      "loss:  76728.1285879544\n",
      "loss:  60768.08450694843\n",
      "loss:  48167.94386000915\n",
      "loss:  38220.37770098269\n",
      "loss:  30366.96441068953\n",
      "loss:  24166.841634404467\n",
      "loss:  19271.957632834627\n",
      "loss:  15407.532733988955\n",
      "loss:  12356.634052276817\n",
      "loss:  9947.997547277591\n",
      "loss:  8046.413790225449\n",
      "loss:  6545.137724876954\n",
      "loss:  5359.89633021259\n",
      "loss:  4424.157793690501\n",
      "loss:  3685.396621071129\n",
      "loss:  3102.145017630294\n",
      "loss:  2641.6650144220043\n",
      "loss:  2278.11065995933\n",
      "loss:  1991.0771084386934\n",
      "loss:  1764.4551548080221\n",
      "loss:  1585.526913820385\n",
      "loss:  1444.2518772923358\n",
      "loss:  1332.7032710294886\n",
      "loss:  1244.6230702405526\n",
      "loss:  1175.070693381451\n",
      "loss:  1120.1456531885742\n",
      "loss:  1076.7685953879065\n",
      "loss:  1042.5084332705037\n",
      "loss:  1015.4458740039472\n",
      "loss:  994.0656754691139\n",
      "loss:  977.17158525424\n",
      "loss:  963.8191867445535\n",
      "loss:  953.2628824946131\n",
      "loss:  944.9140386938959\n",
      "loss:  938.3079410853355\n",
      "loss:  933.0777073443306\n",
      "loss:  928.9336914398579\n",
      "loss:  925.6472238021888\n",
      "loss:  923.0377745205801\n",
      "loss:  920.9628189525009\n",
      "loss:  919.3098368310737\n",
      "loss:  917.9899957253706\n",
      "loss:  916.9331642625432\n",
      "loss:  916.0839751695147\n",
      "loss:  915.3987171255743\n",
      "loss:  914.8428809440716\n",
      "loss:  914.3892223334778\n",
      "loss:  914.0162324871636\n",
      "loss:  913.7069306455782\n",
      "loss:  913.4479108489783\n",
      "loss:  913.2285893683623\n",
      "loss:  913.0406105677099\n",
      "loss:  912.8773778444722\n",
      "loss:  912.7336833167561\n",
      "loss:  912.6054154689925\n",
      "loss:  912.4893283442244\n",
      "loss:  912.3828593261674\n",
      "loss:  912.2839852819144\n",
      "loss:  912.1911089895568\n",
      "loss:  912.1029694751396\n",
      "loss:  912.0185712255357\n",
      "loss:  911.937128303466\n",
      "loss:  911.8580202274652\n",
      "loss:  911.7807571400197\n",
      "loss:  911.7049523085311\n",
      "loss:  911.6303004153982\n",
      "loss:  911.5565604184821\n",
      "loss:  911.4835420197905\n",
      "loss:  911.411094982797\n",
      "loss:  911.3391006986689\n",
      "loss:  911.2674655279903\n",
      "loss:  911.1961155441832\n",
      "loss:  911.1249923835478\n",
      "loss:  911.0540499689599\n",
      "loss:  910.9832519232883\n",
      "loss:  910.9125695273499\n",
      "loss:  910.8419801077503\n",
      "loss:  910.7714657641258\n",
      "loss:  910.7010123643162\n",
      "loss:  910.6306087510852\n",
      "loss:  910.5602461158359\n",
      "loss:  910.4899175041705\n",
      "loss:  910.4196174255483\n",
      "val_loss:  263649.95874410565\n",
      "val_loss:  208269.19766511786\n",
      "val_loss:  164524.25359385446\n",
      "val_loss:  129970.37407767339\n",
      "val_loss:  102676.46329423931\n",
      "val_loss:  81117.15982259931\n",
      "val_loss:  64087.58949875758\n",
      "val_loss:  50636.029189348585\n",
      "val_loss:  40010.718295921586\n",
      "val_loss:  31617.845469807904\n",
      "val_loss:  24988.362561191818\n",
      "val_loss:  19751.77114942365\n",
      "val_loss:  15615.416674928194\n",
      "val_loss:  12348.13299375812\n",
      "val_loss:  9767.323305832642\n",
      "val_loss:  7728.755454874869\n",
      "val_loss:  6118.5012948544045\n",
      "val_loss:  4846.569642161662\n",
      "val_loss:  3841.876981356175\n",
      "val_loss:  3048.2748547257056\n",
      "val_loss:  2421.411920275895\n",
      "val_loss:  1926.2553094855373\n",
      "val_loss:  1535.1327621334945\n",
      "val_loss:  1226.1861199224306\n",
      "val_loss:  982.1497500369072\n",
      "val_loss:  789.3856289772939\n",
      "val_loss:  637.1211608610795\n",
      "val_loss:  516.8471345074004\n",
      "val_loss:  421.84217322022073\n",
      "val_loss:  346.79710042254686\n",
      "val_loss:  287.51822824351575\n",
      "val_loss:  240.6929868894503\n",
      "val_loss:  203.70479663961189\n",
      "val_loss:  174.48683630639556\n",
      "val_loss:  151.406535787816\n",
      "val_loss:  133.17433740278955\n",
      "val_loss:  118.77162699757653\n",
      "val_loss:  107.39380714360297\n",
      "val_loss:  98.40533098566681\n",
      "val_loss:  91.30418373871377\n",
      "val_loss:  85.693826827576\n",
      "val_loss:  81.26103672521768\n",
      "val_loss:  77.75839997924463\n",
      "val_loss:  74.99048613458798\n",
      "val_loss:  72.80292580510499\n",
      "val_loss:  71.07378350550941\n",
      "val_loss:  69.70674310120896\n",
      "val_loss:  68.62572503454626\n",
      "val_loss:  67.77063450294156\n",
      "val_loss:  67.09400296939643\n",
      "val_loss:  66.55833531106805\n",
      "val_loss:  66.13401434727048\n",
      "val_loss:  65.79764563826274\n",
      "val_loss:  65.53075005138524\n",
      "val_loss:  65.31873102661268\n",
      "val_loss:  65.15005882558751\n",
      "val_loss:  65.01562617464936\n",
      "val_loss:  64.90823929099132\n",
      "val_loss:  64.82221584716463\n",
      "val_loss:  64.75306740556528\n",
      "val_loss:  64.69724857527291\n",
      "val_loss:  64.6519588724878\n",
      "val_loss:  64.61498621123661\n",
      "val_loss:  64.58458327758615\n",
      "val_loss:  64.559369878349\n",
      "val_loss:  64.53825580689171\n",
      "val_loss:  64.52037991528292\n",
      "val_loss:  64.50506198773478\n",
      "val_loss:  64.49176472571318\n",
      "val_loss:  64.48006372019645\n",
      "val_loss:  64.46962373293547\n",
      "val_loss:  64.46017996115887\n",
      "val_loss:  64.45152323867163\n",
      "val_loss:  64.44348834628875\n",
      "val_loss:  64.43594477831401\n",
      "val_loss:  64.42878944903568\n",
      "val_loss:  64.42194093162843\n",
      "val_loss:  64.4153349074969\n",
      "val_loss:  64.408920571737\n",
      "val_loss:  64.40265779383138\n",
      "val_loss:  64.39651487489901\n",
      "val_loss:  64.39046677616017\n",
      "val_loss:  64.38449371960952\n",
      "val_loss:  64.3785800826981\n",
      "val_loss:  64.37271352524719\n",
      "val_loss:  64.36688429980379\n",
      "val_loss:  64.36108470689368\n",
      "val_loss:  64.35530866472917\n",
      "val_loss:  64.34955136932403\n",
      "val_loss:  64.34380902602015\n",
      "val_loss:  64.33807863742128\n",
      "val_loss:  64.33235783588438\n",
      "val_loss:  64.32664475120426\n",
      "val_loss:  64.32093790609945\n",
      "val_loss:  64.31523613365651\n",
      "val_loss:  64.30953851211976\n",
      "val_loss:  64.30384431338227\n",
      "val_loss:  64.2981529622995\n",
      "val_loss:  64.29246400455054\n",
      "val_loss:  64.28677708125251\n",
      "[4.21733922e+06 3.32969542e+06 2.62891893e+06 2.07567032e+06\n",
      " 1.63889194e+06 1.29406440e+06 1.02183015e+06 8.06906809e+05\n",
      " 6.37229247e+05 5.03272295e+05 3.97516029e+05 3.14023622e+05\n",
      " 2.48108074e+05 1.96069092e+05 1.54985374e+05 1.22550611e+05\n",
      " 9.69440196e+04 7.67281286e+04 6.07680845e+04 4.81679439e+04\n",
      " 3.82203777e+04 3.03669644e+04 2.41668416e+04 1.92719576e+04\n",
      " 1.54075327e+04 1.23566341e+04 9.94799755e+03 8.04641379e+03\n",
      " 6.54513772e+03 5.35989633e+03 4.42415779e+03 3.68539662e+03\n",
      " 3.10214502e+03 2.64166501e+03 2.27811066e+03 1.99107711e+03\n",
      " 1.76445515e+03 1.58552691e+03 1.44425188e+03 1.33270327e+03\n",
      " 1.24462307e+03 1.17507069e+03 1.12014565e+03 1.07676860e+03\n",
      " 1.04250843e+03 1.01544587e+03 9.94065675e+02 9.77171585e+02\n",
      " 9.63819187e+02 9.53262882e+02 9.44914039e+02 9.38307941e+02\n",
      " 9.33077707e+02 9.28933691e+02 9.25647224e+02 9.23037775e+02\n",
      " 9.20962819e+02 9.19309837e+02 9.17989996e+02 9.16933164e+02\n",
      " 9.16083975e+02 9.15398717e+02 9.14842881e+02 9.14389222e+02\n",
      " 9.14016232e+02 9.13706931e+02 9.13447911e+02 9.13228589e+02\n",
      " 9.13040611e+02 9.12877378e+02 9.12733683e+02 9.12605415e+02\n",
      " 9.12489328e+02 9.12382859e+02 9.12283985e+02 9.12191109e+02\n",
      " 9.12102969e+02 9.12018571e+02 9.11937128e+02 9.11858020e+02\n",
      " 9.11780757e+02 9.11704952e+02 9.11630300e+02 9.11556560e+02\n",
      " 9.11483542e+02 9.11411095e+02 9.11339101e+02 9.11267466e+02\n",
      " 9.11196116e+02 9.11124992e+02 9.11054050e+02 9.10983252e+02\n",
      " 9.10912570e+02 9.10841980e+02 9.10771466e+02 9.10701012e+02\n",
      " 9.10630609e+02 9.10560246e+02 9.10489918e+02 9.10419617e+02]\n"
     ]
    }
   ],
   "source": [
    "SLR.fit(X2_train, y2_train, X_val= X2_val, y_val= y2_val)\n",
    "print(SLR.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SLR.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015078131898317855"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y2_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003095456683099062"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y2_val, lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAETCAYAAABwaNKCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSYcQGZmlgkHkJmZZeL/A4+nMLrSSLiiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(SLR.loss, label=\"loss\")\n",
    "plt.plot(SLR.val_loss, label=\"val_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.title('madel loss (StandardScaler)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自作のコードでのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(itera, alpha, w, X, y):\n",
    "    X = np.insert(X , 0, 1, axis=1)\n",
    "    print(X)\n",
    "    m = len(y)\n",
    "    cost = np.zeros(m)\n",
    "    for i in range(itera):\n",
    "        cost[i] = (1/2*m)* np.sum(np.square(np.dot(X, w)-y))\n",
    "        w = w -alpha* (1/m)* np.dot(X.T, (np.dot(X, w)-y))\n",
    "    \n",
    "    return [cost, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "X3 = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y3 = df['SalePrice']\n",
    "X3 = X3.apply(np.log)\n",
    "y3 = y3.apply(np.log)\n",
    "X3 = X3.values\n",
    "y3 = y3.values\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "itera = 1000\n",
    "\n",
    "init_theta = np.zeros(3).reshape(3,)\n",
    "\n",
    "cost , w = grad(itera, alpha, init_theta, X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost)\n",
    "plt.xlabel('Cost')\n",
    "plt.ylabel('Iteration')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
