{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irisデータセットの準備\n",
    "iris = load_iris()\n",
    "feature = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = feature\n",
    "\n",
    "y = y = pd.DataFrame(iris.target, columns=[\"species\"])\n",
    "name = [\"species\"] \n",
    "y.columns = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.join( y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# virgicolorとvirginicaのデータ抽出\n",
    "df= Xy[Xy['species']>=1]\n",
    "df.loc[df['species'] == 1, 'species'] = 0\n",
    "df.loc[df['species'] == 2, 'species'] = 1\n",
    "X = df.loc[: , ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df.loc[:, ['species']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n",
    "ロジスティック回帰の仮定関数は、線形回帰の仮定関数を シグモイド関数 に通したものです。シグモイド関数は以下の式で表されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1の列追加\n",
    "X = X.values\n",
    "X1=  np.insert(X, 0, 1, axis=1)\n",
    "y1 = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みとバイアスの初期値の作成\n",
    "n = X_train.shape[1]\n",
    "np.random.seed(0)\n",
    "theta = np.random.rand(n,)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仮定関数\n",
    "def _hypothesis(X, theta):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    X:配列\n",
    "    w: 重みとバイアス\n",
    "    戻り値: 予測値\n",
    "    \"\"\"\n",
    "    # シグモイド関数\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    yp = sigmoid(X @ theta)\n",
    "    return yp.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = _hypothesis(X_train, theta)\n",
    "yp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent( X, y, theta, yp, k):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    alpha : 学習率\n",
    "    m : 入力されるデータの数\n",
    "    x : 特徴量ベクトル\n",
    "    y:  正解ラベル\n",
    "    theta : パラメータ（重み）ベクトル\n",
    "    yp : 仮定関数結果\n",
    "    ramda : 正則化パラメータ(適当な値)  \n",
    "    \n",
    "    \"\"\"\n",
    "    #ハイパーパラメータ\n",
    "    alpha = 0.001\n",
    "    lambda_ = 0.001\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # 0番目のthetaの場合の正則化項の処理用の値判定\n",
    "    if k == 0:\n",
    "        j = 0\n",
    "    else:\n",
    "        j = 1\n",
    "    \n",
    "    grad = ((1/ m) * (yp - y).T @ X) \n",
    "    reg = (lambda_ / m / 2) * theta * j\n",
    "    theta = theta - alpha * (grad + reg)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  学習回数\n",
    "num_iter = 10\n",
    "\n",
    "for k in range(num_iter):\n",
    "    theta = _gradient_descent(X_train, y_train, theta, yp, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 \n",
    "hθ(x)の出力がpredict_probaの返り値、さらにその値に閾値を設けて1と0のラベルとしたものがpredictの返り値となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_proba):\n",
    "    \"\"\"\n",
    "    ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        ロジスティック回帰による推定結果\n",
    "    \"\"\"\n",
    "    # 闘値を０.５に設定し\n",
    "    pred = np.where(pred_proba < 0.5, 0, 1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, theta):\n",
    "    \"\"\"\n",
    "    ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        ロジスティック回帰による推定結果\n",
    "    \"\"\"\n",
    "    # シグモイド関数\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    pred_proba = sigmoid(X @ theta.T)\n",
    "    \n",
    "    return pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = predict_proba(X_test, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】目的関数\n",
    "以下の数式で表されるロジスティック回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "なお、この数式には正則化項が含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数（クロスエントロピー）\n",
    "def cost_func(y, yp, theta):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    -------------------\n",
    "    alpha : 学習率\n",
    "    m : 入力されるデータの数\n",
    "    x : 特徴量ベクトル\n",
    "    y:  正解ラベル\n",
    "    theta : パラメータ（重み）ベクトル\n",
    "    yp : 仮定関数結果\n",
    "    ramda : 正則化パラメータ(適当な値)  \n",
    "    i : 学習回数\n",
    "    \"\"\"\n",
    "    #ハイパーパラメータ\n",
    "    alpha = 0.01\n",
    "    lambda_ = 0.01\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    \n",
    "    cross = (1/ m) * (-y * np.log(yp) - (1- y) * np.log(1 - yp)).sum() \n",
    "    L2 = (lambda_ / (2 * m) * (theta**2).sum())\n",
    "    \n",
    "    loss = cross + L2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215,\n",
       "       4.52084215, 4.52084215, 4.52084215, 4.52084215, 4.52084215])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  学習回数\n",
    "num_iter = 100\n",
    "loss = np.zeros(num_iter)\n",
    "\n",
    "for k in range(num_iter):\n",
    "    loss[k] = cost_func(y_train, yp, theta)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終的なモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    alpha : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.theta : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, alpha=0.01,lambda_=0.01, bias=None, verbose=None):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        X = np.array(X)\n",
    "        if np.any(X_val):\n",
    "            y_val = np.array(y_val)\n",
    "            X_val = np.array(X_val)\n",
    "        \n",
    "        if  not self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            if np.any(X_val):\n",
    "                print(yes)\n",
    "                X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "        \n",
    "        # 重みとバイアスの初期値の作成\n",
    "        n = X.shape[1]\n",
    "        np.random.seed(0)\n",
    "        self.theta = np.random.rand(n)\n",
    "\n",
    "        \n",
    "        #　学習\n",
    "        for k in range(self.iter):\n",
    "            # 予測関数の呼び出し\n",
    "            y_pred1 = self._hypothesis(X)\n",
    "\n",
    "            # 最急降下法で学習\n",
    "            self._gradient_descent(X, y, y_pred1, k)\n",
    "\n",
    "            #　lossの記録\n",
    "            self.loss[k] = self._cost_func(y, y_pred1)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"loss: \", self.loss[k])\n",
    "            \n",
    "        # valの引数がある場合の処理\n",
    "        if np.any(y_val):\n",
    "            n = X_val.shape[1]\n",
    "            np.random.seed(0)\n",
    "            self.theta = np.random.rand(n)\n",
    "            \n",
    "            # 学習\n",
    "            for k in range(self.iter):\n",
    "                # 予測関数の呼び出し\n",
    "                y_pred2 = self._hypothesis(X_val)\n",
    "\n",
    "                # 最急降下法で学習\n",
    "                self._gradient_descent(X_val, y_val, y_pred2, k)\n",
    "\n",
    "                #　lossの記録\n",
    "                self.val_loss[k] = self._cost_func(y_val, y_pred2)\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"val_oss: \", self.val_loss[k])\n",
    "\n",
    "                    \n",
    "    \n",
    "    # 仮定関数\n",
    "    def _hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        -------------------\n",
    "        X:配列\n",
    "        w: 重みとバイアス\n",
    "        戻り値: 予測値\n",
    "        \"\"\"\n",
    "        # シグモイド関数\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "        yp = sigmoid(X @ self.theta.T)\n",
    "        return yp.reshape(-1, 1)\n",
    "    \n",
    "    def _gradient_descent(self, X,  y, yp, k):\n",
    "        \"\"\"\n",
    "        勾配降下法でtheta更新\n",
    "        -------------------\n",
    "        alpha : 学習率\n",
    "        m : 入力されるデータの数\n",
    "        x : 特徴量ベクトル\n",
    "        y:  正解ラベル\n",
    "        theta : パラメータ（重み）ベクトル\n",
    "        yp : 仮定関数結果\n",
    "        ramda : 正則化パラメータ(適当な値)  \n",
    "\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # 0番目のthetaの場合の正則化項の処理用の値判定\n",
    "        if k == 0:\n",
    "            j = 0\n",
    "        else:\n",
    "            j = 1\n",
    "\n",
    "        grad = ((1/ m) * (yp - y).T @ X) \n",
    "        reg = (self.lambda_ / m / 2) * self.theta * j\n",
    "        self.theta = self.theta - self.alpha * (grad + reg)\n",
    "    \n",
    "    #損失関数（クロスエントロピー）\n",
    "    def _cost_func(self, y, yp):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        -------------------\n",
    "        alpha : 学習率\n",
    "        m : 入力されるデータの数\n",
    "        x : 特徴量ベクトル\n",
    "        y:  正解ラベル\n",
    "        theta : パラメータ（重み）ベクトル\n",
    "        yp : 仮定関数結果\n",
    "        ramda : 正則化パラメータ(適当な値)  \n",
    "        i : 学習回数\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "\n",
    "        cross = (1/ m) * (-1 * y * np.log(yp) - (1- y) * np.log(1 - yp)).sum() \n",
    "        L2 = (self.lambda_ / (2 * m)) * (self.theta**2).sum()\n",
    "\n",
    "        loss = cross + L2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        # 闘値を０.５に設定し\n",
    "        pred = np.where(X < 0.5, 0, 1)\n",
    "        return pred\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        #if  not self.bias:\n",
    "            #X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        # シグモイド関数\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "        pred_proba = sigmoid(X @ self.theta.T)\n",
    "\n",
    "        return pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# virgicolorとvirginicaのデータ抽出\n",
    "df= Xy[Xy['species']>=1]\n",
    "df.loc[df['species'] == 1, 'species'] = 0\n",
    "df.loc[df['species'] == 2, 'species'] = 1\n",
    "X = df.loc[: , ['sepal_length',  'petal_length']]\n",
    "y = df.loc[:, ['species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = ScratchLogisticRegression(num_iter=1000, alpha=0.01,lambda_=0.01, bias=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  3.1212393587338436\n",
      "loss:  2.991087228121476\n",
      "loss:  2.861174467354291\n",
      "loss:  2.7315754098138694\n",
      "loss:  2.6023840496363744\n",
      "loss:  2.473722207981159\n",
      "loss:  2.3457473424035684\n",
      "loss:  2.2186621941985036\n",
      "loss:  2.0927264381601627\n",
      "loss:  1.9682702947523152\n",
      "loss:  1.8457096657197039\n",
      "loss:  1.7255616667000695\n",
      "loss:  1.6084583639514716\n",
      "loss:  1.49515505094762\n",
      "loss:  1.3865276831060631\n",
      "loss:  1.283552647243768\n",
      "loss:  1.187261921681651\n",
      "loss:  1.0986693740694888\n",
      "loss:  1.018670725364768\n",
      "loss:  0.9479302436224517\n",
      "loss:  0.8867780769799759\n",
      "loss:  0.8351469577301998\n",
      "loss:  0.7925701261535377\n",
      "loss:  0.7582440824493027\n",
      "loss:  0.7311385292866125\n",
      "loss:  0.7101228253984738\n",
      "loss:  0.6940791096467682\n",
      "loss:  0.6819836902087504\n",
      "loss:  0.6729522483024115\n",
      "loss:  0.6662544339233789\n",
      "loss:  0.6613076018641664\n",
      "loss:  0.6576591640716383\n",
      "loss:  0.6549646689923111\n",
      "loss:  0.6529660298944147\n",
      "loss:  0.6514721661165289\n",
      "loss:  0.6503428921971764\n",
      "loss:  0.6494760782566136\n",
      "loss:  0.6487977177159366\n",
      "loss:  0.648254404679133\n",
      "loss:  0.6478077229371865\n",
      "loss:  0.647430107716646\n",
      "loss:  0.6471018191448085\n",
      "loss:  0.6468087427421031\n",
      "loss:  0.6465407986292725\n",
      "loss:  0.6462907952333379\n",
      "loss:  0.6460536056514522\n",
      "loss:  0.6458255771712973\n",
      "loss:  0.6456041086880174\n",
      "loss:  0.6453873486979426\n",
      "loss:  0.6451739797032667\n",
      "loss:  0.6449630644398606\n",
      "loss:  0.6447539362783417\n",
      "loss:  0.6445461211540743\n",
      "loss:  0.6443392819820547\n",
      "loss:  0.6441331790959268\n",
      "loss:  0.6439276421004458\n",
      "loss:  0.6437225498496852\n",
      "loss:  0.6435178162081805\n",
      "loss:  0.6433133799264407\n",
      "loss:  0.6431091974429686\n",
      "loss:  0.6429052377674587\n",
      "loss:  0.6427014788437662\n",
      "loss:  0.6424979049648959\n",
      "loss:  0.6422945049358192\n",
      "loss:  0.6420912707678388\n",
      "loss:  0.6418881967507433\n",
      "loss:  0.641685278793456\n",
      "loss:  0.6414825139554915\n",
      "loss:  0.6412799001140059\n",
      "loss:  0.6410774357272057\n",
      "loss:  0.640875119666224\n",
      "loss:  0.640672951095655\n",
      "loss:  0.6404709293886593\n",
      "loss:  0.640269054066638\n",
      "loss:  0.6400673247563631\n",
      "loss:  0.6398657411595136\n",
      "loss:  0.6396643030310262\n",
      "loss:  0.6394630101637089\n",
      "loss:  0.6392618623773074\n",
      "loss:  0.6390608595107338\n",
      "loss:  0.6388600014165423\n",
      "loss:  0.6386592879570044\n",
      "loss:  0.6384587190013176\n",
      "loss:  0.6382582944236217\n",
      "loss:  0.6380580141015887\n",
      "loss:  0.6378578779154198\n",
      "loss:  0.6376578857471333\n",
      "loss:  0.6374580374800562\n",
      "loss:  0.6372583329984649\n",
      "loss:  0.6370587721873282\n",
      "loss:  0.6368593549321246\n",
      "loss:  0.6366600811187133\n",
      "loss:  0.6364609506332406\n",
      "loss:  0.6362619633620753\n",
      "loss:  0.6360631191917614\n",
      "loss:  0.6358644180089843\n",
      "loss:  0.635665859700548\n",
      "loss:  0.6354674441533573\n",
      "loss:  0.6352691712544064\n",
      "loss:  0.6350710408907699\n",
      "loss:  0.6348730529495968\n",
      "loss:  0.6346752073181061\n",
      "loss:  0.634477503883584\n",
      "loss:  0.6342799425333812\n",
      "loss:  0.6340825231549116\n",
      "loss:  0.6338852456356514\n",
      "loss:  0.6336881098631378\n",
      "loss:  0.6334911157249692\n",
      "loss:  0.6332942631088041\n",
      "loss:  0.633097551902361\n",
      "loss:  0.6329009819934189\n",
      "loss:  0.6327045532698162\n",
      "loss:  0.6325082656194514\n",
      "loss:  0.6323121189302828\n",
      "loss:  0.6321161130903284\n",
      "loss:  0.6319202479876662\n",
      "loss:  0.6317245235104341\n",
      "loss:  0.6315289395468296\n",
      "loss:  0.6313334959851108\n",
      "loss:  0.6311381927135951\n",
      "loss:  0.6309430296206606\n",
      "loss:  0.6307480065947453\n",
      "loss:  0.6305531235243473\n",
      "loss:  0.6303583802980256\n",
      "loss:  0.6301637768043985\n",
      "loss:  0.6299693129321459\n",
      "loss:  0.6297749885700071\n",
      "loss:  0.629580803606783\n",
      "loss:  0.6293867579313346\n",
      "loss:  0.6291928514325832\n",
      "loss:  0.6289990839995119\n",
      "loss:  0.6288054555211636\n",
      "loss:  0.6286119658866429\n",
      "loss:  0.6284186149851149\n",
      "loss:  0.628225402705806\n",
      "loss:  0.6280323289380034\n",
      "loss:  0.6278393935710562\n",
      "loss:  0.627646596494374\n",
      "loss:  0.6274539375974281\n",
      "loss:  0.6272614167697509\n",
      "loss:  0.6270690339009368\n",
      "loss:  0.626876788880641\n",
      "loss:  0.626684681598581\n",
      "loss:  0.6264927119445356\n",
      "loss:  0.6263008798083455\n",
      "loss:  0.6261091850799125\n",
      "loss:  0.6259176276492017\n",
      "loss:  0.6257262074062384\n",
      "loss:  0.6255349242411112\n",
      "loss:  0.6253437780439701\n",
      "loss:  0.6251527687050276\n",
      "loss:  0.6249618961145581\n",
      "loss:  0.6247711601628982\n",
      "loss:  0.6245805607404472\n",
      "loss:  0.6243900977376663\n",
      "loss:  0.6241997710450795\n",
      "loss:  0.6240095805532728\n",
      "loss:  0.6238195261528956\n",
      "loss:  0.6236296077346588\n",
      "loss:  0.623439825189337\n",
      "loss:  0.6232501784077672\n",
      "loss:  0.6230606672808489\n",
      "loss:  0.6228712916995446\n",
      "loss:  0.6226820515548801\n",
      "loss:  0.6224929467379438\n",
      "loss:  0.6223039771398869\n",
      "loss:  0.6221151426519247\n",
      "loss:  0.6219264431653343\n",
      "loss:  0.6217378785714571\n",
      "loss:  0.6215494487616974\n",
      "loss:  0.6213611536275224\n",
      "loss:  0.6211729930604634\n",
      "loss:  0.620984966952115\n",
      "loss:  0.6207970751941349\n",
      "loss:  0.6206093176782446\n",
      "loss:  0.6204216942962292\n",
      "loss:  0.6202342049399374\n",
      "loss:  0.6200468495012821\n",
      "loss:  0.6198596278722391\n",
      "loss:  0.619672539944849\n",
      "loss:  0.6194855856112154\n",
      "loss:  0.6192987647635066\n",
      "loss:  0.619112077293954\n",
      "loss:  0.6189255230948542\n",
      "loss:  0.6187391020585672\n",
      "loss:  0.6185528140775169\n",
      "loss:  0.6183666590441922\n",
      "loss:  0.6181806368511457\n",
      "loss:  0.6179947473909942\n",
      "loss:  0.6178089905564196\n",
      "loss:  0.6176233662401672\n",
      "loss:  0.6174378743350476\n",
      "loss:  0.6172525147339356\n",
      "loss:  0.6170672873297707\n",
      "loss:  0.6168821920155565\n",
      "loss:  0.6166972286843622\n",
      "loss:  0.6165123972293208\n",
      "loss:  0.6163276975436306\n",
      "loss:  0.6161431295205545\n",
      "loss:  0.6159586930534204\n",
      "loss:  0.6157743880356211\n",
      "loss:  0.6155902143606142\n",
      "loss:  0.6154061719219224\n",
      "loss:  0.6152222606131337\n",
      "loss:  0.6150384803279006\n",
      "loss:  0.6148548309599414\n",
      "loss:  0.6146713124030392\n",
      "loss:  0.6144879245510426\n",
      "loss:  0.6143046672978653\n",
      "loss:  0.6141215405374861\n",
      "loss:  0.61393854416395\n",
      "loss:  0.6137556780713664\n",
      "loss:  0.6135729421539107\n",
      "loss:  0.6133903363058242\n",
      "loss:  0.6132078604214127\n",
      "loss:  0.6130255143950486\n",
      "loss:  0.6128432981211693\n",
      "loss:  0.6126612114942784\n",
      "loss:  0.612479254408945\n",
      "loss:  0.6122974267598035\n",
      "loss:  0.6121157284415552\n",
      "loss:  0.6119341593489661\n",
      "loss:  0.6117527193768689\n",
      "loss:  0.6115714084201616\n",
      "loss:  0.6113902263738089\n",
      "loss:  0.6112091731328408\n",
      "loss:  0.6110282485923543\n",
      "loss:  0.6108474526475113\n",
      "loss:  0.6106667851935406\n",
      "loss:  0.6104862461257371\n",
      "loss:  0.610305835339462\n",
      "loss:  0.6101255527301422\n",
      "loss:  0.6099453981932716\n",
      "loss:  0.6097653716244104\n",
      "loss:  0.6095854729191843\n",
      "loss:  0.6094057019732864\n",
      "loss:  0.6092260586824758\n",
      "loss:  0.6090465429425783\n",
      "loss:  0.6088671546494859\n",
      "loss:  0.6086878936991573\n",
      "loss:  0.6085087599876183\n",
      "loss:  0.6083297534109604\n",
      "loss:  0.6081508738653425\n",
      "loss:  0.6079721212469898\n",
      "loss:  0.6077934954521945\n",
      "loss:  0.6076149963773156\n",
      "loss:  0.6074366239187786\n",
      "loss:  0.6072583779730759\n",
      "loss:  0.6070802584367674\n",
      "loss:  0.606902265206479\n",
      "loss:  0.6067243981789042\n",
      "loss:  0.6065466572508033\n",
      "loss:  0.6063690423190033\n",
      "loss:  0.6061915532803989\n",
      "loss:  0.6060141900319511\n",
      "loss:  0.6058369524706887\n",
      "loss:  0.6056598404937075\n",
      "loss:  0.6054828539981697\n",
      "loss:  0.6053059928813058\n",
      "loss:  0.6051292570404134\n",
      "loss:  0.6049526463728567\n",
      "loss:  0.6047761607760673\n",
      "loss:  0.604599800147545\n",
      "loss:  0.6044235643848559\n",
      "loss:  0.604247453385634\n",
      "loss:  0.6040714670475807\n",
      "loss:  0.6038956052684651\n",
      "loss:  0.6037198679461232\n",
      "loss:  0.6035442549784588\n",
      "loss:  0.6033687662634434\n",
      "loss:  0.6031934016991161\n",
      "loss:  0.6030181611835829\n",
      "loss:  0.6028430446150184\n",
      "loss:  0.602668051891664\n",
      "loss:  0.6024931829118295\n",
      "loss:  0.6023184375738919\n",
      "loss:  0.6021438157762963\n",
      "loss:  0.601969317417555\n",
      "loss:  0.601794942396249\n",
      "loss:  0.601620690611026\n",
      "loss:  0.6014465619606023\n",
      "loss:  0.6012725563437621\n",
      "loss:  0.601098673659357\n",
      "loss:  0.6009249138063067\n",
      "loss:  0.6007512766835991\n",
      "loss:  0.6005777621902897\n",
      "loss:  0.6004043702255024\n",
      "loss:  0.6002311006884286\n",
      "loss:  0.600057953478328\n",
      "loss:  0.5998849284945286\n",
      "loss:  0.599712025636426\n",
      "loss:  0.5995392448034842\n",
      "loss:  0.5993665858952356\n",
      "loss:  0.5991940488112799\n",
      "loss:  0.5990216334512857\n",
      "loss:  0.5988493397149897\n",
      "loss:  0.5986771675021966\n",
      "loss:  0.5985051167127795\n",
      "loss:  0.5983331872466796\n",
      "loss:  0.5981613790039069\n",
      "loss:  0.5979896918845387\n",
      "loss:  0.5978181257887215\n",
      "loss:  0.5976466806166699\n",
      "loss:  0.5974753562686668\n",
      "loss:  0.5973041526450632\n",
      "loss:  0.5971330696462793\n",
      "loss:  0.5969621071728032\n",
      "loss:  0.596791265125191\n",
      "loss:  0.596620543404068\n",
      "loss:  0.5964499419101277\n",
      "loss:  0.5962794605441322\n",
      "loss:  0.596109099206912\n",
      "loss:  0.595938857799366\n",
      "loss:  0.595768736222462\n",
      "loss:  0.5955987343772361\n",
      "loss:  0.5954288521647928\n",
      "loss:  0.595259089486306\n",
      "loss:  0.5950894462430173\n",
      "loss:  0.5949199223362376\n",
      "loss:  0.5947505176673459\n",
      "loss:  0.5945812321377905\n",
      "loss:  0.5944120656490876\n",
      "loss:  0.594243018102823\n",
      "loss:  0.5940740894006502\n",
      "loss:  0.5939052794442925\n",
      "loss:  0.5937365881355412\n",
      "loss:  0.5935680153762567\n",
      "loss:  0.5933995610683681\n",
      "loss:  0.5932312251138733\n",
      "loss:  0.5930630074148389\n",
      "loss:  0.5928949078734003\n",
      "loss:  0.5927269263917618\n",
      "loss:  0.5925590628721967\n",
      "loss:  0.592391317217047\n",
      "loss:  0.5922236893287238\n",
      "loss:  0.5920561791097066\n",
      "loss:  0.5918887864625442\n",
      "loss:  0.5917215112898541\n",
      "loss:  0.5915543534943232\n",
      "loss:  0.5913873129787063\n",
      "loss:  0.5912203896458283\n",
      "loss:  0.5910535833985827\n",
      "loss:  0.5908868941399313\n",
      "loss:  0.5907203217729059\n",
      "loss:  0.5905538662006063\n",
      "loss:  0.5903875273262024\n",
      "loss:  0.590221305052932\n",
      "loss:  0.5900551992841025\n",
      "loss:  0.5898892099230907\n",
      "loss:  0.5897233368733416\n",
      "loss:  0.5895575800383694\n",
      "loss:  0.5893919393217585\n",
      "loss:  0.5892264146271607\n",
      "loss:  0.5890610058582981\n",
      "loss:  0.5888957129189613\n",
      "loss:  0.58873053571301\n",
      "loss:  0.5885654741443734\n",
      "loss:  0.5884005281170495\n",
      "loss:  0.5882356975351054\n",
      "loss:  0.5880709823026775\n",
      "loss:  0.5879063823239712\n",
      "loss:  0.5877418975032611\n",
      "loss:  0.587577527744891\n",
      "loss:  0.5874132729532738\n",
      "loss:  0.5872491330328915\n",
      "loss:  0.5870851078882953\n",
      "loss:  0.5869211974241058\n",
      "loss:  0.5867574015450123\n",
      "loss:  0.5865937201557737\n",
      "loss:  0.5864301531612179\n",
      "loss:  0.5862667004662423\n",
      "loss:  0.5861033619758129\n",
      "loss:  0.5859401375949658\n",
      "loss:  0.5857770272288052\n",
      "loss:  0.5856140307825055\n",
      "loss:  0.5854511481613098\n",
      "loss:  0.5852883792705306\n",
      "loss:  0.5851257240155495\n",
      "loss:  0.5849631823018174\n",
      "loss:  0.5848007540348548\n",
      "loss:  0.5846384391202507\n",
      "loss:  0.5844762374636636\n",
      "loss:  0.5843141489708219\n",
      "loss:  0.5841521735475227\n",
      "loss:  0.583990311099632\n",
      "loss:  0.5838285615330855\n",
      "loss:  0.5836669247538885\n",
      "loss:  0.5835054006681148\n",
      "loss:  0.5833439891819082\n",
      "loss:  0.583182690201481\n",
      "loss:  0.5830215036331152\n",
      "loss:  0.5828604293831623\n",
      "loss:  0.5826994673580425\n",
      "loss:  0.5825386174642457\n",
      "loss:  0.5823778796083311\n",
      "loss:  0.5822172536969267\n",
      "loss:  0.5820567396367302\n",
      "loss:  0.5818963373345084\n",
      "loss:  0.5817360466970974\n",
      "loss:  0.5815758676314028\n",
      "loss:  0.5814158000443989\n",
      "loss:  0.5812558438431298\n",
      "loss:  0.5810959989347088\n",
      "loss:  0.580936265226318\n",
      "loss:  0.5807766426252094\n",
      "loss:  0.5806171310387039\n",
      "loss:  0.5804577303741919\n",
      "loss:  0.5802984405391329\n",
      "loss:  0.5801392614410553\n",
      "loss:  0.5799801929875575\n",
      "loss:  0.5798212350863068\n",
      "loss:  0.5796623876450394\n",
      "loss:  0.5795036505715613\n",
      "loss:  0.5793450237737476\n",
      "loss:  0.5791865071595425\n",
      "loss:  0.5790281006369595\n",
      "loss:  0.5788698041140814\n",
      "loss:  0.5787116174990603\n",
      "loss:  0.5785535407001173\n",
      "loss:  0.578395573625543\n",
      "loss:  0.5782377161836966\n",
      "loss:  0.5780799682830078\n",
      "loss:  0.577922329831974\n",
      "loss:  0.5777648007391629\n",
      "loss:  0.577607380913211\n",
      "loss:  0.5774500702628241\n",
      "loss:  0.577292868696777\n",
      "loss:  0.5771357761239138\n",
      "loss:  0.5769787924531478\n",
      "loss:  0.5768219175934619\n",
      "loss:  0.5766651514539071\n",
      "loss:  0.5765084939436047\n",
      "loss:  0.5763519449717446\n",
      "loss:  0.576195504447586\n",
      "loss:  0.5760391722804571\n",
      "loss:  0.5758829483797556\n",
      "loss:  0.5757268326549478\n",
      "loss:  0.5755708250155696\n",
      "loss:  0.5754149253712257\n",
      "loss:  0.5752591336315903\n",
      "loss:  0.5751034497064063\n",
      "loss:  0.5749478735054858\n",
      "loss:  0.5747924049387102\n",
      "loss:  0.5746370439160301\n",
      "loss:  0.5744817903474646\n",
      "loss:  0.5743266441431022\n",
      "loss:  0.5741716052131007\n",
      "loss:  0.5740166734676866\n",
      "loss:  0.5738618488171555\n",
      "loss:  0.5737071311718722\n",
      "loss:  0.5735525204422706\n",
      "loss:  0.5733980165388531\n",
      "loss:  0.5732436193721917\n",
      "loss:  0.5730893288529272\n",
      "loss:  0.5729351448917691\n",
      "loss:  0.5727810673994963\n",
      "loss:  0.5726270962869565\n",
      "loss:  0.5724732314650665\n",
      "loss:  0.5723194728448117\n",
      "loss:  0.5721658203372466\n",
      "loss:  0.5720122738534951\n",
      "loss:  0.5718588333047493\n",
      "loss:  0.5717054986022705\n",
      "loss:  0.5715522696573889\n",
      "loss:  0.5713991463815037\n",
      "loss:  0.5712461286860828\n",
      "loss:  0.5710932164826633\n",
      "loss:  0.5709404096828505\n",
      "loss:  0.5707877081983193\n",
      "loss:  0.5706351119408126\n",
      "loss:  0.5704826208221431\n",
      "loss:  0.5703302347541913\n",
      "loss:  0.5701779536489073\n",
      "loss:  0.5700257774183096\n",
      "loss:  0.5698737059744855\n",
      "loss:  0.569721739229591\n",
      "loss:  0.569569877095851\n",
      "loss:  0.5694181194855591\n",
      "loss:  0.5692664663110776\n",
      "loss:  0.5691149174848372\n",
      "loss:  0.5689634729193378\n",
      "loss:  0.5688121325271476\n",
      "loss:  0.5686608962209038\n",
      "loss:  0.5685097639133116\n",
      "loss:  0.5683587355171458\n",
      "loss:  0.5682078109452491\n",
      "loss:  0.5680569901105329\n",
      "loss:  0.5679062729259773\n",
      "loss:  0.5677556593046312\n",
      "loss:  0.5676051491596116\n",
      "loss:  0.5674547424041043\n",
      "loss:  0.5673044389513636\n",
      "loss:  0.5671542387147126\n",
      "loss:  0.5670041416075423\n",
      "loss:  0.5668541475433125\n",
      "loss:  0.5667042564355519\n",
      "loss:  0.5665544681978568\n",
      "loss:  0.5664047827438925\n",
      "loss:  0.5662551999873928\n",
      "loss:  0.5661057198421595\n",
      "loss:  0.565956342222063\n",
      "loss:  0.5658070670410423\n",
      "loss:  0.5656578942131043\n",
      "loss:  0.5655088236523247\n",
      "loss:  0.565359855272847\n",
      "loss:  0.5652109889888836\n",
      "loss:  0.5650622247147147\n",
      "loss:  0.564913562364689\n",
      "loss:  0.5647650018532236\n",
      "loss:  0.5646165430948037\n",
      "loss:  0.5644681860039825\n",
      "loss:  0.5643199304953816\n",
      "loss:  0.5641717764836909\n",
      "loss:  0.5640237238836684\n",
      "loss:  0.5638757726101401\n",
      "loss:  0.5637279225780003\n",
      "loss:  0.5635801737022114\n",
      "loss:  0.5634325258978038\n",
      "loss:  0.5632849790798758\n",
      "loss:  0.5631375331635944\n",
      "loss:  0.562990188064194\n",
      "loss:  0.5628429436969772\n",
      "loss:  0.5626957999773148\n",
      "loss:  0.5625487568206453\n",
      "loss:  0.5624018141424756\n",
      "loss:  0.5622549718583797\n",
      "loss:  0.5621082298840006\n",
      "loss:  0.5619615881350483\n",
      "loss:  0.5618150465273013\n",
      "loss:  0.5616686049766056\n",
      "loss:  0.5615222633988752\n",
      "loss:  0.5613760217100919\n",
      "loss:  0.5612298798263052\n",
      "loss:  0.5610838376636329\n",
      "loss:  0.5609378951382596\n",
      "loss:  0.5607920521664385\n",
      "loss:  0.5606463086644901\n",
      "loss:  0.5605006645488028\n",
      "loss:  0.5603551197358325\n",
      "loss:  0.5602096741421029\n",
      "loss:  0.5600643276842053\n",
      "loss:  0.5599190802787987\n",
      "loss:  0.5597739318426096\n",
      "loss:  0.559628882292432\n",
      "loss:  0.5594839315451275\n",
      "loss:  0.5593390795176256\n",
      "loss:  0.5591943261269224\n",
      "loss:  0.5590496712900828\n",
      "loss:  0.5589051149242379\n",
      "loss:  0.5587606569465868\n",
      "loss:  0.5586162972743962\n",
      "loss:  0.558472035825\n",
      "loss:  0.5583278725157994\n",
      "loss:  0.5581838072642631\n",
      "loss:  0.558039839987927\n",
      "loss:  0.5578959706043943\n",
      "loss:  0.5577521990313357\n",
      "loss:  0.5576085251864888\n",
      "loss:  0.557464948987659\n",
      "loss:  0.5573214703527183\n",
      "loss:  0.5571780891996064\n",
      "loss:  0.5570348054463297\n",
      "loss:  0.5568916190109618\n",
      "loss:  0.556748529811644\n",
      "loss:  0.5566055377665843\n",
      "loss:  0.5564626427940575\n",
      "loss:  0.5563198448124058\n",
      "loss:  0.5561771437400386\n",
      "loss:  0.5560345394954315\n",
      "loss:  0.5558920319971281\n",
      "loss:  0.5557496211637384\n",
      "loss:  0.5556073069139392\n",
      "loss:  0.5554650891664745\n",
      "loss:  0.5553229678401554\n",
      "loss:  0.5551809428538589\n",
      "loss:  0.5550390141265301\n",
      "loss:  0.5548971815771799\n",
      "loss:  0.5547554451248865\n",
      "loss:  0.5546138046887945\n",
      "loss:  0.5544722601881157\n",
      "loss:  0.5543308115421284\n",
      "loss:  0.5541894586701769\n",
      "loss:  0.5540482014916733\n",
      "loss:  0.553907039926096\n",
      "loss:  0.5537659738929891\n",
      "loss:  0.5536250033119644\n",
      "loss:  0.5534841281026998\n",
      "loss:  0.5533433481849396\n",
      "loss:  0.5532026634784948\n",
      "loss:  0.5530620739032426\n",
      "loss:  0.5529215793791271\n",
      "loss:  0.5527811798261585\n",
      "loss:  0.5526408751644134\n",
      "loss:  0.5525006653140346\n",
      "loss:  0.5523605501952318\n",
      "loss:  0.5522205297282805\n",
      "loss:  0.5520806038335224\n",
      "loss:  0.5519407724313661\n",
      "loss:  0.5518010354422856\n",
      "loss:  0.5516613927868219\n",
      "loss:  0.5515218443855815\n",
      "loss:  0.5513823901592374\n",
      "loss:  0.5512430300285288\n",
      "loss:  0.5511037639142606\n",
      "loss:  0.5509645917373043\n",
      "loss:  0.5508255134185968\n",
      "loss:  0.5506865288791415\n",
      "loss:  0.5505476380400075\n",
      "loss:  0.5504088408223303\n",
      "loss:  0.5502701371473108\n",
      "loss:  0.5501315269362158\n",
      "loss:  0.5499930101103785\n",
      "loss:  0.5498545865911972\n",
      "loss:  0.549716256300137\n",
      "loss:  0.5495780191587276\n",
      "loss:  0.5494398750885654\n",
      "loss:  0.5493018240113119\n",
      "loss:  0.5491638658486947\n",
      "loss:  0.5490260005225069\n",
      "loss:  0.5488882279546072\n",
      "loss:  0.5487505480669203\n",
      "loss:  0.5486129607814355\n",
      "loss:  0.5484754660202088\n",
      "loss:  0.5483380637053613\n",
      "loss:  0.5482007537590791\n",
      "loss:  0.5480635361036144\n",
      "loss:  0.5479264106612842\n",
      "loss:  0.547789377354472\n",
      "loss:  0.5476524361056255\n",
      "loss:  0.5475155868372583\n",
      "loss:  0.5473788294719492\n",
      "loss:  0.5472421639323423\n",
      "loss:  0.5471055901411468\n",
      "loss:  0.5469691080211377\n",
      "loss:  0.5468327174951545\n",
      "loss:  0.5466964184861021\n",
      "loss:  0.5465602109169505\n",
      "loss:  0.546424094710735\n",
      "loss:  0.546288069790556\n",
      "loss:  0.5461521360795782\n",
      "loss:  0.5460162935010324\n",
      "loss:  0.5458805419782137\n",
      "loss:  0.5457448814344824\n",
      "loss:  0.5456093117932631\n",
      "loss:  0.5454738329780464\n",
      "loss:  0.545338444912387\n",
      "loss:  0.5452031475199046\n",
      "loss:  0.5450679407242834\n",
      "loss:  0.5449328244492732\n",
      "loss:  0.5447977986186872\n",
      "loss:  0.5446628631564043\n",
      "loss:  0.5445280179863681\n",
      "loss:  0.5443932630325865\n",
      "loss:  0.5442585982191319\n",
      "loss:  0.5441240234701413\n",
      "loss:  0.5439895387098168\n",
      "loss:  0.5438551438624242\n",
      "loss:  0.5437208388522943\n",
      "loss:  0.543586623603822\n",
      "loss:  0.5434524980414669\n",
      "loss:  0.543318462089753\n",
      "loss:  0.5431845156732684\n",
      "loss:  0.5430506587166658\n",
      "loss:  0.5429168911446618\n",
      "loss:  0.5427832128820377\n",
      "loss:  0.5426496238536385\n",
      "loss:  0.5425161239843737\n",
      "loss:  0.5423827131992173\n",
      "loss:  0.5422493914232067\n",
      "loss:  0.5421161585814438\n",
      "loss:  0.5419830145990944\n",
      "loss:  0.5418499594013885\n",
      "loss:  0.54171699291362\n",
      "loss:  0.5415841150611467\n",
      "loss:  0.5414513257693904\n",
      "loss:  0.5413186249638368\n",
      "loss:  0.5411860125700351\n",
      "loss:  0.541053488513599\n",
      "loss:  0.5409210527202055\n",
      "loss:  0.5407887051155955\n",
      "loss:  0.5406564456255734\n",
      "loss:  0.5405242741760077\n",
      "loss:  0.5403921906928302\n",
      "loss:  0.5402601951020363\n",
      "loss:  0.5401282873296855\n",
      "loss:  0.5399964673019002\n",
      "loss:  0.5398647349448669\n",
      "loss:  0.5397330901848351\n",
      "loss:  0.539601532948118\n",
      "loss:  0.5394700631610919\n",
      "loss:  0.5393386807501971\n",
      "loss:  0.5392073856419367\n",
      "loss:  0.5390761777628775\n",
      "loss:  0.5389450570396492\n",
      "loss:  0.5388140233989449\n",
      "loss:  0.5386830767675213\n",
      "loss:  0.5385522170721975\n",
      "loss:  0.5384214442398563\n",
      "loss:  0.5382907581974433\n",
      "loss:  0.5381601588719678\n",
      "loss:  0.5380296461905015\n",
      "loss:  0.5378992200801789\n",
      "loss:  0.5377688804681983\n",
      "loss:  0.5376386272818204\n",
      "loss:  0.5375084604483689\n",
      "loss:  0.5373783798952302\n",
      "loss:  0.5372483855498538\n",
      "loss:  0.5371184773397517\n",
      "loss:  0.536988655192499\n",
      "loss:  0.5368589190357336\n",
      "loss:  0.5367292687971554\n",
      "loss:  0.5365997044045274\n",
      "loss:  0.5364702257856757\n",
      "loss:  0.5363408328684879\n",
      "loss:  0.5362115255809152\n",
      "loss:  0.5360823038509707\n",
      "loss:  0.5359531676067301\n",
      "loss:  0.5358241167763315\n",
      "loss:  0.5356951512879756\n",
      "loss:  0.5355662710699254\n",
      "loss:  0.5354374760505061\n",
      "loss:  0.5353087661581052\n",
      "loss:  0.5351801413211723\n",
      "loss:  0.5350516014682203\n",
      "loss:  0.5349231465278225\n",
      "loss:  0.5347947764286158\n",
      "loss:  0.5346664910992986\n",
      "loss:  0.5345382904686317\n",
      "loss:  0.5344101744654374\n",
      "loss:  0.5342821430186006\n",
      "loss:  0.5341541960570677\n",
      "loss:  0.5340263335098475\n",
      "loss:  0.5338985553060104\n",
      "loss:  0.5337708613746887\n",
      "loss:  0.5336432516450763\n",
      "loss:  0.5335157260464296\n",
      "loss:  0.5333882845080661\n",
      "loss:  0.5332609269593652\n",
      "loss:  0.5331336533297677\n",
      "loss:  0.5330064635487768\n",
      "loss:  0.5328793575459567\n",
      "loss:  0.532752335250933\n",
      "loss:  0.5326253965933937\n",
      "loss:  0.5324985415030871\n",
      "loss:  0.5323717699098243\n",
      "loss:  0.5322450817434765\n",
      "loss:  0.5321184769339774\n",
      "loss:  0.531991955411321\n",
      "loss:  0.5318655171055637\n",
      "loss:  0.5317391619468224\n",
      "loss:  0.5316128898652756\n",
      "loss:  0.5314867007911626\n",
      "loss:  0.5313605946547844\n",
      "loss:  0.5312345713865028\n",
      "loss:  0.5311086309167407\n",
      "loss:  0.5309827731759822\n",
      "loss:  0.5308569980947722\n",
      "loss:  0.5307313056037165\n",
      "loss:  0.5306056956334826\n",
      "loss:  0.5304801681147977\n",
      "loss:  0.5303547229784511\n",
      "loss:  0.5302293601552919\n",
      "loss:  0.5301040795762305\n",
      "loss:  0.5299788811722377\n",
      "loss:  0.5298537648743458\n",
      "loss:  0.5297287306136468\n",
      "loss:  0.5296037783212939\n",
      "loss:  0.5294789079285008\n",
      "loss:  0.5293541193665419\n",
      "loss:  0.5292294125667515\n",
      "loss:  0.5291047874605251\n",
      "loss:  0.5289802439793186\n",
      "loss:  0.5288557820546478\n",
      "loss:  0.5287314016180892\n",
      "loss:  0.5286071026012797\n",
      "loss:  0.5284828849359163\n",
      "loss:  0.5283587485537565\n",
      "loss:  0.5282346933866174\n",
      "loss:  0.5281107193663774\n",
      "loss:  0.5279868264249739\n",
      "loss:  0.5278630144944049\n",
      "loss:  0.5277392835067286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.5276156333940631\n",
      "loss:  0.5274920640885864\n",
      "loss:  0.5273685755225366\n",
      "loss:  0.5272451676282114\n",
      "loss:  0.5271218403379687\n",
      "loss:  0.5269985935842261\n",
      "loss:  0.526875427299461\n",
      "loss:  0.5267523414162106\n",
      "loss:  0.5266293358670717\n",
      "loss:  0.526506410584701\n",
      "loss:  0.5263835655018148\n",
      "loss:  0.5262608005511885\n",
      "loss:  0.5261381156656578\n",
      "loss:  0.5260155107781176\n",
      "loss:  0.5258929858215219\n",
      "loss:  0.525770540728885\n",
      "loss:  0.5256481754332798\n",
      "loss:  0.525525889867839\n",
      "loss:  0.5254036839657544\n",
      "loss:  0.5252815576602772\n",
      "loss:  0.525159510884718\n",
      "loss:  0.5250375435724466\n",
      "loss:  0.5249156556568915\n",
      "loss:  0.5247938470715408\n",
      "loss:  0.5246721177499415\n",
      "loss:  0.5245504676256997\n",
      "loss:  0.5244288966324808\n",
      "loss:  0.5243074047040084\n",
      "loss:  0.524185991774066\n",
      "loss:  0.5240646577764954\n",
      "loss:  0.5239434026451972\n",
      "loss:  0.5238222263141311\n",
      "loss:  0.5237011287173157\n",
      "loss:  0.5235801097888279\n",
      "loss:  0.5234591694628035\n",
      "loss:  0.523338307673437\n",
      "loss:  0.5232175243549818\n",
      "loss:  0.5230968194417491\n",
      "loss:  0.5229761928681095\n",
      "loss:  0.5228556445684915\n",
      "loss:  0.5227351744773825\n",
      "loss:  0.5226147825293278\n",
      "loss:  0.5224944686589319\n",
      "loss:  0.5223742328008564\n",
      "loss:  0.5222540748898227\n",
      "loss:  0.5221339948606091\n",
      "loss:  0.522013992648053\n",
      "loss:  0.5218940681870496\n",
      "loss:  0.5217742214125526\n",
      "loss:  0.5216544522595732\n",
      "loss:  0.521534760663181\n",
      "loss:  0.5214151465585042\n",
      "loss:  0.5212956098807279\n",
      "loss:  0.5211761505650958\n",
      "loss:  0.5210567685469092\n",
      "loss:  0.5209374637615277\n",
      "loss:  0.5208182361443684\n",
      "loss:  0.5206990856309063\n",
      "loss:  0.5205800121566739\n",
      "loss:  0.5204610156572618\n",
      "loss:  0.5203420960683177\n",
      "loss:  0.5202232533255476\n",
      "loss:  0.5201044873647147\n",
      "loss:  0.5199857981216398\n",
      "loss:  0.5198671855322011\n",
      "loss:  0.5197486495323345\n",
      "loss:  0.5196301900580331\n",
      "loss:  0.5195118070453476\n",
      "loss:  0.5193935004303856\n",
      "loss:  0.5192752701493127\n",
      "loss:  0.519157116138351\n",
      "loss:  0.5190390383337805\n",
      "loss:  0.5189210366719381\n",
      "loss:  0.5188031110892175\n",
      "loss:  0.5186852615220701\n",
      "loss:  0.5185674879070039\n",
      "loss:  0.5184497901805842\n",
      "loss:  0.5183321682794332\n",
      "loss:  0.5182146221402298\n",
      "loss:  0.5180971516997102\n",
      "loss:  0.5179797568946672\n",
      "loss:  0.5178624376619505\n",
      "loss:  0.5177451939384665\n",
      "loss:  0.5176280256611785\n",
      "loss:  0.5175109327671062\n",
      "loss:  0.5173939151933263\n",
      "loss:  0.5172769728769718\n",
      "loss:  0.5171601057552327\n",
      "loss:  0.517043313765355\n",
      "loss:  0.5169265968446415\n",
      "loss:  0.5168099549304513\n",
      "loss:  0.5166933879602003\n",
      "loss:  0.51657689587136\n",
      "loss:  0.5164604786014592\n",
      "loss:  0.5163441360880822\n",
      "loss:  0.5162278682688699\n",
      "loss:  0.5161116750815194\n",
      "loss:  0.5159955564637838\n",
      "loss:  0.5158795123534725\n",
      "loss:  0.5157635426884508\n",
      "loss:  0.5156476474066406\n",
      "loss:  0.5155318264460187\n",
      "loss:  0.5154160797446188\n",
      "loss:  0.5153004072405304\n",
      "loss:  0.5151848088718985\n",
      "loss:  0.5150692845769245\n",
      "loss:  0.5149538342938647\n",
      "loss:  0.5148384579610322\n",
      "loss:  0.514723155516795\n",
      "loss:  0.5146079268995774\n",
      "loss:  0.5144927720478587\n",
      "loss:  0.5143776909001746\n",
      "loss:  0.5142626833951154\n",
      "loss:  0.5141477494713276\n",
      "loss:  0.5140328890675128\n",
      "loss:  0.5139181021224285\n",
      "loss:  0.513803388574887\n",
      "loss:  0.5136887483637563\n",
      "loss:  0.5135741814279596\n",
      "loss:  0.5134596877064757\n",
      "loss:  0.5133452671383378\n",
      "loss:  0.513230919662635\n",
      "loss:  0.5131166452185116\n",
      "loss:  0.5130024437451663\n",
      "loss:  0.5128883151818537\n",
      "loss:  0.5127742594678828\n",
      "loss:  0.5126602765426177\n",
      "loss:  0.5125463663454776\n",
      "loss:  0.5124325288159366\n",
      "loss:  0.5123187638935236\n",
      "loss:  0.5122050715178221\n",
      "loss:  0.5120914516284708\n",
      "loss:  0.5119779041651628\n",
      "loss:  0.5118644290676458\n",
      "loss:  0.5117510262757224\n",
      "loss:  0.5116376957292498\n",
      "loss:  0.5115244373681395\n",
      "loss:  0.5114112511323581\n",
      "loss:  0.5112981369619259\n",
      "loss:  0.5111850947969182\n",
      "loss:  0.5110721245774646\n",
      "loss:  0.5109592262437489\n",
      "loss:  0.5108463997360094\n",
      "loss:  0.5107336449945383\n",
      "loss:  0.5106209619596827\n",
      "loss:  0.5105083505718432\n",
      "loss:  0.5103958107714752\n",
      "loss:  0.5102833424990874\n",
      "loss:  0.5101709456952436\n",
      "loss:  0.5100586203005606\n",
      "loss:  0.5099463662557098\n",
      "loss:  0.5098341835014165\n",
      "loss:  0.5097220719784598\n",
      "loss:  0.5096100316276726\n",
      "loss:  0.5094980623899417\n",
      "loss:  0.5093861642062075\n",
      "loss:  0.5092743370174648\n",
      "loss:  0.5091625807647612\n",
      "loss:  0.5090508953891981\n",
      "loss:  0.5089392808319315\n",
      "loss:  0.5088277370341698\n",
      "loss:  0.5087162639371754\n",
      "loss:  0.5086048614822641\n",
      "loss:  0.5084935296108056\n",
      "loss:  0.5083822682642221\n",
      "loss:  0.50827107738399\n",
      "loss:  0.5081599569116386\n",
      "loss:  0.5080489067887506\n",
      "loss:  0.507937926956962\n",
      "loss:  0.5078270173579621\n",
      "loss:  0.5077161779334929\n",
      "loss:  0.5076054086253498\n",
      "loss:  0.5074947093753817\n",
      "loss:  0.5073840801254896\n",
      "loss:  0.5072735208176283\n",
      "loss:  0.5071630313938054\n",
      "loss:  0.507052611796081\n",
      "loss:  0.5069422619665683\n",
      "loss:  0.5068319818474337\n",
      "loss:  0.5067217713808959\n",
      "loss:  0.5066116305092264\n",
      "loss:  0.5065015591747497\n",
      "loss:  0.5063915573198425\n",
      "loss:  0.5062816248869347\n",
      "loss:  0.5061717618185084\n",
      "loss:  0.5060619680570981\n",
      "loss:  0.5059522435452911\n",
      "loss:  0.5058425882257274\n",
      "loss:  0.5057330020410985\n",
      "loss:  0.505623484934149\n",
      "loss:  0.5055140368476759\n",
      "loss:  0.5054046577245283\n",
      "loss:  0.505295347507607\n",
      "loss:  0.505186106139866\n",
      "loss:  0.5050769335643109\n",
      "loss:  0.5049678297239995\n",
      "loss:  0.5048587945620412\n",
      "loss:  0.5047498280215985\n",
      "loss:  0.5046409300458852\n",
      "loss:  0.504532100578167\n",
      "loss:  0.5044233395617619\n",
      "loss:  0.5043146469400392\n",
      "loss:  0.5042060226564209\n",
      "loss:  0.5040974666543799\n",
      "loss:  0.5039889788774414\n",
      "loss:  0.5038805592691821\n",
      "loss:  0.5037722077732304\n",
      "loss:  0.5036639243332663\n",
      "loss:  0.5035557088930216\n",
      "loss:  0.5034475613962792\n",
      "loss:  0.503339481786874\n",
      "loss:  0.5032314700086921\n",
      "loss:  0.5031235260056708\n",
      "loss:  0.5030156497217991\n",
      "loss:  0.5029078411011174\n",
      "loss:  0.5028001000877172\n",
      "loss:  0.5026924266257413\n",
      "loss:  0.5025848206593837\n",
      "val_oss:  3.283610230758643\n",
      "val_oss:  3.1405734080241783\n",
      "val_oss:  2.997758772954606\n",
      "val_oss:  2.855249788696984\n",
      "val_oss:  2.713146492229391\n",
      "val_oss:  2.5715816346740374\n",
      "val_oss:  2.4307309509346147\n",
      "val_oss:  2.290826225202729\n",
      "val_oss:  2.1521715471391882\n",
      "val_oss:  2.0151628848575833\n",
      "val_oss:  1.8803104979075929\n",
      "val_oss:  1.7482625771403395\n",
      "val_oss:  1.6198266136132886\n",
      "val_oss:  1.495982263919992\n",
      "val_oss:  1.3778761924708742\n",
      "val_oss:  1.2667866848156415\n",
      "val_oss:  1.16404615415017\n",
      "val_oss:  1.0709163675105076\n",
      "val_oss:  0.988426565861993\n",
      "val_oss:  0.917205921847688\n",
      "val_oss:  0.857358432822432\n",
      "val_oss:  0.8084259702685214\n",
      "val_oss:  0.7694577499058051\n",
      "val_oss:  0.7391643035053224\n",
      "val_oss:  0.7161052117892348\n",
      "val_oss:  0.6988580239794526\n",
      "val_oss:  0.6861362090073293\n",
      "val_oss:  0.6768497057554794\n",
      "val_oss:  0.6701191390300923\n",
      "val_oss:  0.6652607779302602\n",
      "val_oss:  0.6617574833419737\n",
      "val_oss:  0.6592260880410666\n",
      "val_oss:  0.6573870037674937\n",
      "val_oss:  0.6560385295842706\n",
      "val_oss:  0.6550363801265677\n",
      "val_oss:  0.654277990724434\n",
      "val_oss:  0.6536907968527353\n",
      "val_oss:  0.6532236408387512\n",
      "val_oss:  0.6528405533008421\n",
      "val_oss:  0.6525162953853162\n",
      "val_oss:  0.6522331852174468\n",
      "val_oss:  0.6519788500967453\n",
      "val_oss:  0.651744640434974\n",
      "val_oss:  0.651524513822381\n",
      "val_oss:  0.6513142515754702\n",
      "val_oss:  0.6511109096217144\n",
      "val_oss:  0.6509124341235308\n",
      "val_oss:  0.6507173926872611\n",
      "val_oss:  0.6505247865469856\n",
      "val_oss:  0.6503339194100651\n",
      "val_oss:  0.6501443059151254\n",
      "val_oss:  0.6499556077632976\n",
      "val_oss:  0.6497675891709513\n",
      "val_oss:  0.6495800858065826\n",
      "val_oss:  0.6493929831346474\n",
      "val_oss:  0.6492062013200455\n",
      "val_oss:  0.6490196847070946\n",
      "val_oss:  0.6488333944875284\n",
      "val_oss:  0.6486473035913132\n",
      "val_oss:  0.6484613931266372\n",
      "val_oss:  0.6482756498994684\n",
      "val_oss:  0.6480900646853756\n",
      "val_oss:  0.6479046310255004\n",
      "val_oss:  0.6477193443877272\n",
      "val_oss:  0.6475342015822889\n",
      "val_oss:  0.6473492003546392\n",
      "val_oss:  0.6471643391018143\n",
      "val_oss:  0.6469796166748356\n",
      "val_oss:  0.6467950322410456\n",
      "val_oss:  0.6466105851882006\n",
      "val_oss:  0.646426275057653\n",
      "val_oss:  0.646242101497799\n",
      "val_oss:  0.6460580642316441\n",
      "val_oss:  0.6458741630342048\n",
      "val_oss:  0.6456903977167634\n",
      "val_oss:  0.6455067681158966\n",
      "val_oss:  0.6453232740858289\n",
      "val_oss:  0.6451399154931078\n",
      "val_oss:  0.6449566922128873\n",
      "val_oss:  0.6447736041263428\n",
      "val_oss:  0.6445906511188672\n",
      "val_oss:  0.6444078330788121\n",
      "val_oss:  0.6442251498966126\n",
      "val_oss:  0.6440426014641755\n",
      "val_oss:  0.6438601876744532\n",
      "val_oss:  0.6436779084211466\n",
      "val_oss:  0.643495763598497\n",
      "val_oss:  0.6433137531011421\n",
      "val_oss:  0.6431318768240152\n",
      "val_oss:  0.6429501346622729\n",
      "val_oss:  0.6427685265112486\n",
      "val_oss:  0.6425870522664151\n",
      "val_oss:  0.6424057118233629\n",
      "val_oss:  0.6422245050777816\n",
      "val_oss:  0.6420434319254497\n",
      "val_oss:  0.6418624922622251\n",
      "val_oss:  0.6416816859840396\n",
      "val_oss:  0.6415010129868949\n",
      "val_oss:  0.6413204731668604\n",
      "val_oss:  0.64114006642007\n",
      "val_oss:  0.6409597926427214\n",
      "val_oss:  0.6407796517310752\n",
      "val_oss:  0.640599643581454\n",
      "val_oss:  0.6404197680902419\n",
      "val_oss:  0.640240025153884\n",
      "val_oss:  0.6400604146688861\n",
      "val_oss:  0.6398809365318159\n",
      "val_oss:  0.6397015906393005\n",
      "val_oss:  0.6395223768880283\n",
      "val_oss:  0.6393432951747482\n",
      "val_oss:  0.6391643453962693\n",
      "val_oss:  0.6389855274494618\n",
      "val_oss:  0.6388068412312562\n",
      "val_oss:  0.6386282866386429\n",
      "val_oss:  0.6384498635686743\n",
      "val_oss:  0.638271571918462\n",
      "val_oss:  0.6380934115851786\n",
      "val_oss:  0.6379153824660575\n",
      "val_oss:  0.6377374844583932\n",
      "val_oss:  0.6375597174595397\n",
      "val_oss:  0.6373820813669124\n",
      "val_oss:  0.6372045760779879\n",
      "val_oss:  0.6370272014903026\n",
      "val_oss:  0.6368499575014542\n",
      "val_oss:  0.6366728440091008\n",
      "val_oss:  0.6364958609109623\n",
      "val_oss:  0.6363190081048185\n",
      "val_oss:  0.6361422854885104\n",
      "val_oss:  0.6359656929599399\n",
      "val_oss:  0.6357892304170701\n",
      "val_oss:  0.6356128977579251\n",
      "val_oss:  0.6354366948805891\n",
      "val_oss:  0.6352606216832087\n",
      "val_oss:  0.6350846780639908\n",
      "val_oss:  0.6349088639212034\n",
      "val_oss:  0.634733179153176\n",
      "val_oss:  0.6345576236582987\n",
      "val_oss:  0.6343821973350231\n",
      "val_oss:  0.6342069000818623\n",
      "val_oss:  0.6340317317973903\n",
      "val_oss:  0.6338566923802423\n",
      "val_oss:  0.6336817817291148\n",
      "val_oss:  0.6335069997427658\n",
      "val_oss:  0.6333323463200148\n",
      "val_oss:  0.6331578213597422\n",
      "val_oss:  0.63298342476089\n",
      "val_oss:  0.632809156422462\n",
      "val_oss:  0.6326350162435229\n",
      "val_oss:  0.6324610041231993\n",
      "val_oss:  0.6322871199606787\n",
      "val_oss:  0.6321133636552108\n",
      "val_oss:  0.6319397351061066\n",
      "val_oss:  0.6317662342127386\n",
      "val_oss:  0.6315928608745411\n",
      "val_oss:  0.6314196149910098\n",
      "val_oss:  0.6312464964617022\n",
      "val_oss:  0.6310735051862372\n",
      "val_oss:  0.630900641064296\n",
      "val_oss:  0.630727903995621\n",
      "val_oss:  0.6305552938800164\n",
      "val_oss:  0.6303828106173482\n",
      "val_oss:  0.6302104541075447\n",
      "val_oss:  0.6300382242505951\n",
      "val_oss:  0.629866120946551\n",
      "val_oss:  0.6296941440955264\n",
      "val_oss:  0.6295222935976953\n",
      "val_oss:  0.6293505693532961\n",
      "val_oss:  0.6291789712626272\n",
      "val_oss:  0.6290074992260503\n",
      "val_oss:  0.6288361531439876\n",
      "val_oss:  0.6286649329169248\n",
      "val_oss:  0.6284938384454084\n",
      "val_oss:  0.6283228696300479\n",
      "val_oss:  0.6281520263715145\n",
      "val_oss:  0.6279813085705408\n",
      "val_oss:  0.6278107161279227\n",
      "val_oss:  0.6276402489445171\n",
      "val_oss:  0.6274699069212442\n",
      "val_oss:  0.6272996899590852\n",
      "val_oss:  0.6271295979590843\n",
      "val_oss:  0.6269596308223473\n",
      "val_oss:  0.6267897884500426\n",
      "val_oss:  0.6266200707434009\n",
      "val_oss:  0.6264504776037148\n",
      "val_oss:  0.6262810089323395\n",
      "val_oss:  0.6261116646306925\n",
      "val_oss:  0.6259424446002532\n",
      "val_oss:  0.6257733487425634\n",
      "val_oss:  0.625604376959228\n",
      "val_oss:  0.6254355291519134\n",
      "val_oss:  0.6252668052223488\n",
      "val_oss:  0.6250982050723254\n",
      "val_oss:  0.6249297286036977\n",
      "val_oss:  0.6247613757183812\n",
      "val_oss:  0.6245931463183556\n",
      "val_oss:  0.6244250403056614\n",
      "val_oss:  0.6242570575824028\n",
      "val_oss:  0.6240891980507458\n",
      "val_oss:  0.6239214616129193\n",
      "val_oss:  0.6237538481712145\n",
      "val_oss:  0.6235863576279852\n",
      "val_oss:  0.623418989885648\n",
      "val_oss:  0.6232517448466819\n",
      "val_oss:  0.6230846224136279\n",
      "val_oss:  0.6229176224890907\n",
      "val_oss:  0.6227507449757371\n",
      "val_oss:  0.6225839897762964\n",
      "val_oss:  0.6224173567935607\n",
      "val_oss:  0.6222508459303848\n",
      "val_oss:  0.6220844570896864\n",
      "val_oss:  0.621918190174445\n",
      "val_oss:  0.6217520450877041\n",
      "val_oss:  0.6215860217325688\n",
      "val_oss:  0.6214201200122077\n",
      "val_oss:  0.6212543398298518\n",
      "val_oss:  0.6210886810887949\n",
      "val_oss:  0.6209231436923938\n",
      "val_oss:  0.6207577275440673\n",
      "val_oss:  0.6205924325472983\n",
      "val_oss:  0.6204272586056314\n",
      "val_oss:  0.6202622056226745\n",
      "val_oss:  0.6200972735020983\n",
      "val_oss:  0.6199324621476362\n",
      "val_oss:  0.619767771463085\n",
      "val_oss:  0.6196032013523035\n",
      "val_oss:  0.619438751719214\n",
      "val_oss:  0.6192744224678017\n",
      "val_oss:  0.6191102135021141\n",
      "val_oss:  0.6189461247262624\n",
      "val_oss:  0.6187821560444207\n",
      "val_oss:  0.6186183073608251\n",
      "val_oss:  0.6184545785797756\n",
      "val_oss:  0.6182909696056348\n",
      "val_oss:  0.6181274803428283\n",
      "val_oss:  0.6179641106958449\n",
      "val_oss:  0.6178008605692358\n",
      "val_oss:  0.617637729867616\n",
      "val_oss:  0.6174747184956629\n",
      "val_oss:  0.617311826358117\n",
      "val_oss:  0.6171490533597821\n",
      "val_oss:  0.6169863994055246\n",
      "val_oss:  0.6168238644002748\n",
      "val_oss:  0.616661448249025\n",
      "val_oss:  0.6164991508568313\n",
      "val_oss:  0.6163369721288124\n",
      "val_oss:  0.6161749119701503\n",
      "val_oss:  0.6160129702860901\n",
      "val_oss:  0.6158511469819401\n",
      "val_oss:  0.6156894419630715\n",
      "val_oss:  0.6155278551349189\n",
      "val_oss:  0.6153663864029794\n",
      "val_oss:  0.6152050356728136\n",
      "val_oss:  0.6150438028500457\n",
      "val_oss:  0.6148826878403624\n",
      "val_oss:  0.6147216905495133\n",
      "val_oss:  0.6145608108833124\n",
      "val_oss:  0.6144000487476354\n",
      "val_oss:  0.6142394040484218\n",
      "val_oss:  0.6140788766916749\n",
      "val_oss:  0.6139184665834599\n",
      "val_oss:  0.6137581736299061\n",
      "val_oss:  0.6135979977372058\n",
      "val_oss:  0.6134379388116145\n",
      "val_oss:  0.6132779967594503\n",
      "val_oss:  0.6131181714870955\n",
      "val_oss:  0.6129584629009949\n",
      "val_oss:  0.6127988709076568\n",
      "val_oss:  0.6126393954136526\n",
      "val_oss:  0.6124800363256172\n",
      "val_oss:  0.6123207935502479\n",
      "val_oss:  0.6121616669943069\n",
      "val_oss:  0.6120026565646173\n",
      "val_oss:  0.6118437621680677\n",
      "val_oss:  0.6116849837116082\n",
      "val_oss:  0.6115263211022535\n",
      "val_oss:  0.6113677742470804\n",
      "val_oss:  0.61120934305323\n",
      "val_oss:  0.6110510274279058\n",
      "val_oss:  0.610892827278375\n",
      "val_oss:  0.6107347425119679\n",
      "val_oss:  0.6105767730360782\n",
      "val_oss:  0.6104189187581629\n",
      "val_oss:  0.6102611795857419\n",
      "val_oss:  0.6101035554263987\n",
      "val_oss:  0.6099460461877804\n",
      "val_oss:  0.6097886517775963\n",
      "val_oss:  0.6096313721036202\n",
      "val_oss:  0.6094742070736882\n",
      "val_oss:  0.6093171565957006\n",
      "val_oss:  0.6091602205776202\n",
      "val_oss:  0.6090033989274736\n",
      "val_oss:  0.6088466915533499\n",
      "val_oss:  0.6086900983634026\n",
      "val_oss:  0.6085336192658479\n",
      "val_oss:  0.608377254168965\n",
      "val_oss:  0.6082210029810972\n",
      "val_oss:  0.60806486561065\n",
      "val_oss:  0.6079088419660932\n",
      "val_oss:  0.6077529319559594\n",
      "val_oss:  0.6075971354888444\n",
      "val_oss:  0.6074414524734075\n",
      "val_oss:  0.6072858828183713\n",
      "val_oss:  0.6071304264325216\n",
      "val_oss:  0.6069750832247074\n",
      "val_oss:  0.6068198531038415\n",
      "val_oss:  0.606664735978899\n",
      "val_oss:  0.6065097317589192\n",
      "val_oss:  0.606354840353004\n",
      "val_oss:  0.6062000616703195\n",
      "val_oss:  0.6060453956200939\n",
      "val_oss:  0.6058908421116196\n",
      "val_oss:  0.6057364010542519\n",
      "val_oss:  0.6055820723574094\n",
      "val_oss:  0.605427855930574\n",
      "val_oss:  0.6052737516832907\n",
      "val_oss:  0.6051197595251682\n",
      "val_oss:  0.6049658793658782\n",
      "val_oss:  0.6048121111151555\n",
      "val_oss:  0.6046584546827986\n",
      "val_oss:  0.6045049099786683\n",
      "val_oss:  0.6043514769126902\n",
      "val_oss:  0.6041981553948519\n",
      "val_oss:  0.6040449453352046\n",
      "val_oss:  0.6038918466438629\n",
      "val_oss:  0.6037388592310041\n",
      "val_oss:  0.6035859830068698\n",
      "val_oss:  0.6034332178817641\n",
      "val_oss:  0.6032805637660539\n",
      "val_oss:  0.6031280205701703\n",
      "val_oss:  0.6029755882046072\n",
      "val_oss:  0.6028232665799214\n",
      "val_oss:  0.6026710556067336\n",
      "val_oss:  0.6025189551957271\n",
      "val_oss:  0.6023669652576484\n",
      "val_oss:  0.6022150857033077\n",
      "val_oss:  0.6020633164435782\n",
      "val_oss:  0.601911657389396\n",
      "val_oss:  0.6017601084517609\n",
      "val_oss:  0.6016086695417351\n",
      "val_oss:  0.6014573405704449\n",
      "val_oss:  0.6013061214490789\n",
      "val_oss:  0.6011550120888899\n",
      "val_oss:  0.6010040124011928\n",
      "val_oss:  0.6008531222973662\n",
      "val_oss:  0.6007023416888515\n",
      "val_oss:  0.6005516704871539\n",
      "val_oss:  0.600401108603841\n",
      "val_oss:  0.6002506559505442\n",
      "val_oss:  0.6001003124389572\n",
      "val_oss:  0.5999500779808377\n",
      "val_oss:  0.5997999524880057\n",
      "val_oss:  0.5996499358723449\n",
      "val_oss:  0.5995000280458022\n",
      "val_oss:  0.5993502289203866\n",
      "val_oss:  0.5992005384081713\n",
      "val_oss:  0.5990509564212921\n",
      "val_oss:  0.5989014828719477\n",
      "val_oss:  0.5987521176724003\n",
      "val_oss:  0.5986028607349745\n",
      "val_oss:  0.5984537119720588\n",
      "val_oss:  0.598304671296104\n",
      "val_oss:  0.5981557386196241\n",
      "val_oss:  0.5980069138551964\n",
      "val_oss:  0.5978581969154612\n",
      "val_oss:  0.5977095877131212\n",
      "val_oss:  0.5975610861609428\n",
      "val_oss:  0.5974126921717551\n",
      "val_oss:  0.5972644056584501\n",
      "val_oss:  0.5971162265339828\n",
      "val_oss:  0.5969681547113711\n",
      "val_oss:  0.5968201901036964\n",
      "val_oss:  0.5966723326241019\n",
      "val_oss:  0.5965245821857952\n",
      "val_oss:  0.5963769387020453\n",
      "val_oss:  0.5962294020861854\n",
      "val_oss:  0.5960819722516106\n",
      "val_oss:  0.5959346491117796\n",
      "val_oss:  0.595787432580214\n",
      "val_oss:  0.5956403225704975\n",
      "val_oss:  0.5954933189962773\n",
      "val_oss:  0.5953464217712635\n",
      "val_oss:  0.5951996308092287\n",
      "val_oss:  0.5950529460240085\n",
      "val_oss:  0.594906367329501\n",
      "val_oss:  0.5947598946396679\n",
      "val_oss:  0.5946135278685329\n",
      "val_oss:  0.5944672669301831\n",
      "val_oss:  0.5943211117387674\n",
      "val_oss:  0.5941750622084991\n",
      "val_oss:  0.5940291182536526\n",
      "val_oss:  0.593883279788566\n",
      "val_oss:  0.5937375467276397\n",
      "val_oss:  0.5935919189853371\n",
      "val_oss:  0.5934463964761841\n",
      "val_oss:  0.5933009791147695\n",
      "val_oss:  0.5931556668157448\n",
      "val_oss:  0.5930104594938236\n",
      "val_oss:  0.5928653570637831\n",
      "val_oss:  0.5927203594404623\n",
      "val_oss:  0.5925754665387633\n",
      "val_oss:  0.592430678273651\n",
      "val_oss:  0.5922859945601521\n",
      "val_oss:  0.5921414153133567\n",
      "val_oss:  0.5919969404484177\n",
      "val_oss:  0.5918525698805491\n",
      "val_oss:  0.5917083035250293\n",
      "val_oss:  0.5915641412971979\n",
      "val_oss:  0.5914200831124579\n",
      "val_oss:  0.5912761288862741\n",
      "val_oss:  0.5911322785341745\n",
      "val_oss:  0.5909885319717488\n",
      "val_oss:  0.59084488911465\n",
      "val_oss:  0.590701349878593\n",
      "val_oss:  0.5905579141793555\n",
      "val_oss:  0.5904145819327776\n",
      "val_oss:  0.5902713530547617\n",
      "val_oss:  0.5901282274612722\n",
      "val_oss:  0.589985205068337\n",
      "val_oss:  0.5898422857920453\n",
      "val_oss:  0.5896994695485491\n",
      "val_oss:  0.5895567562540628\n",
      "val_oss:  0.5894141458248631\n",
      "val_oss:  0.5892716381772889\n",
      "val_oss:  0.5891292332277417\n",
      "val_oss:  0.5889869308926852\n",
      "val_oss:  0.588844731088645\n",
      "val_oss:  0.5887026337322093\n",
      "val_oss:  0.5885606387400287\n",
      "val_oss:  0.5884187460288158\n",
      "val_oss:  0.5882769555153454\n",
      "val_oss:  0.5881352671164548\n",
      "val_oss:  0.5879936807490428\n",
      "val_oss:  0.5878521963300712\n",
      "val_oss:  0.5877108137765636\n",
      "val_oss:  0.5875695330056057\n",
      "val_oss:  0.5874283539343452\n",
      "val_oss:  0.5872872764799923\n",
      "val_oss:  0.5871463005598193\n",
      "val_oss:  0.5870054260911598\n",
      "val_oss:  0.5868646529914103\n",
      "val_oss:  0.5867239811780289\n",
      "val_oss:  0.5865834105685365\n",
      "val_oss:  0.5864429410805145\n",
      "val_oss:  0.5863025726316079\n",
      "val_oss:  0.5861623051395226\n",
      "val_oss:  0.5860221385220273\n",
      "val_oss:  0.5858820726969518\n",
      "val_oss:  0.5857421075821885\n",
      "val_oss:  0.585602243095691\n",
      "val_oss:  0.5854624791554757\n",
      "val_oss:  0.5853228156796201\n",
      "val_oss:  0.5851832525862645\n",
      "val_oss:  0.5850437897936094\n",
      "val_oss:  0.5849044272199191\n",
      "val_oss:  0.5847651647835179\n",
      "val_oss:  0.5846260024027936\n",
      "val_oss:  0.5844869399961945\n",
      "val_oss:  0.584347977482231\n",
      "val_oss:  0.5842091147794753\n",
      "val_oss:  0.5840703518065619\n",
      "val_oss:  0.5839316884821856\n",
      "val_oss:  0.5837931247251043\n",
      "val_oss:  0.5836546604541366\n",
      "val_oss:  0.5835162955881636\n",
      "val_oss:  0.5833780300461271\n",
      "val_oss:  0.5832398637470312\n",
      "val_oss:  0.5831017966099414\n",
      "val_oss:  0.5829638285539845\n",
      "val_oss:  0.5828259594983495\n",
      "val_oss:  0.582688189362286\n",
      "val_oss:  0.582550518065106\n",
      "val_oss:  0.5824129455261826\n",
      "val_oss:  0.5822754716649505\n",
      "val_oss:  0.5821380964009053\n",
      "val_oss:  0.5820008196536048\n",
      "val_oss:  0.5818636413426682\n",
      "val_oss:  0.5817265613877756\n",
      "val_oss:  0.5815895797086685\n",
      "val_oss:  0.5814526962251503\n",
      "val_oss:  0.5813159108570853\n",
      "val_oss:  0.5811792235243993\n",
      "val_oss:  0.5810426341470791\n",
      "val_oss:  0.5809061426451734\n",
      "val_oss:  0.5807697489387914\n",
      "val_oss:  0.5806334529481041\n",
      "val_oss:  0.5804972545933438\n",
      "val_oss:  0.5803611537948031\n",
      "val_oss:  0.5802251504728371\n",
      "val_oss:  0.5800892445478609\n",
      "val_oss:  0.5799534359403515\n",
      "val_oss:  0.5798177245708469\n",
      "val_oss:  0.5796821103599457\n",
      "val_oss:  0.5795465932283083\n",
      "val_oss:  0.5794111730966552\n",
      "val_oss:  0.5792758498857691\n",
      "val_oss:  0.579140623516493\n",
      "val_oss:  0.5790054939097307\n",
      "val_oss:  0.5788704609864478\n",
      "val_oss:  0.5787355246676701\n",
      "val_oss:  0.5786006848744847\n",
      "val_oss:  0.5784659415280394\n",
      "val_oss:  0.5783312945495428\n",
      "val_oss:  0.5781967438602652\n",
      "val_oss:  0.5780622893815364\n",
      "val_oss:  0.5779279310347484\n",
      "val_oss:  0.5777936687413524\n",
      "val_oss:  0.5776595024228625\n",
      "val_oss:  0.5775254320008514\n",
      "val_oss:  0.5773914573969539\n",
      "val_oss:  0.5772575785328652\n",
      "val_oss:  0.577123795330341\n",
      "val_oss:  0.5769901077111977\n",
      "val_oss:  0.5768565155973124\n",
      "val_oss:  0.5767230189106234\n",
      "val_oss:  0.5765896175731285\n",
      "val_oss:  0.5764563115068867\n",
      "val_oss:  0.5763231006340178\n",
      "val_oss:  0.5761899848767016\n",
      "val_oss:  0.5760569641571788\n",
      "val_oss:  0.5759240383977504\n",
      "val_oss:  0.575791207520778\n",
      "val_oss:  0.5756584714486832\n",
      "val_oss:  0.5755258301039489\n",
      "val_oss:  0.5753932834091177\n",
      "val_oss:  0.5752608312867925\n",
      "val_oss:  0.5751284736596374\n",
      "val_oss:  0.5749962104503756\n",
      "val_oss:  0.5748640415817916\n",
      "val_oss:  0.5747319669767301\n",
      "val_oss:  0.5745999865580952\n",
      "val_oss:  0.5744681002488522\n",
      "val_oss:  0.5743363079720265\n",
      "val_oss:  0.5742046096507026\n",
      "val_oss:  0.5740730052080272\n",
      "val_oss:  0.573941494567205\n",
      "val_oss:  0.5738100776515022\n",
      "val_oss:  0.5736787543842444\n",
      "val_oss:  0.573547524688818\n",
      "val_oss:  0.5734163884886684\n",
      "val_oss:  0.573285345707302\n",
      "val_oss:  0.5731543962682849\n",
      "val_oss:  0.5730235400952428\n",
      "val_oss:  0.5728927771118616\n",
      "val_oss:  0.5727621072418876\n",
      "val_oss:  0.5726315304091262\n",
      "val_oss:  0.572501046537443\n",
      "val_oss:  0.5723706555507635\n",
      "val_oss:  0.5722403573730735\n",
      "val_oss:  0.5721101519284177\n",
      "val_oss:  0.5719800391409009\n",
      "val_oss:  0.571850018934688\n",
      "val_oss:  0.5717200912340036\n",
      "val_oss:  0.5715902559631312\n",
      "val_oss:  0.5714605130464153\n",
      "val_oss:  0.5713308624082587\n",
      "val_oss:  0.571201303973125\n",
      "val_oss:  0.5710718376655365\n",
      "val_oss:  0.5709424634100754\n",
      "val_oss:  0.5708131811313838\n",
      "val_oss:  0.570683990754163\n",
      "val_oss:  0.5705548922031738\n",
      "val_oss:  0.5704258854032364\n",
      "val_oss:  0.5702969702792305\n",
      "val_oss:  0.5701681467560957\n",
      "val_oss:  0.5700394147588302\n",
      "val_oss:  0.5699107742124921\n",
      "val_oss:  0.5697822250421988\n",
      "val_oss:  0.5696537671731271\n",
      "val_oss:  0.5695254005305127\n",
      "val_oss:  0.5693971250396509\n",
      "val_oss:  0.5692689406258962\n",
      "val_oss:  0.5691408472146624\n",
      "val_oss:  0.5690128447314221\n",
      "val_oss:  0.5688849331017078\n",
      "val_oss:  0.5687571122511103\n",
      "val_oss:  0.56862938210528\n",
      "val_oss:  0.5685017425899268\n",
      "val_oss:  0.5683741936308184\n",
      "val_oss:  0.5682467351537828\n",
      "val_oss:  0.5681193670847067\n",
      "val_oss:  0.5679920893495352\n",
      "val_oss:  0.5678649018742729\n",
      "val_oss:  0.5677378045849834\n",
      "val_oss:  0.5676107974077886\n",
      "val_oss:  0.5674838802688702\n",
      "val_oss:  0.5673570530944679\n",
      "val_oss:  0.5672303158108809\n",
      "val_oss:  0.5671036683444666\n",
      "val_oss:  0.5669771106216414\n",
      "val_oss:  0.5668506425688807\n",
      "val_oss:  0.566724264112718\n",
      "val_oss:  0.5665979751797465\n",
      "val_oss:  0.5664717756966173\n",
      "val_oss:  0.5663456655900398\n",
      "val_oss:  0.5662196447867828\n",
      "val_oss:  0.5660937132136739\n",
      "val_oss:  0.5659678707975978\n",
      "val_oss:  0.5658421174654994\n",
      "val_oss:  0.565716453144381\n",
      "val_oss:  0.5655908777613041\n",
      "val_oss:  0.5654653912433878\n",
      "val_oss:  0.5653399935178105\n",
      "val_oss:  0.5652146845118086\n",
      "val_oss:  0.5650894641526765\n",
      "val_oss:  0.5649643323677676\n",
      "val_oss:  0.564839289084493\n",
      "val_oss:  0.5647143342303228\n",
      "val_oss:  0.5645894677327847\n",
      "val_oss:  0.5644646895194645\n",
      "val_oss:  0.5643399995180073\n",
      "val_oss:  0.5642153976561151\n",
      "val_oss:  0.5640908838615486\n",
      "val_oss:  0.5639664580621265\n",
      "val_oss:  0.563842120185726\n",
      "val_oss:  0.5637178701602816\n",
      "val_oss:  0.5635937079137864\n",
      "val_oss:  0.5634696333742912\n",
      "val_oss:  0.563345646469905\n",
      "val_oss:  0.5632217471287947\n",
      "val_oss:  0.5630979352791851\n",
      "val_oss:  0.5629742108493586\n",
      "val_oss:  0.5628505737676557\n",
      "val_oss:  0.5627270239624748\n",
      "val_oss:  0.562603561362272\n",
      "val_oss:  0.5624801858955609\n",
      "val_oss:  0.5623568974909139\n",
      "val_oss:  0.5622336960769595\n",
      "val_oss:  0.5621105815823852\n",
      "val_oss:  0.5619875539359355\n",
      "val_oss:  0.5618646130664127\n",
      "val_oss:  0.5617417589026766\n",
      "val_oss:  0.5616189913736449\n",
      "val_oss:  0.5614963104082924\n",
      "val_oss:  0.5613737159356518\n",
      "val_oss:  0.5612512078848129\n",
      "val_oss:  0.561128786184923\n",
      "val_oss:  0.5610064507651873\n",
      "val_oss:  0.5608842015548681\n",
      "val_oss:  0.5607620384832848\n",
      "val_oss:  0.5606399614798143\n",
      "val_oss:  0.5605179704738908\n",
      "val_oss:  0.5603960653950065\n",
      "val_oss:  0.5602742461727093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_oss:  0.5601525127366056\n",
      "val_oss:  0.5600308650163587\n",
      "val_oss:  0.5599093029416888\n",
      "val_oss:  0.5597878264423735\n",
      "val_oss:  0.5596664354482471\n",
      "val_oss:  0.5595451298892015\n",
      "val_oss:  0.5594239096951853\n",
      "val_oss:  0.559302774796204\n",
      "val_oss:  0.5591817251223206\n",
      "val_oss:  0.5590607606036546\n",
      "val_oss:  0.5589398811703824\n",
      "val_oss:  0.5588190867527375\n",
      "val_oss:  0.5586983772810101\n",
      "val_oss:  0.5585777526855475\n",
      "val_oss:  0.5584572128967538\n",
      "val_oss:  0.5583367578450891\n",
      "val_oss:  0.5582163874610714\n",
      "val_oss:  0.5580961016752742\n",
      "val_oss:  0.5579759004183292\n",
      "val_oss:  0.5578557836209231\n",
      "val_oss:  0.5577357512138001\n",
      "val_oss:  0.5576158031277613\n",
      "val_oss:  0.5574959392936635\n",
      "val_oss:  0.5573761596424206\n",
      "val_oss:  0.5572564641050028\n",
      "val_oss:  0.5571368526124371\n",
      "val_oss:  0.5570173250958061\n",
      "val_oss:  0.5568978814862499\n",
      "val_oss:  0.5567785217149642\n",
      "val_oss:  0.5566592457132012\n",
      "val_oss:  0.5565400534122696\n",
      "val_oss:  0.5564209447435345\n",
      "val_oss:  0.5563019196384168\n",
      "val_oss:  0.5561829780283936\n",
      "val_oss:  0.5560641198449989\n",
      "val_oss:  0.5559453450198222\n",
      "val_oss:  0.5558266534845094\n",
      "val_oss:  0.5557080451707622\n",
      "val_oss:  0.5555895200103389\n",
      "val_oss:  0.5554710779350535\n",
      "val_oss:  0.555352718876776\n",
      "val_oss:  0.5552344427674322\n",
      "val_oss:  0.5551162495390043\n",
      "val_oss:  0.55499813912353\n",
      "val_oss:  0.5548801114531037\n",
      "val_oss:  0.5547621664598739\n",
      "val_oss:  0.5546443040760469\n",
      "val_oss:  0.5545265242338836\n",
      "val_oss:  0.5544088268657008\n",
      "val_oss:  0.5542912119038715\n",
      "val_oss:  0.5541736792808243\n",
      "val_oss:  0.5540562289290426\n",
      "val_oss:  0.5539388607810665\n",
      "val_oss:  0.5538215747694911\n",
      "val_oss:  0.5537043708269674\n",
      "val_oss:  0.5535872488862019\n",
      "val_oss:  0.5534702088799562\n",
      "val_oss:  0.5533532507410479\n",
      "val_oss:  0.5532363744023495\n",
      "val_oss:  0.5531195797967896\n",
      "val_oss:  0.5530028668573519\n",
      "val_oss:  0.5528862355170746\n",
      "val_oss:  0.5527696857090528\n",
      "val_oss:  0.5526532173664357\n",
      "val_oss:  0.5525368304224282\n",
      "val_oss:  0.5524205248102901\n",
      "val_oss:  0.5523043004633367\n",
      "val_oss:  0.5521881573149386\n",
      "val_oss:  0.5520720952985211\n",
      "val_oss:  0.5519561143475648\n",
      "val_oss:  0.5518402143956053\n",
      "val_oss:  0.5517243953762335\n",
      "val_oss:  0.5516086572230948\n",
      "val_oss:  0.5514929998698903\n",
      "val_oss:  0.5513774232503749\n",
      "val_oss:  0.5512619272983594\n",
      "val_oss:  0.5511465119477095\n",
      "val_oss:  0.551031177132345\n",
      "val_oss:  0.5509159227862409\n",
      "val_oss:  0.5508007488434273\n",
      "val_oss:  0.5506856552379881\n",
      "val_oss:  0.550570641904063\n",
      "val_oss:  0.5504557087758458\n",
      "val_oss:  0.5503408557875851\n",
      "val_oss:  0.5502260828735838\n",
      "val_oss:  0.5501113899682\n",
      "val_oss:  0.5499967770058456\n",
      "val_oss:  0.5498822439209875\n",
      "val_oss:  0.5497677906481471\n",
      "val_oss:  0.5496534171219001\n",
      "val_oss:  0.5495391232768766\n",
      "val_oss:  0.5494249090477611\n",
      "val_oss:  0.5493107743692924\n",
      "val_oss:  0.549196719176264\n",
      "val_oss:  0.5490827434035231\n",
      "val_oss:  0.5489688469859718\n",
      "val_oss:  0.5488550298585656\n",
      "val_oss:  0.548741291956315\n",
      "val_oss:  0.5486276332142843\n",
      "val_oss:  0.5485140535675916\n",
      "val_oss:  0.5484005529514098\n",
      "val_oss:  0.5482871313009653\n",
      "val_oss:  0.5481737885515389\n",
      "val_oss:  0.548060524638465\n",
      "val_oss:  0.5479473394971325\n",
      "val_oss:  0.5478342330629835\n",
      "val_oss:  0.5477212052715147\n",
      "val_oss:  0.5476082560582762\n",
      "val_oss:  0.5474953853588722\n",
      "val_oss:  0.5473825931089604\n",
      "val_oss:  0.5472698792442527\n",
      "val_oss:  0.5471572437005143\n",
      "val_oss:  0.5470446864135644\n",
      "val_oss:  0.546932207319276\n",
      "val_oss:  0.5468198063535746\n",
      "val_oss:  0.5467074834524414\n",
      "val_oss:  0.546595238551909\n",
      "val_oss:  0.5464830715880646\n",
      "val_oss:  0.5463709824970493\n",
      "val_oss:  0.5462589712150566\n",
      "val_oss:  0.5461470376783344\n",
      "val_oss:  0.5460351818231834\n",
      "val_oss:  0.5459234035859577\n",
      "val_oss:  0.545811702903065\n",
      "val_oss:  0.5457000797109663\n",
      "val_oss:  0.5455885339461759\n",
      "val_oss:  0.5454770655452607\n",
      "val_oss:  0.5453656744448415\n",
      "val_oss:  0.5452543605815923\n",
      "val_oss:  0.5451431238922396\n",
      "val_oss:  0.5450319643135636\n",
      "val_oss:  0.5449208817823974\n",
      "val_oss:  0.5448098762356273\n",
      "val_oss:  0.5446989476101919\n",
      "val_oss:  0.5445880958430837\n",
      "val_oss:  0.5444773208713474\n",
      "val_oss:  0.544366622632081\n",
      "val_oss:  0.5442560010624353\n",
      "val_oss:  0.544145456099614\n",
      "val_oss:  0.5440349876808734\n",
      "val_oss:  0.5439245957435228\n",
      "val_oss:  0.543814280224924\n",
      "val_oss:  0.5437040410624915\n",
      "val_oss:  0.543593878193693\n",
      "val_oss:  0.5434837915560478\n",
      "val_oss:  0.5433737810871289\n",
      "val_oss:  0.543263846724561\n",
      "val_oss:  0.5431539884060222\n",
      "val_oss:  0.5430442060692422\n",
      "val_oss:  0.5429344996520037\n",
      "val_oss:  0.5428248690921416\n",
      "val_oss:  0.5427153143275437\n",
      "val_oss:  0.5426058352961494\n",
      "val_oss:  0.5424964319359508\n",
      "val_oss:  0.5423871041849927\n",
      "val_oss:  0.5422778519813711\n",
      "val_oss:  0.5421686752632352\n",
      "val_oss:  0.5420595739687863\n",
      "val_oss:  0.5419505480362775\n",
      "val_oss:  0.541841597404014\n",
      "val_oss:  0.5417327220103535\n",
      "val_oss:  0.5416239217937056\n",
      "val_oss:  0.5415151966925315\n",
      "val_oss:  0.5414065466453449\n",
      "val_oss:  0.5412979715907116\n",
      "val_oss:  0.5411894714672488\n",
      "val_oss:  0.5410810462136256\n",
      "val_oss:  0.5409726957685638\n",
      "val_oss:  0.5408644200708356\n",
      "val_oss:  0.5407562190592663\n",
      "val_oss:  0.5406480926727326\n",
      "val_oss:  0.5405400408501623\n",
      "val_oss:  0.5404320635305357\n",
      "val_oss:  0.5403241606528842\n",
      "val_oss:  0.5402163321562914\n",
      "val_oss:  0.5401085779798916\n",
      "val_oss:  0.5400008980628717\n",
      "val_oss:  0.5398932923444691\n",
      "val_oss:  0.5397857607639734\n",
      "val_oss:  0.5396783032607254\n",
      "val_oss:  0.5395709197741173\n",
      "val_oss:  0.5394636102435927\n",
      "val_oss:  0.5393563746086468\n",
      "val_oss:  0.5392492128088254\n",
      "val_oss:  0.5391421247837267\n",
      "val_oss:  0.539035110472999\n",
      "val_oss:  0.5389281698163425\n",
      "val_oss:  0.5388213027535083\n",
      "val_oss:  0.5387145092242986\n",
      "val_oss:  0.5386077891685672\n",
      "val_oss:  0.5385011425262184\n",
      "val_oss:  0.5383945692372076\n",
      "val_oss:  0.5382880692415416\n",
      "val_oss:  0.5381816424792777\n",
      "val_oss:  0.5380752888905246\n",
      "val_oss:  0.5379690084154416\n",
      "val_oss:  0.5378628009942388\n",
      "val_oss:  0.5377566665671772\n",
      "val_oss:  0.5376506050745691\n",
      "val_oss:  0.5375446164567768\n",
      "val_oss:  0.5374387006542136\n",
      "val_oss:  0.5373328576073437\n",
      "val_oss:  0.5372270872566818\n",
      "val_oss:  0.5371213895427931\n",
      "val_oss:  0.537015764406294\n",
      "val_oss:  0.5369102117878503\n",
      "val_oss:  0.5368047316281795\n",
      "val_oss:  0.5366993238680492\n",
      "val_oss:  0.536593988448277\n",
      "val_oss:  0.5364887253097316\n",
      "val_oss:  0.5363835343933318\n",
      "val_oss:  0.5362784156400465\n",
      "val_oss:  0.5361733689908952\n",
      "val_oss:  0.5360683943869478\n",
      "val_oss:  0.5359634917693242\n",
      "val_oss:  0.5358586610791947\n",
      "val_oss:  0.5357539022577794\n",
      "val_oss:  0.5356492152463492\n",
      "val_oss:  0.5355445999862245\n",
      "val_oss:  0.535440056418776\n",
      "val_oss:  0.5353355844854247\n",
      "val_oss:  0.5352311841276413\n",
      "val_oss:  0.5351268552869464\n",
      "val_oss:  0.5350225979049108\n",
      "val_oss:  0.5349184119231551\n",
      "val_oss:  0.5348142972833497\n",
      "val_oss:  0.5347102539272153\n",
      "val_oss:  0.5346062817965213\n",
      "val_oss:  0.5345023808330877\n",
      "val_oss:  0.5343985509787847\n",
      "val_oss:  0.534294792175531\n",
      "val_oss:  0.5341911043652959\n",
      "val_oss:  0.5340874874900979\n",
      "val_oss:  0.5339839414920052\n",
      "val_oss:  0.5338804663131358\n",
      "val_oss:  0.5337770618956564\n",
      "val_oss:  0.5336737281817842\n",
      "val_oss:  0.5335704651137851\n",
      "val_oss:  0.5334672726339749\n",
      "val_oss:  0.533364150684719\n",
      "val_oss:  0.533261099208431\n",
      "val_oss:  0.5331581181475754\n",
      "val_oss:  0.5330552074446648\n",
      "val_oss:  0.5329523670422615\n",
      "val_oss:  0.5328495968829766\n",
      "val_oss:  0.5327468969094713\n",
      "val_oss:  0.5326442670644551\n",
      "val_oss:  0.5325417072906868\n",
      "val_oss:  0.5324392175309747\n",
      "val_oss:  0.5323367977281753\n",
      "val_oss:  0.532234447825195\n",
      "val_oss:  0.5321321677649886\n",
      "val_oss:  0.5320299574905598\n",
      "val_oss:  0.5319278169449617\n",
      "val_oss:  0.5318257460712963\n",
      "val_oss:  0.5317237448127133\n",
      "val_oss:  0.5316218131124127\n",
      "val_oss:  0.5315199509136423\n",
      "val_oss:  0.5314181581596988\n",
      "val_oss:  0.5313164347939281\n",
      "val_oss:  0.5312147807597241\n",
      "val_oss:  0.5311131960005294\n",
      "val_oss:  0.5310116804598357\n",
      "val_oss:  0.5309102340811828\n",
      "val_oss:  0.5308088568081591\n",
      "val_oss:  0.5307075485844015\n",
      "val_oss:  0.5306063093535957\n",
      "val_oss:  0.530505139059475\n",
      "val_oss:  0.530404037645822\n",
      "val_oss:  0.5303030050564671\n",
      "val_oss:  0.5302020412352891\n",
      "val_oss:  0.5301011461262153\n",
      "val_oss:  0.5300003196732209\n",
      "val_oss:  0.5298995618203294\n",
      "val_oss:  0.5297988725116128\n",
      "val_oss:  0.5296982516911909\n",
      "val_oss:  0.5295976993032314\n",
      "val_oss:  0.529497215291951\n",
      "val_oss:  0.5293967996016131\n",
      "val_oss:  0.5292964521765303\n",
      "val_oss:  0.5291961729610625\n",
      "val_oss:  0.5290959618996175\n",
      "val_oss:  0.5289958189366516\n",
      "val_oss:  0.5288957440166682\n",
      "val_oss:  0.5287957370842192\n",
      "val_oss:  0.5286957980839034\n",
      "val_oss:  0.5285959269603688\n",
      "val_oss:  0.5284961236583093\n",
      "val_oss:  0.528396388122468\n",
      "val_oss:  0.5282967202976352\n",
      "val_oss:  0.5281971201286483\n",
      "val_oss:  0.5280975875603932\n",
      "val_oss:  0.5279981225378022\n",
      "val_oss:  0.5278987250058562\n",
      "val_oss:  0.5277993949095833\n",
      "val_oss:  0.5277001321940583\n",
      "val_oss:  0.5276009368044045\n",
      "val_oss:  0.5275018086857916\n",
      "val_oss:  0.5274027477834375\n",
      "val_oss:  0.527303754042607\n",
      "val_oss:  0.5272048274086119\n",
      "val_oss:  0.5271059678268118\n",
      "val_oss:  0.5270071752426131\n",
      "val_oss:  0.5269084496014695\n",
      "val_oss:  0.5268097908488818\n",
      "val_oss:  0.5267111989303979\n",
      "val_oss:  0.5266126737916129\n",
      "val_oss:  0.5265142153781686\n",
      "val_oss:  0.5264158236357542\n",
      "val_oss:  0.5263174985101055\n",
      "val_oss:  0.5262192399470056\n",
      "val_oss:  0.5261210478922839\n",
      "val_oss:  0.5260229222918176\n",
      "val_oss:  0.5259248630915294\n",
      "val_oss:  0.52582687023739\n",
      "val_oss:  0.5257289436754167\n",
      "val_oss:  0.5256310833516726\n",
      "val_oss:  0.525533289212268\n",
      "val_oss:  0.5254355612033605\n",
      "val_oss:  0.5253378992711534\n",
      "val_oss:  0.5252403033618971\n",
      "val_oss:  0.5251427734218876\n",
      "val_oss:  0.5250453093974694\n",
      "val_oss:  0.5249479112350312\n",
      "val_oss:  0.5248505788810098\n",
      "val_oss:  0.5247533122818874\n",
      "val_oss:  0.5246561113841931\n",
      "val_oss:  0.5245589761345021\n",
      "val_oss:  0.524461906479436\n",
      "val_oss:  0.5243649023656628\n",
      "val_oss:  0.5242679637398966\n",
      "val_oss:  0.5241710905488971\n",
      "val_oss:  0.5240742827394714\n",
      "val_oss:  0.5239775402584714\n",
      "val_oss:  0.5238808630527966\n",
      "val_oss:  0.5237842510693907\n",
      "val_oss:  0.5236877042552454\n",
      "val_oss:  0.5235912225573967\n",
      "val_oss:  0.5234948059229275\n",
      "val_oss:  0.5233984542989664\n",
      "val_oss:  0.5233021676326877\n",
      "val_oss:  0.5232059458713119\n",
      "val_oss:  0.5231097889621051\n",
      "val_oss:  0.5230136968523792\n",
      "val_oss:  0.5229176694894918\n",
      "val_oss:  0.5228217068208462\n",
      "val_oss:  0.5227258087938913\n",
      "val_oss:  0.5226299753561224\n",
      "val_oss:  0.5225342064550791\n",
      "val_oss:  0.5224385020383475\n",
      "val_oss:  0.5223428620535591\n",
      "val_oss:  0.5222472864483904\n",
      "val_oss:  0.5221517751705641\n",
      "val_oss:  0.5220563281678482\n",
      "val_oss:  0.5219609453880554\n",
      "val_oss:  0.5218656267790445\n",
      "val_oss:  0.5217703722887195\n"
     ]
    }
   ],
   "source": [
    "SR.fit(X_train, y_train, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = SR.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SR.predict(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクラッチ正解率： 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# 実装したデータの正解率\n",
    "print(\"スクラッチ正解率：\", accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn正解率： 0.85\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = lr.predict(X_val)  \n",
    "print(\"sklearn正解率：\", accuracy_score(y_val, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(SR.loss, label=\"loss\")\n",
    "plt.plot(SR.val_loss, label=\"val_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.title('madel loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】決定領域の可視化\n",
    "決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# 分類問題の決定境界線を引くために特徴量を2つにして再計算\n",
    "# virgicolorとvirginicaのデータ抽出\n",
    "df= Xy[Xy['species']>=1]\n",
    "df.loc[df['species'] == 1, 'species'] = 0\n",
    "df.loc[df['species'] == 2, 'species'] = 1\n",
    "X3 = df.loc[: , ['sepal_length', 'petal_length']]\n",
    "y3 = df.loc[:, ['species']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X3, y3, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#trainデータのフィッテイング\n",
    "sc_x = StandardScaler()\n",
    "sc_x.fit(X_train)\n",
    "#trainとtestデータのtransform\n",
    "X_train_std = sc_x.transform(X_train)\n",
    "X_val_std = sc_x.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解値：　 0.85\n"
     ]
    }
   ],
   "source": [
    "SR = ScratchLogisticRegression(num_iter=1000, alpha=0.01,lambda_=0.01, bias=True, verbose=None)\n",
    "SR.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "pred_proba = SR.predict_proba(X_val)\n",
    "y_pred = SR.predict(pred_proba)\n",
    "print(\"正解値：　\", accuracy_score(y_val, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.88337549, -1.04231467, -0.80201157, -2.24383013, -0.20125384,\n",
       "        0.1592008 , -0.44155693, -0.44155693, -1.04231467, -0.32140539,\n",
       "       -0.32140539, -0.56170848, -0.92216312, -0.56170848, -0.92216312,\n",
       "       -0.44155693, -0.20125384, -0.44155693, -0.44155693, -0.80201157,\n",
       "       -0.44155693, -0.20125384, -1.6430724 , -1.40276931, -1.16246621,\n",
       "       -1.52292085, -1.16246621, -1.6430724 , -0.92216312, -0.08110229,\n",
       "       -0.56170848, -0.56170848, -0.80201157, -1.04231467, -1.04231467,\n",
       "       -0.20125384, -1.04231467, -1.16246621, -0.68186003, -0.80201157])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = SR.theta\n",
    "coef = ww[:,1]\n",
    "intercept = ww[:, 0]\n",
    "\n",
    "y = y_train.values.ravel()\n",
    "X_train_std[y==0][:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# プロット\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('petal_length')\n",
    "plt.scatter(X_train_std[y==0][:, 0],X_train_std[y==0][:, 1] )\n",
    "plt.scatter(X_train_std[y==1][:, 0],X_train_std[y==1][:, 1] )\n",
    "line = np.linspace(-10, 10)\n",
    "plt.plot(line, -1*(line * coef + intercept)/ coef[0], c='r', label=\"LogisticRegression\")\n",
    "plt.ylim(X_train_std[:,1].min()-0.5, X_train_std[:,1].max()+0.5)\n",
    "plt.xlim(X_train_std[:,0].min()-0.5, X_train_std[:,0].max()+0.5)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】（アドバンス課題）重みの保存\n",
    "検証が容易になるように、学習した重みを保存および読み込みができるようにしましょう。pickleモジュールやNumPyのnp.savezを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.85459577  1.09363282]]\n"
     ]
    }
   ],
   "source": [
    "l_theta = SR.theta\n",
    "print(l_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickleモジュール呼び出し\n",
    "import pickle\n",
    "\n",
    "# 保存\n",
    "with open('l_theta.pickle', 'wb') as f:\n",
    "    pickle.dump(l_theta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "with open('l_theta.pickle', 'rb') as f:\n",
    "    m_theta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.85459577  1.09363282]]\n"
     ]
    }
   ],
   "source": [
    "print(m_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2, 3, 4, ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
